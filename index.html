<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Econometric Modelling Experts Development Programme - Module III</title>
    <meta charset="utf-8" />
    <meta name="author" content="William Godfred Cantah (Ph.D)" />
    <meta name="date" content="2025-09-02" />
    <script src="libs/header-attrs-2.29/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/panelset-0.3.0/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.3.0/panelset.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/style2.css" type="text/css" />
    <link rel="stylesheet" href="css/rutgers-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





class: title-slide, middle
background-image: url(fig/Tpage.png)
background-position:center
background-size: contain

# **Econometric Modelling Experts Development Programme - Module III**
### Bayesian Vector Autoregressive (BVARs) Models
### 

.directorate[
William Godfred Cantah (Ph.D) &lt;br&gt;
Department of Data Science and Economic Policy &lt;br&gt;
School of Economics, UCC
]


---
layout: true
background-image: url(fig/Page.png)
background-size: contain
background-position: center

---
# Introduction to BVAR Models
&lt;div style="font-size: 30px;"&gt;

--

* BVAR model combines the standard VAR model with Bayesian methods.

&lt;br&gt;
--

* It treats the VAR's parameters as random variables with prior probability distributions, rather than fixed values.

???
A prior probability distribution, in the context of Bayesian statistics, represents the initial degree of belief in a hypothesis or the probability of an event before considering any new data. It is a way to incorporate existing knowledge or beliefs into the statistical analysis, allowing for a more flexible and informative approach to inference.

&lt;br&gt;
--

&lt;br&gt;
* It allows for the incorporation of prior information, which can help improve estimation and forecasting accuracy, especially in small samples or when dealing with many variables.

--



* BVAR models are particularly useful when the number of variables are large relative to the number of observations, as they can help mitigate overfitting and improve model stability.

---
# Why BVAR Models?
&lt;div style="font-size: 27px;"&gt;

--

- BVAR effectively tackles the ***.my-coral["curse of dimensionality"]*** in VARs by using prior information to shrink coefficients, preventing overfitting and leading to more stable estimates, especially with many variables.

???

- Multivariate macroeconomic models usually have a large number of parameters

  - Note that `\(dof=(k^2\times p)+C\)`
  
  - Therefore for `\(VAR(4)\)` with 5 variables and a constant has 105 parameters, which requires 27 years of quarterly data
  
  - Without prior information, it is hard to obtain precise estimates of all the parameters
  
  - Also useful in State-Space models where all the unobserved variables and parameters are all random variables
  
  - Most macroeconomic datasets have a fairly limited number of observations
  
  - Yet we would often include a number of variables in the model
  
  - The use of time varying parameters (and similar innovations) eats up additional degrees of freedom
  
  - By imposing restrictions on parameters or shrinking them towards zero, we can deal with the over-parametrisation
--

- By reducing parameter uncertainty, BVAR consistently delivers more accurate out-of-sample forecasts compared to unrestricted VAR models.

--

- BVAR provides a formal mechanism to incorporate existing economic theory, expert opinions, or historical data into the model via prior distributions, leading to more economically sensible results.


???

BVARs are able to deal with misspecification &amp; uncertainty in a theoretically consistent manner since the parameters are random variables
--

- Unlike VARs that provide point estimates, BVAR yields full posterior distributions for parameters and derived quantities (like impulse responses), offering a more complete picture of model uncertainty through credible intervals.

--

- BVAR's ability to manage a large number of variables makes it well-suited for modern macroeconomic analysis where researchers often utilize extensive datasets with relatively short time series.


---
# Formulating BVAR Model
&lt;div style="font-size: 27px;"&gt;
--

A ***.my-coral[Vector Autoregression]*** of order `\(p\)` is:

--

`$$y_t = c + A_1 y_{t-1} + \cdots + A_p y_{t-p} + \varepsilon_t$$`

--

where  
- `\(y_t\)` is a `\(k\times1\)` vector of endogenous variables at time `\(t\)`.  

--

- `\(c\)` is a `\(k\times1\)` intercept vector.  

--

- `\(A_i\)` is a `\(k\times k\)` coefficient matrix for lag `\(i\)`.  

--

- `\(\varepsilon_t\sim \mathcal{N}(0,\Sigma)\)` is a `\(k\times1\)` error term. 

--

**Problem:** If `\(k\)` and `\(p\)` are large relative to `\(T\)`, OLS estimates get very imprecise.  
???
When the number of parameters `\((k·p)\)` approaches the sample size `\((T)\)`, there simply isn’t enough information to estimate each coefficient precisely, so the OLS estimator’s variance blows up. Algebraically, an ill‑conditioned `\(X'X\)` matrix (from too many regressors) leads to very large entries in its inverse, which directly inflate the variances of the estimated coefficients. In practice, this “overfitting” problem means large standard errors and unstable forecasts—one must therefore reduce dimensionality or introduce shrinkage (e.g., via Bayesian priors) to regain precision.


---
# From VAR to BVAR
&lt;div style="font-size: 27px;"&gt;
--

- A Vector Autoregression of order `\(p\)` for a `\(k\)`‑dimensional series `\(y_t\)` is written in stacked form as:

`$$Y=X\beta +E$$`

--

where

* `\(Y\)` is the `\((T-p)\times k\)` matrix whose rows are `\(y_t' = (y_{t,1},\dots,y_{t,k})\)` for `\(t=p,\dots,T\)`.

* `\(X\)` is the `\((T-p)\times(1 + k p)\)` regressor matrix, each row containing a 1 (intercept) followed by the stacked lags `\((y_{t-1}',\dots,y_{t-p}')\)`.

* `\(\beta\)` is the `\((1 + k p)\times k\)` matrix that vertically stacks the intercept vector `\(c\)` and the coefficient matrices `\(A_1,\dots,A_p\)`.

* `\(E\)` is the `\((T-p)\times k\)` matrix of errors, each row `\(\varepsilon_t'\sim \mathcal{N}(0,\Sigma)\)`.


???
# From VAR to BVAR (stacked form)

A `\(k\)`-variable VAR(`\(p\)`) can be written as one big regression:

`$$Y = X\beta + E$$`

- `\(Y\)` is a `\((T-p)\times k\)` matrix of the outcomes we want to explain (one row per time `\(t=p,\dots,T\)`).  
  *Rule of thumb*: we lose the first `\(p\)` rows because we need `\(p\)` lags.

- `\(X\)` is a `\((T-p)\times(1+kp)\)` matrix of predictors.  
  Each row contains a **1** (intercept) and the stacked lags
  `\((y_{t-1}^\top,\,\dots,\,y_{t-p}^\top)\)`.  
  *Why* `\((1+kp)\)`? → one intercept **+** `\(k\)` variables `\(\times\)` `\(p\)` lags.

- `\(\beta\)` is a `\((1+kp)\times k\)` matrix holding all coefficients
  (intercepts and lag coefficients, stacked).

- `\(E\)` is a `\((T-p)\times k\)` matrix of shocks with rows
  `\(\varepsilon_t^\top \sim \mathcal N(0,\Sigma)\)`.

This stacked form makes a VAR look like a standard multivariate regression, which is ideal for Bayesian estimation.

# What makes it “Bayesian”?
Instead of using only the data, we combine:

- the **likelihood** (from the regression above), and  
- **priors** (our beliefs about plausible parameter values before seeing the data).

The result is the **posterior**, which blends data and prior information.


---
# Prior Specification
&lt;div style="font-size: 28px;"&gt;

--

- In a ***Bayesian VAR***, we place a joint prior on `\((\beta,\Sigma)\)`. A common choice is the Normal–Inverse‑Wishart:
`$$\beta \mid \Sigma \;\sim\;\mathcal{N}\bigl(\beta_0,\;\Sigma \otimes V_0\bigr)$$`

- `\(\beta\)` is the `\(((1 + kp) \times k)\)` matrix of all intercepts and lag coefficients, stacked column‑wise.  
- `\(\beta_0\)` is the prior mean of the same size, typically zeros for off‑diagonal lags and ones on own first lags (Minnesota style).  
- `\(\Sigma\)` is the `\((k\times k)\)` residual covariance matrix of the VAR errors.  
- `\(V_0\)` is the `\(((1 + kp)\times(1 + kp))\)` covariance matrix governing how strongly each element of `\((\beta)\)` is shrunk toward its prior mean.  
- `\(\otimes\)` denotes the Kronecker product, which ensures that uncertainty in each coefficient is scaled by the appropriate element of `\((\Sigma)\)`.

???
# Prior Specification — presenter notes (Normal–Inverse-Wishart in a BVAR)

• What the single line **β | Σ ~ N(β₀, Σ ⊗ V₀)** is telling us  
  – We are using the **matrix-normal / inverse-Wishart (MNIW)** joint prior for a multivariate regression/VAR: coefficients are **matrix-normal conditional on Σ**, and Σ itself gets an inverse-Wishart prior. This is **conjugate**, so posterior updates and Gibbs draws are fast (draw Σ from IW, then β | Σ from matrix-normal). :contentReference[oaicite:0]{index=0}

• Interpreting each symbol on the slide  
  – **β**: the full coefficient matrix (size \((1+kp) \times k\)): intercept + all lag coefficients for all equations, stacked column-wise.  
  – **β₀**: the **prior mean** of β. In “Minnesota style,” we often set **own first lags near 1** (random-walk belief) and **everything else near 0** (cross-lags and higher lags), which encodes shrinkage toward persistence-with-parsimony. :contentReference[oaicite:1]{index=1}  
  – **Σ**: the \(k \times k\) **residual covariance** across equations (how shocks co-move contemporaneously). Inverse-Wishart is the classic conjugate prior for this block. :contentReference[oaicite:2]{index=2}  
  – **V₀**: the prior covariance for the **rows** of β (across regressors); tuning its scale controls **how tightly** we shrink coefficients toward β₀ (smaller \(V₀\) ⇒ stronger shrinkage).  
  – **⊗ (Kronecker product)**: makes the matrix-normal covariance **separable**—**Σ** scales columns (equations) and **V₀** scales rows (regressors). This “row/column” scaling is the hallmark of the matrix-normal prior. :contentReference[oaicite:3]{index=3}

• How to explain the **role of shrinkage** here  
  – **β₀** sets *where* we shrink (e.g., own-lag ≈ 1, others ≈ 0).  
  – **V₀** sets *how hard* we shrink (overall tightness and lag-decay you choose).  
  – Using Σ inside the covariance (via **Σ ⊗ V₀**) automatically **scales uncertainty by equation variance**, so coefficients in noisier equations get looser priors than in quiet ones. :contentReference[oaicite:4]{index=4}

• One-slide “what the audience should take away”  
  – This prior is **computationally convenient (conjugate)** and **economically sensible** when paired with Minnesota-style choices for β₀ and V₀: it shrinks toward a parsimonious, persistent benchmark while letting the data pull coefficients away when strongly warranted. :contentReference[oaicite:5]{index=5}

# Plain-English takeaway

- The **Normal prior** on `\(\beta\)` says: *“Most coefficients are probably small (near their prior means), unless the data present strong evidence otherwise.”*

- The **Inverse-Wishart prior** on `\(\Sigma\)` says: *“Shock variances/correlations should be reasonable, not extreme; the data can refine this.”*

- Together, NIW **stabilizes** estimation in high-dimensional VARs, improving forecasts and avoiding overfitting.

# Optional: tiny dimension check (no computation)

- If `\(k=3\)` variables and `\(p=2\)` lags → `\((1+kp) = 1 + 3\times2 = 7\)` regressors per equation.  
- With `\(T=120\)` months and `\(p=2\)` → usable rows `\((T-p)=118\)`.  
- Shapes:  
  - `\(Y:\;118\times3\)`  
  - `\(X:\;118\times7\)`  
  - `\(\beta:\;7\times3\)`  
  - `\(E:\;118\times3\)`

---
# Prior Specification Continued
&lt;div style="font-size: 28px;"&gt;
#### Why a "joint prior" on both `\(\beta\)` and `\(\Sigma\)`?
- In a VAR we need to estimate two things:
  - `\(\beta\)` the collection of all slope and intercept coefficients in each equation,
  - `\(\Sigma\)` the matrix that describes how the shocks (residuals) in those equations move together.
- A joint prior simply means we choose a rule that tells us, before seeing data, what combinations of `\((\beta -\Sigma)\)` we think are reasonable.
#### Normal for `\(\beta\)`, Invers-Wishart for `\(\Sigma\)`
- `\(\beta\)` is treated as if it comes from a multivariate normal distribution (that’s the “Normal” part).
- `\(\Sigma\)` is treated as if it comes from an Inverse-Wishart distribution, which is a common choice for covariance matrices in Bayesian statistics (that’s the “Inverse-Wishart” part).


???
# Prior Specification (continued): **Why a joint prior on β and Σ?** — presenter notes

• What this slide is about  
  – In a VAR we need to estimate two blocks: **β** (all intercepts and lag coefficients) and **Σ** (the covariance of the reduced-form shocks).  
  – A **joint prior** simply means we state, *before seeing the data*, what combinations of (β, Σ) we regard as plausible. This gives a coherent starting point for estimation.

• The standard joint prior used here  
  – **Normal for β | Σ**: treat the coefficients as coming from a (matrix-)normal distribution **conditional on Σ**. Intuition: the scale of shocks (Σ) tells us how dispersed the coefficients should be across equations; using Σ to scale the prior makes the prior **scale-aware** and comparable across equations. :contentReference[oaicite:0]{index=0}  
  – **Inverse-Wishart for Σ**: treat the shock covariance itself as coming from an inverse-Wishart distribution—popular because it is **conjugate** to the Gaussian likelihood. Its hyperparameters act like “prior sample size” and a “prior scatter” matrix. :contentReference[oaicite:1]{index=1}

• Why choose this pair (the conjugacy pay-off)  
  – With this Normal–Inverse-Wishart (matrix-normal / IW) setup, the **posterior stays in the same family**: after combining prior with data, you update a handful of hyperparameters and you’re done.  
  – Sampling is fast: one Gibbs iteration is **draw Σ** from inverse-Wishart, then **draw β | Σ** from matrix-normal. This is why this prior is the workhorse in reduced-form BVARs. :contentReference[oaicite:2]{index=2}

• How to *read* the slide bullets to the audience  
  – “**β is Normal**” ⇒ we encode beliefs about coefficient sizes (e.g., shrink cross-lags toward zero, allow own first lag more freedom by choosing the prior mean/covariance appropriately).  
  – “**Σ is Inverse-Wishart**” ⇒ we encode beliefs about overall volatility and co-movement of shocks (e.g., stronger prior df = tighter belief).  
  – Together this joint prior is often called **MNIW/NIW** and is the natural conjugate choice for VARs written as a multivariate regression. :contentReference[oaicite:3]{index=3}

• Practical guidance to say aloud  
  – Pick **β’s prior mean/covariance** to reflect Minnesota-style shrinkage if desired (own-lag loose, cross-lags tight, lag-decay). Conjugacy is retained. :contentReference[oaicite:4]{index=4}  
  – Avoid ultra-diffuse IW on Σ in small samples (it can be too permissive). Use reasonable degrees-of-freedom and a scale that reflects the data’s units, or move to hierarchical variants if needed. :contentReference[oaicite:5]{index=5}

• One-liner close  
  – “A **joint Normal–Inverse-Wishart prior** ties together our beliefs about dynamics (β) and shock co-movement (Σ), keeps computation easy (conjugate), and plays nicely with Minnesota-style shrinkage.”


---
# Shrinkage Priors
&lt;div style="font-size: 30px;"&gt;

--

- In basic terms, ***.my-coral[shrinkage prior]*** is a way of regularising (or controlling) how much freedom we allow the parameters of a BVAR model to have, especially when we have many variables or a small sample size.

--

- In BVAR, we often estimate many coefficients (lags of multiple variables). 

--

- This can lead to overfitting, where the model fits the sample data well but performs poorly in forecasting.

--

- To solve this, we use priors that "shrink" the coefficients—pulling them toward zero or toward some prior belief (*e.g., that variables only affect themselves and not others, or that longer lags matter less*).

???
# Shrinkage Priors — presenter notes

• Big idea: a *shrinkage prior* regularizes the VAR so we don’t overfit when k (vars) and p (lags) are large or T is small.
  – Think “rubber band” gently pulling coefficients toward plausible values.

• Why needed:
  – BVAR has many coefficients (lags × variables).
  – Without control, the model can fit noise → great in-sample fit, poor forecasts.

• What “shrinkage” does:
  – Pulls coefficients toward 0 or toward a belief (e.g., own first lag ≈ persistent; cross-lags small; higher lags weaker).
  – Bias–variance tradeoff: a little bias buys a big drop in variance → more stable forecasts.

• Practical beliefs often encoded:
  – Own first lag allowed to be large (random-walk flavor).
  – Cross-variable effects smaller than own-lag effects.
  – Higher lags decay in importance.

• Tuning (don’t dive into math here; just signal knobs):
  – Overall tightness (λ): smaller = stronger shrinkage.
  – Cross-lag tightness (θ) and lag decay (∝ 1/ℓ).
  – Scale by residual std devs so units don’t matter.

• Take-away line:
  – “Shrinkage priors keep the VAR humble: flexible where it matters, conservative where evidence is weak.”

• Transition:
  – “Next I’ll show the Minnesota-style variance formula that implements these ideas.”

---
# Shrinkage Priors
&lt;div style="font-size: 30px;"&gt;

--

-  This shrinkage helps to stabilise estimates, reduce variance, and improve out-of-sample forecasts.

--

- The shrinkage prior is typically specified as:
`$$A_i \sim \mathcal{N}(M_i,\,V_i)$$`

where

--

- `\(A_i\)` is the coefficient matrix for lag `\(i\)`,

- `\(M_i\)` is the prior mean (often zero or random-walk structure),

- `\(V_i\)` is the prior covariance controlling **strength** of shrinkage.

- Smaller diagonal entries of `\(V_i\)` ⇒ stronger shrinkage.

???
- Shrinkage priors are particularly useful in high-dimensional settings where the number of parameters to estimate is large relative to the number of observations, as they help prevent overfitting and improve model stability.

- The choice of prior mean `\(M_i\)` and covariance `\(V_i\)` can be informed by economic theory, previous research, or empirical evidence, allowing for a more tailored approach to each specific application.

• Key message:
  – Shrinkage **stabilises** estimates, **reduces variance**, and **improves forecasts** when k and p are large or T is small.

• What the equation says:
  – For each lag i, the whole coefficient matrix **A_i** is assumed Normal with
    mean **M_i** and covariance **V_i**.
  – Think of a “rubber band” pulling coefficients toward **M_i**; the strength
    of that pull is set by **V_i**.

• Roles of the components:
  – **A_i**: all coefficients on lag i across equations (a k×k block).
  – **M_i** (prior mean):
      * Often zeros → “most effects are small unless data insist”.
      * Minnesota flavor: own first-lag entries ≈ 1 (random-walk persistence), others ≈ 0.
  – **V_i** (prior covariance):
      * Sets how tight the shrinkage is. **Smaller diagonal entries ⇒ stronger shrinkage**.
      * Off-diagonals (optional) encode beliefs about groups of coefficients moving together.

• Intuition &amp; effect:
  – Tight **V_i** trades a little bias for a big drop in variance → more stable IRFs and better OOS forecasts.
  – We usually make higher lags tighter (decay with i) and cross-variable effects tighter than own-lags.

• Practical guidance (one line):
  – Start with means: own first-lag ≈ 1, others 0; choose **V_i** to decay with lag and penalise cross-lags, then check forecast performance and adjust.

• Transition:
  – Next, I’ll show the Minnesota variance recipe that implements these choices mechanically.

---
# Minnesota Prior
&lt;div style="font-size: 30px;"&gt;

--

- Introduced in Doan, Litterman and Sims (1984), Litterman (1984), sometimes referred as the Litterman prior. Popular for its simplicity and good forecasting performance.

--

- ***.my-coral[Priors on variances of residuals:]*** diagonal (no covariances) with estimates of variances on diagonal. All equations can be estimated one-by-one.

--

- ***.my-coral[Priors on parameters:]*** most of all elements of the matrix of coefficients are set to zero. The restrictions differ for VAR in levels and VAR in growth rates.

--

  - VAR in levels: the dependent variable is assumed to follow RW, hence coefficient at the first lag of dependent variable is set to 1, all others to zero.
  
  - VAR in first differences: all priors set to zero, but there are some degrees of flexibility. *For instance, if a fair degree of persistence is expected, the prior for the coefficient at the first lag could be set to 0.9 or to similar values.*


???
# Minnesota Prior — presenter notes

• Context &amp; why it’s popular
  – Introduced by Doan–Litterman–Sims (1984). Simple, fast, and strong forecasting performance.
  – It’s a **shrinkage** prior, not an identification scheme.

• Residual variance prior (Σ)
  – Assume **diagonal Σ** (no contemporaneous covariances across equations).
  – Each equation’s error variance estimated from a univariate AR; scale factors carry into the coefficient priors.
  – Consequence: we can estimate equations **one-by-one** (computationally cheap).

• Parameter priors (β): means
  – Most coefficients centered at **0**.
  – **VAR in levels**: own first lag mean ≈ **1** (random-walk persistence); all others 0.
  – **VAR in first differences**: all means **0**; if some persistence is expected, set own first-lag mean near 0.9.

• Parameter priors (β): variances (shrinkage strength)
  – Key ideas:
      1) **Overall tightness** (λ): smaller ⇒ stronger shrinkage.
      2) **Lag decay**: higher lags get tighter (∝ 1/ℓ²).
      3) **Cross-lag penalty** (θ): coefficients on *other* variables tighter than own lags.
      4) **Scale normalization**: multiply by (σ_i/σ_j) so units/volatility are comparable.
  – Intuition: allow persistence in own first lags; damp higher lags and cross effects.

• Practical use
  – Start with standard hyperparameters; tune λ (and θ) by forecast CV or marginal likelihood.
  – Check that forecasts/IRFs are stable; loosen/tighten as needed.
  – Remember: diagonal Σ is fine for forecasting; structural analysis still needs an identification step (e.g., Cholesky, sign, long-run).

• Take-away line
  – “Minnesota prior keeps the VAR parsimonious: own-lag persistence, decaying higher lags, and small cross effects—cheap to estimate and hard to overfit.”

---
# Minnesota Prior
&lt;div style="font-size: 23px;"&gt;

--
- The **Minnesota prior** (Litterman 1979; Sims 1989) sets  

  - `\(M_i=0\)` for all off-diagonal elements,  
  
  - Own‑lag (lag-1): `\(mean = 1\)`(random walk belief)

- Prior variances for coefficient on lag `\(\ell\)` of variable `\(j\)` in equation for variable `\(i\)`:

`$$V_i(a_{ij,\ell}) = \begin{cases}
  (\frac{\lambda}{\ell})^2, &amp; \text{if } \ell=1\\
  (\frac{\lambda \ \theta \ \sigma_i}{\ell \ \sigma_j} )^2 &amp; \text{if } \ell&gt;1
  \end{cases}$$`
- where  
  - `\(\lambda&gt;0\)` is overall shrinkage hyperparameter,  
  - `\(\theta&gt;0\)` is a relative shrinkage parameter,  
  - `\(\ell\)` is the lag length,
  - `\(\sigma_i\)` and `\(\sigma_j\)` estimated residual std devs from univariate AR models

???
# Minnesota Prior — slide notes

• What it sets (means):
  – Off-diagonal elements of A_i: **M_i = 0** → cross-variable effects are small unless data insist.
  – Own first lag (lag 1): prior mean **≈ 1** → random-walk belief for persistence (in levels).

• What it sets (variances):
  – For coefficient on lag ℓ of variable j in equation i:
      V_i(a_{ij,ℓ}) =
        { (λ/ℓ)^2                    if ℓ = 1
        { (λ θ σ_i / (ℓ σ_j))^2      if ℓ &gt; 1
  – Read it as “how free each coefficient can be” around its mean.

• Intuition for the knobs:
  – **λ** (overall tightness): smaller ⇒ stronger shrinkage everywhere.
  – **θ** (relative/cross-lag tightness): &lt; 1 shrinks *other-variable* lags more than own lags → diagonal dominance.
  – **ℓ** (lag length): variance ∝ 1/ℓ² → higher lags get tighter → parsimony.
  – **σ_i, σ_j** (scale factors): residual std devs from univariate ARs; normalize units so volatile variables don’t dominate.

• Practical takeaways:
  – Allows **own lag-1** to be flexible (captures persistence).
  – Forces **higher lags** and **cross-lags** toward zero unless the data push back.
  – Improves forecast stability in small samples and big systems.

• One-liner:
  – “Minnesota = random-walk on own lag-1 + tight, decaying, scale-adjusted shrinkage on everything else.”

• Transition:
  – Next: show how tuning λ and θ changes IRFs/forecasts; pick values via forecast CV or marginal likelihood.

---
# Minnesota Prior
&lt;div style="font-size: 28px;"&gt;

--
- The prior variances are inversely proportional to the square of the lag index, meaning that coefficients for longer lags are shrunk more strongly toward zero.

--

- The Minnesota prior is particularly effective in reducing the influence of long lags, which are often less reliable due to limited data, while allowing for more flexibility in the first lag, which is typically more informative.

--

- The prior can be adjusted by changing the `\(\lambda\)` parameter, which controls the overall strength of shrinkage applied to all coefficients.

--

- The Minnesota prior is widely used in empirical macroeconomic research due to its simplicity and effectiveness in improving forecast accuracy, especially in small samples or when dealing with many variables.

--

- It is often implemented in Bayesian VAR software packages, allowing researchers to easily apply it to their data without needing to manually specify the prior distributions for each coefficient.

???
# Minnesota Prior — presenter notes (usage &amp; intuition)

• Core rule to highlight:
  – Prior variances shrink with lag:  variance ∝ 1/ℓ².
  – Translation: **longer lags are pushed harder toward zero**; the first lag is left more flexible.

• Why this helps:
  – Long lags are noisy in small samples; shrinking them reduces overfitting.
  – Keeping lag-1 looser captures persistence that’s usually informative.

• The master knob:
  – **λ (overall tightness)** controls how strong shrinkage is across all coefficients.
    • Smaller λ ⇒ stronger shrinkage (more conservative).
    • Larger λ ⇒ looser priors (let the data move coefficients more).

• When it shines:
  – Many variables and/or short samples.
  – Empirically strong for **forecasting**; simple to implement and tune.

• Implementation note:
  – Built into most BVAR packages; you typically set λ (and sometimes cross-lag tightness θ and lag decay),
    with automatic scaling by residual standard deviations so units don’t matter.

• Practical guidance:
  – Start with standard defaults; **tune λ by out-of-sample forecast performance or marginal likelihood**.
  – Always do a quick sensitivity check (IRFs/forecasts under tighter vs looser λ).

• Caveat / reminder:
  – Minnesota is a **forecasting prior**, not an identification strategy. For structural analysis, add
    an identification step (e.g., Cholesky, sign, or long-run restrictions).

• One-liner to close:
  – “Minnesota trims the fat from distant lags and keeps the first lag flexible—simple, robust, and forecast-friendly.”


---
# Minnesota Prior: Intuition
&lt;div style="font-size: 25px;"&gt;

--

- **Random Walk Belief:** The Minnesota prior assumes each variable `\(y_{i}\)` behaves like a random walk (especially suitable for macroeconomic aggregates). This implies a priori that variable `\(i\)`’s own first lag coefficient `\(\approx 1\)`, and other coefficients `\(\approx 0\)`.  

--

- **Prior Means:** Traditionally, `\(E[\text{coef}_{i,j,\ell}] = 0\)` for most coefficients, **except** `\(E[\text{coef}_{i,i,1}] = 1\)` for the own first lag (to allow for unit-root behavior). In practice, some implementations center all lag coefficients at 0, treating the random-walk as a soft tendency rather than exact mean 1.

--

- **Shrinkage Structure:** The Minnesota prior imposes **greater shrinkage on certain coefficients**:

  - Coefficients on **longer lags** are more tightly shrunk to zero than short lags (decay with lag length).
  
  - **Cross-variable lags** (lag of variable `\(j\)` in equation `\(i\)` for `\(i\neq j\)`) are more tightly shrunk than own lags (lag of `\(i\)` in its own equation). 
  
  - Typically an uninformative (very loose) prior is placed on intercepts and any deterministic terms

???
# Minnesota Prior: Intuition — presenter notes

• Big picture:
  – Encode “**persistent in its own history, cautious elsewhere**.”
  – Works well for macro aggregates where persistence is strong.

• Random-walk belief:
  – For a VAR in **levels**, own first lag of variable i gets prior mean ≈ **1** (unit-root flavor);
    other coefficients ≈ **0**.
  – Many implementations soften this: keep all means at 0 but give a **looser variance** to the
    own lag-1 so data can put it near 1.

• Prior means (what to say):
  – `\((E[\text{coef}_{i,j,\ell}] = 0)\)` for most coefficients,
    except `\((E[\text{coef}_{i,i,1}]\)` `\(approx 1)\)` (levels).  
  – In **first differences**, all means are typically 0.

• Shrinkage structure (how tightness is assigned):
  – **Lag decay:** variance shrinks with lag (∝ 1/ℓ²), so **longer lags** are pushed harder toward 0.
  – **Cross-variable lags:** coefficients of variable `\((j)\)` in equation `\((i)\)` (with `\((i \ne j))\)`
    are shrunk **more** than own lags → diagonal dominance.
  – **Deterministics (intercept, trend, dummies):** usually given **loose** priors unless you have
    strong beliefs.

• Why it helps:
  – Reduces overfitting, stabilizes IRFs, and improves out-of-sample forecasts—especially with many
    variables or short samples.

• One-liner to close:
  – “Minnesota lets **lag-1 own effects breathe**, while keeping **distant and cross effects** modest
    unless the data really demand them.”
 
---
# Minnesota Prior: Pros and Cons
&lt;div style="font-size: 25px;"&gt;

- Reduces overfitting by shrinking most coefficients towards zero.

--

- Often conjugate (or semi-conjugate) under assumptions (normal prior on coefficients, fixed diagonal `\(\Sigma\)`), so posterior computation is fast and analytically.

--

- Only a few intuitive hyperparameters (`\(\lambda\)`’s) which can be set to standard values (e.g. `\(\lambda_1 \approx0.2\)`, `\(\lambda_2=1\)`) that usually work well, or tuned to beliefs.


--

- **Cons:**
  - Assumes all variables have similar dynamics (e.g. all behave like random walks), which may not hold in practice.
  
  - Can be too restrictive if some variables have very different dynamics (e.g. stationary vs. unit-root).
  
  - Shrinkage can lead to biased estimates if the true coefficients are far from the prior means.

???
# Minnesota Prior — Pros &amp; Cons (presenter notes)

• Pros — why people use it
  – **Reduces overfitting** by shrinking most coefficients toward 0; improves OOS forecasts.
  – **Computationally simple**: with diagonal Σ and Normal priors, conditionals are conjugate/close → fast Gibbs; often **equation-by-equation**.
  – **Few, intuitive hyperparameters** (e.g., overall tightness λ, cross-lag tightness θ, lag decay) that are easy to set or tune.
  – **Good empirical track record** in macro; robust starting point when k, p are large and T is modest.

• Cons — what to watch
  – **Homogeneous dynamics assumption**: tends to treat variables as similarly persistent (random-walk flavor); may be wrong for mixes of stationary and unit-root series.
  – **Too restrictive** if some coefficients are truly large (risk of bias toward zero).
  – **Diagonal Σ** ignores contemporaneous correlations; fine for forecasting, weak for structural analysis.
  – **Hyperparameter sensitivity**: overly tight λ can overshrink; too loose λ can overfit.

• Mitigations / good practice
  – **Tune λ (and θ)** via forecast CV or marginal likelihood; run sensitivity checks on IRFs/forecasts.
  – Allow **variable-specific tightness** or **looser own-lag(1)** if persistence differs.
  – For richer structure, move **beyond Minnesota**: SSVS, horseshoe (global–local), or allow non-diagonal Σ / SUR-style estimation.
  – Remember: Minnesota is a **forecasting prior**; for identification add Cholesky, sign, or long-run restrictions.

• One-liner
  – “Great for fast, stable forecasts in big VARs—just don’t force one-size-fits-all persistence, and always tune the tightness.”

---
# Beyond Minnesota: Global-Local Shrinkage
&lt;div style="font-size: 25px;"&gt;

--

- Minnesota prior&lt;/span&gt; applies the same shrinkage level to all coefficients in a group (e.g. all cross-lags). 

--

- **Global-local shrinkage** allows different degrees of shrinkage for each coefficient, improving flexibility.

--

- The key idea: introduce a global parameter for overall shrinkage and a local parameter for each coefficient.

--

- Global-Local Structure: Each coefficient `\(\beta_{ij}\)` gets a prior with two levels
  - a shared global shrinkage parameter (controls overall tightness/sparsity across all coefficients or a large subset),
  
  - an individual local shrinkage parameter, specific to `\(\beta_{ij}\)`, allowing some coefficients to be very tightly shrunk (if unimportant) while others can be relatively free (if the data deems them important).


???
# Beyond Minnesota: Global–Local Shrinkage — presenter notes

• Motivation
  – Minnesota uses the **same shrinkage level** within groups (e.g., all cross-lags).
  – Risk: important coefficients get **over-shrunk**, unimportant ones **under-shrunk**.

• Core idea
  – Add two layers of shrinkage:
      1) **Global** parameter τ: controls overall tightness/sparsity across *all* coefficients.
      2) **Local** parameter λ_{ijℓ}: coefficient-specific “release valve” so some β’s can escape heavy shrinkage.

• Generic structure (say for coefficient β_{ijℓ})
  – β_{ijℓ} ~ N(0,  τ² · λ_{ijℓ}² · s_{ijℓ})
    where:
      • τ (global) ⇒ how parsimonious the whole VAR is,
      • λ_{ijℓ} (local) ⇒ lets data decide if a specific coefficient is big/small,
      • s_{ijℓ} ⇒ optional scaling (e.g., lag decay, σ_i/σ_j) to keep Minnesota-style normalization.

• Examples of global–local priors
  – **Horseshoe**: λ ∼ C⁺(0,1) (aggressive on noise, gentle on signals).
  – **Bayesian Lasso**: Laplace (Normal–Exponential mixture).
  – **SSVS**: two-component Normal mixture (small vs large variance) → probabilistic inclusion.
  – **Dirichlet–Laplace / Spike–Slab**: alternative sparse families.

• Intuition
  – If data say a coefficient is irrelevant → local λ shrinks it hard (nearly zero).
  – If it’s important → local λ inflates variance so it **survives** the global shrinkage.

• Benefits
  – **Adaptive sparsity**: different coefficients get different shrinkage automatically.
  – Protects real signals, reduces noise → cleaner IRFs and better forecasts in large k,p systems.

• Practical use
  – Keep Minnesota-style scaling (lag decay, σ_i/σ_j), replace group tightness with τ + λ_{ijℓ}.
  – Set τ via hierarchical prior or tune by forecast CV / marginal likelihood.
  – Monitor MCMC mixing (global–local can be slower); standardize variables.

• One-liner
  – “Global–local shrinkage = **one big knob** for overall parsimony and **many tiny knobs** that let true effects breathe.”

- Spike-and-Slab vs. Continuous Shrinkage: One approach is spike-and-slab (e.g. Stochastic Search Variable Selection) which puts a mixture prior on each coefficient — a point-mass (spike at 0) with a certain probability and a diffuse distribution (slab) otherwise.

- Global-local continuous shrinkage priors (like the Horseshoe or Normal-Gamma) achieve a similar effect without a discrete mixture their density is sharply peaked at zero (most coefficients a priori very small) but with heavy tails (allowing a few large coefficients if supported by data).

  
---
#  Adaptive Hierarchical Priors (AHPs)
&lt;div style="font-size: 27px;"&gt;

--

- The AHPs extend the classic Minnesota-style shrinkage used in Bayesian VARs by letting the data (rather than the researcher) determine how tightly each coefficient is shrunk.

--

- AHPs use a hierarchical structure where the prior variances of coefficients are not fixed but adapt based on the data, allowing for more flexibility in how much shrinkage is applied to each coefficient.

--

- This approach allows for different degrees of shrinkage across coefficients, depending on their estimated importance or relevance in the model.

--

- AHPs are particularly useful in situations where the researcher is uncertain about the appropriate level of shrinkage or when the data suggest that some coefficients should be shrunk more than others.

- Giannone et al. propose treating `\(\lambda\)` (and related hyper-parameters)  as an unknown and sampling them from a hyper-prior; the posterior therefore “learns” how much global shrinkage is optimal for the particular data-set and model dimension. 

???
# Adaptive Hierarchical Priors (AHPs) — presenter notes

• Motivation
  – Move beyond “one-size-fits-all” Minnesota tightness.
  – Let the **data learn** how much shrinkage each group/coef needs.

• What “hierarchical” means
  – Put priors on the **shrinkage parameters themselves** (hyperparameters).
  – Example structure (schematic):
     $ β | Σ, τ, λ  ~  N(0, Σ ⊗ D(τ^2 · λ_j^2 · s_j))$
        • $τ $= global tightness (how sparse overall),
        • `\(λ_j\)` = local, coefficient- or group-specific tightness,
        • `\(s_j\)` = scaling (lag decay, σ_i/σ_j) as in Minnesota.

• How it adapts
  – If data suggest a coefficient matters → its λ_j increases ⇒ weaker shrinkage.
  – If not → λ_j shrinks it hard toward 0.
  – Global τ balances overall parsimony vs flexibility.

• Estimation (intuition)
  – Gibbs sampler cycles:
      1) draw (β, Σ) | current τ, λ, data
      2) draw τ and λ from their hyper-priors | β, Σ, data
  – Posterior “learns” tightness levels automatically.

• When to use
  – Many variables/lags with heterogeneous persistence.
  – You’re unsure about how tight to shrink; want **data-driven** regularization.

• Benefits
  – **Adaptive sparsity**: keeps real signals, suppresses noise.
  – More robust than fixed Minnesota when dynamics differ across variables.
  – Often improves forecasts and IRFs in large BVARs.

• Practical tips
  – Keep Minnesota-style scaling (lag decay, σ_i/σ_j) inside s_j.
  – Use weakly-informative hyper-priors (e.g., half-Cauchy/IG) for τ, λ.
  – Standardize variables; check sensitivity and mixing.
  – Cite/anchor: Giannone–Lenza–Primiceri (2015) style hierarchical BVARs.

• One-liner
  – “AHPs let the **model learn its own shrinkage**: one global knob + many local knobs, tuned by the data.”

---
#  Adaptive Hierarchical Priors (AHPs)
&lt;div style="font-size: 27px;"&gt;

--

- Subsequent work (e.g. Huber &amp; Feldkircher’s Normal-Gamma, Follett &amp; Yu’s Horseshoe) nests the hierarchical Minnesota inside a richer global–local scheme:

`$$\beta_j \mid \lambda_j, \tau \sim \mathcal{N}(0, \tau^2 \lambda_j^2 \sigma^2), \quad \lambda_j^2 \sim \text{Local prior}, \quad \tau^2 \sim \text{Global prior}$$`

#### Where:
- `\(\beta_j\)`: Regression coefficient for the `\(j\)`-th predictor.
- `\(\lambda_j^2\)`: Local shrinkage parameter that allows individual coefficients to escape global shrinkage (local prior).
- `\(\tau^2\)`: Global shrinkage parameter that controls the overall degree of shrinkage applied to all coefficients (global prior).
- `\(\sigma^2\)`: Variance of the model error term.
- `\(\mathcal{N}(0, \cdot)\)`: Normal distribution with mean 0 and specified variance.

???
# Adaptive Hierarchical Priors (AHPs) — presenter notes

• What this slide adds
  – We embed Minnesota-style scaling in a **global–local** scheme (Normal–Gamma, Horseshoe, etc.).
  – Each coefficient gets its own *local* shrinkage and shares a *global* shrinkage.

• Equation walkthrough
  – β_j | λ_j, τ  ~  N(0,  τ^2 λ_j^2 σ^2)
      • **τ (global)**: overall tightness/sparsity across all coefficients.
      • **λ_j (local)**: coefficient-specific “release valve” that can loosen/tighten shrinkage.
      • **σ^2**: model error variance (scale factor).
  – λ_j^2  ~ Local prior  (e.g., Gamma for Normal–Gamma, half-Cauchy for Horseshoe).
  – τ^2     ~ Global prior (e.g., Gamma or half-Cauchy).

• Intuition
  – If data suggest β_j is small → λ_j ↓ ⇒ **strong shrinkage** (β_j ≈ 0).
  – If β_j is important → λ_j ↑ ⇒ **escapes** global shrinkage (retains signal).
  – Heavy-tailed local priors (Horseshoe) kill noise but spare big signals.

• Why this helps
  – Minnesota uses one level of tightness per group; AHPs **adapt** per coefficient.
  – Better forecast performance and cleaner IRFs when signals are sparse/heterogeneous.

• Implementation sketch (Gibbs)
  – Draw β | (λ, τ, σ^2, data)  — Gaussian.
  – Draw local λ’s | (β, τ, σ^2)  — from their conditionals.
  – Draw τ | (β, λ, σ^2)         — from its conditional.
  – Keep Minnesota **scaling** (lag decay, σ_i/σ_j) inside σ^2 or an extra s_j factor.

• Practical tips
  – Standardize variables; use weakly-informative hyperpriors.
  – Tune via forecast CV or marginal likelihood; check mixing and sensitivity.

• One-liner
  – “AHPs let the **model learn its own shrinkage**: one global knob (τ) for parsimony and many local knobs (λ_j) so true effects can breathe.”

---
#  Adaptive Hierarchical Priors (AHPs)
&lt;div style="font-size: 27px;"&gt;

--

- This structure allows **adaptive shrinkage**: coefficients that are likely irrelevant are shrunk more (via `\(\tau^2\)`), while those with strong signals are allowed to escape shrinkage through larger `\(\lambda_j^2\)` values.

--

- The AHPs are particularly effective in improving forecast accuracy and impulse response estimation in Bayesian VARs, as they allow for a more data-driven approach to shrinkage.

--

- They are implemented in various Bayesian VAR software packages, allowing researchers to easily apply them to their data without needing to manually specify the prior distributions for each coefficient.

--

- AHPs are a powerful tool for Bayesian VAR modeling, providing a flexible and adaptive framework for incorporating prior information while allowing the data to guide the estimation process.

???
# Adaptive Hierarchical Priors (AHPs) — presenter notes

• What “adaptive” means:
  – We have a **global** tightness τ governing overall shrinkage and **local** weights λ_j for each coefficient.
  – Small τ ⇒ strong shrinkage everywhere; large λ_j² lets important coefficients **escape** that shrinkage.
  – Net effect: irrelevant β’s collapse toward 0; relevant ones remain flexible.

• Why it helps:
  – More **data-driven** than fixed Minnesota: shrinkage strength adjusts to signal strength.
  – Typically improves **forecast accuracy** and yields cleaner, less noisy **IRFs**, especially in big systems or short samples.

• Practical angle:
  – Available in several BVAR toolkits; you usually supply weakly informative hyper-priors and the sampler learns τ and {λ_j}.
  – Keep Minnesota-style scaling (lag decay, σ_i/σ_j) so units and volatility are handled properly.

• Take-away line:
  – “AHPs give you **one global knob** for parsimony and **many local knobs** so true effects can breathe—letting the data guide the amount of shrinkage.”


---
# Normal-Gamma (NG) Prior
&lt;div style="font-size: 20px;"&gt;

- The Normal-Gamma (NG) prior is a flexible global-local shrinkage prior that combines a normal distribution for the coefficients with a gamma distribution for their variances.

--

- The NG prior is defined as:

 `$$\beta_{ij} \sim N(0, \tau^2 \sigma^2_{ij}), \quad \sigma^2_{ij} \sim \text{Gamma}(a, b)$$`

- Where:
  - `\(\beta_{ij}\)` is the coefficient for variable `\(i\)` at lag `\(j\)`.
  - `\(\tau^2\)` is a global shrinkage parameter that controls the overall tightness of the prior.
  - `\(\sigma^2_{ij}\)` is a local variance parameter for each coefficient, allowing for different degrees of shrinkage across coefficients.
  - `\(a\)` and `\(b\)` are hyperparameters of the gamma distribution, controlling the shape and scale of the local variances.
  
- The NG prior allows for flexible shrinkage, where some coefficients can be very small (shrunk towards zero) while others can be larger, depending on the data.

- The NG prior is particularly useful in high-dimensional settings where the number of coefficients is large, as it allows for adaptive shrinkage based on the data.

???
# Normal–Gamma (NG) Prior — presenter notes

• What it is:
  – A **global–local shrinkage** prior: each coefficient β_{ij} is Normal with a variance
    that’s multiplied by a *local* Gamma–distributed factor; a single *global* τ controls
    overall tightness. Formally, one common form is
      β_{ij} | ψ_{ij}, τ  ~  N(0, τ² ψ_{ij}),   ψ_{ij} ~ Gamma(a, b),
    (equivalently some papers place the Gamma on the **precision**). :contentReference[oaicite:0]{index=0}

• Intuition:
  – **Global τ** shrinks everything toward zero (parsimony).  
  – **Local ψ_{ij}** lets important coefficients “escape” shrinkage (adaptive).  
  – The NG mixture produces a prior that is **peaked at zero** with **heavy tails**:
    irrelevant β’s are pulled hard to 0, while true signals are spared. :contentReference[oaicite:1]{index=1}

• Why use it in BVARs:
  – In large VARs (many k, p), NG priors improve forecasts and IRFs by targeting sparsity
    while allowing a few sizable effects. Practical VAR implementations with stochastic
    volatility and NG priors are available and documented. :contentReference[oaicite:2]{index=2}

• How to read the slide equation:
  – If you see  β_{ij} ~ N(0, τ² σ²_{ij}) and  σ²_{ij} ~ Gamma(a, b),
    treat σ²_{ij} (or its inverse) as the **local scale/precision**. Different authors
    parameterize the Gamma on variance or precision; the global–local logic is the same. :contentReference[oaicite:3]{index=3}

• Practical tips:
  – Standardize variables; keep Minnesota-style scaling (e.g., lag decay, σ_i/σ_j) if desired.
  – Put weakly-informative hyperpriors on τ and Gamma(a, b); tune via OOS forecasts or
    marginal likelihood; check MCMC mixing (global–local priors can mix more slowly). :contentReference[oaicite:4]{index=4}

• One-liner to close:
  – “NG = one global knob for overall sparsity + a local knob per coefficient so real
    signals can breathe—widely used and effective in high-dimensional BVARs.” :contentReference[oaicite:5]{index=5}

---
# Normal-Gamma (NG) Prior

&lt;div style="font-size: 25px;"&gt;

--

- The NG prior is conjugate, meaning that the posterior distribution of the coefficients given the data can be computed analytically.

--

- The posterior distribution of the coefficients under the NG prior is also a normal distribution, which allows for efficient sampling using Gibbs sampling or other MCMC methods.

--

- The NG prior can be combined with other priors, such as the Minnesota prior, to further improve estimation accuracy and control overfitting.

--

- The NG prior is particularly useful in large VAR models where the number of coefficients is large, as it allows for flexible shrinkage and improves estimation accuracy.

--

- The NG prior can also be used in conjunction with other Bayesian methods, such as empirical Bayes or hierarchical modeling, to further improve estimation accuracy and control overfitting.

???
# Normal–Gamma (NG) Prior — presenter notes

• Key property — (conditionally) conjugate:
  – Under the NG setup, the conditional posteriors needed for Gibbs sampling are available in closed form, making BVAR estimation fast and stable. :contentReference[oaicite:0]{index=0}

• What the sampler draws:
  – Coefficients are conditionally Normal with NG-driven local scales; this mixture yields a spike-at-zero with heavy tails (shrinks noise hard, lets signals survive). :contentReference[oaicite:1]{index=1}

• Why use NG in large VARs:
  – In high-dimensional VARs, NG shrinkage improves forecast performance and keeps IRFs well-behaved, with successful macro applications documented. :contentReference[oaicite:2]{index=2}

• Relation to other priors / frameworks:
  – NG is one member of the global–local class (alongside horseshoe, etc.) and can be embedded in hierarchical BVARs or paired with Minnesota-style scaling when desired. :contentReference[oaicite:3]{index=3}

• Practical message:
  – Start from NG for adaptive sparsity; tune hyperparameters (global vs local) or let a hierarchical layer learn tightness from the data; evaluate by out-of-sample forecasts. :contentReference[oaicite:4]{index=4}

• One-liner:
  – “NG gives you closed-form Gibbs updates and adaptive shrinkage—ideal for big BVARs where we need speed and robustness.” :contentReference[oaicite:5]{index=5}

---
# Minnesota-Type Normal-Gamma Prior
&lt;div style="font-size: 25px;"&gt;

--

- Hybrid Prior: Recent work (Chan, 2021) combines the best of both worlds by embedding the NG prior into a Minnesota-style framework

--

- This allows for the intuitive shrinkage structure of Minnesota while retaining the flexibility of NG.

--

-  Each coefficient `\(\beta_{i,j,\ell}\)` has:

  - a Minnesota variance scale `\(C_{i,j,\ell}\)` (as if it had a Minnesota prior, accounting for own vs cross and lag `\(\ell\)`),
  
  - its own local shrinkage `\(\psi_{i,j,\ell} \sim \Gamma(\nu_\psi,;\nu_\psi/2)\)`,
  
  - and separate global scales `\(\kappa_1\)` and `\(\kappa_2\)` for own-lag vs cross-lag coefficients respectively

???
# Minnesota-Type Normal–Gamma Prior — presenter notes

• What it is (hybrid idea)
  – Combines **Minnesota-style scaling** with a **Normal–Gamma (NG) global–local** prior.
  – Keep the intuitive Minnesota structure (own-lag vs cross-lag, lag-decay, scale by σ’s), but add NG’s adaptive local shrinkage so important coefficients can “escape” heavy shrinkage. :contentReference[oaicite:0]{index=0}

• How it’s set up (speak to the form, not derive)
  – For coefficient `\((\beta_{i j,\ell})\)`:
    • Minnesota variance scale `\((C_{i,j,\ell})\)` (encodes lag-decay and `\((σ_i/σ_j)\)` scaling).  
    • Local scale `\(( \psi_{i,j,\ell} \sim \mathrm{Gamma}(\nu_\psi, \nu_\psi/2) )\)` (lets the data relax/tighten shrinkage per coefficient).  
    • Separate global scales `\(( \kappa_1) \ and \ ( \kappa_2 )\)` for **own-lags** vs **cross-lags** (diagonal dominance).  
  – In Chan’s slides: `\(( (\theta_{i,j}\mid \kappa_1,\kappa_2,\psi_{i,j}) \sim \mathcal N(m_{i,j},\, 2\,\kappa_{i,j}\,\psi_{i,j}\,C_{i,j}) )\)` with `\(( \psi_{i,j}\sim \Gamma(\nu_\psi,\nu_\psi/2) )\)`; setting `\(( \nu_\psi=1 )\)` recovers a Lasso-type shrinkage. :contentReference[oaicite:1]{index=1}

• Intuition to emphasize
  – **Global** (κ’s): set overall tightness; own-lags can be looser than cross-lags.  
  – **Local** (ψ’s): coefficient-specific “release valves” → small/irrelevant β’s are crushed toward 0; real signals get room.  
  – **C scales**: keep Minnesota’s practical lag-decay and unit-scaling so the prior is well-behaved across variables. :contentReference[oaicite:2]{index=2}

• Why use it
  – Empirically, the Minnesota-type NG prior often **outperforms** a standard NG and a data-based Minnesota prior for forecasting in large VARs; it strikes a better bias–variance trade-off. (IJF 2021). :contentReference[oaicite:3]{index=3}

• Position in the toolbox
  – Think of it as “**Minnesota + adaptivity**”: same transparent structure, upgraded with global–local learning.  
  – Related NG/global-local priors (Normal-Gamma, Horseshoe) are standard in large BVARs; this variant nests their advantages while preserving Minnesota’s interpretability. :contentReference[oaicite:4]{index=4}

• Practical tips (quick bullets)
  – Keep Minnesota scaling for lag-decay and σ-normalization; tune (or place hyperpriors on) `\(( \kappa_1,\kappa_2,\nu_\psi )\)`.  
  – Evaluate tightness via out-of-sample forecasts / marginal likelihood; check IRF robustness. :contentReference[oaicite:5]{index=5}

---
# Minnesota-Type Normal-Gamma Prior
&lt;div style="font-size: 25px;"&gt;

- The prior becomes:

`$$\beta_{i,j,\ell} \sim N\left(0, \frac{C_{i,j,\ell}}{\kappa_1^{\ell-1} + \kappa_2^{\ell-1}} \psi_{i,j,\ell}\right)$$`

- where `\(\kappa_1\)` controls shrinkage for own lags and `\(\kappa_2\)` for cross lags.

- The local `\(\psi_{i,j,\ell}\)` allows each coefficient to adaptively shrink based on its importance, while the global `\(\kappa_{g(i,j)}\)` controls overall tightness.

- Essentially, we still give each coefficient a Minnesota-like prior variance `\(C_{i,j,\ell}\)`, but instead of a fixed normal prior, we make it an NG prior (with local `\(\psi\)` and appropriate global scale `\(\kappa_{g(i,j)}\)`).

- This allows for flexible shrinkage while still imposing the Minnesota structure, enabling better estimation in large VARs.


???
# Minnesota-Type Normal–Gamma Prior — presenter notes

• What this slide is showing:
  – Each coefficient β_{i,j,ℓ} has a **Normal** prior with variance
    `\(( \displaystyle \frac{C_{i,j,\ell}}{\kappa_1^{\ell-1}+\kappa_2^{\ell-1}} \,\psi_{i,j,\ell} )\)`.
    This is a **hybrid**: Minnesota-style scaling \(C_{i,j,\ell}\) + NG global–local shrinkage (ψ plus κ’s). :contentReference[oaicite:0]{index=0}

• Pieces to explain (quick):
  – `\((C_{i,j,\ell})\)`: Minnesota variance scale (captures lag decay and σ-normalization; own vs cross treated differently). Keeps the familiar “own-lag flexible, higher/cross lags tighter” structure. 
  
  – `\((\kappa_1, \kappa_2)\)`: **global** tightness for own-lag vs cross-lag blocks. Exponent `\((\ell-1)\)` gives automatic lag-decay: larger ℓ ⇒ bigger denominator ⇒ **smaller variance ⇒ stronger shrinkage**. 
  
  – `\((\psi_{i,j,\ell})\)`: **local** scale (from a Gamma/NG component) that lets important coefficients escape the global shrinkage; irrelevant ones get pulled hard toward 0. :contentReference[oaicite:3]{index=3}

• Intuition in one breath:
  – Minnesota provides **structure** (diagonal dominance + lag decay), NG adds **adaptivity** (global–local). Together: first own lags can breathe, distant/cross lags are tight **unless the data demand otherwise**. 
  
• Why use this hybrid:
  – In large VARs it often forecasts as well as or better than stand-alone Minnesota or stand-alone NG because it balances bias–variance with data-driven flexibility. (See Chan’s IJF paper/slides for empirical evidence and a fast Gibbs sampler.) 

• One-liner to close:
  – “Same transparent Minnesota scaling, but with NG’s global–local ‘release valves’—structured **and** adaptive shrinkage for big BVARs.” 

---
# Example BVAR Model
&lt;div style="font-size: 23px;"&gt;
--

- To illustrate Bayesian VAR methods using some of the priors and methods described above, we use a quarterly US data set on the inflation
rate `\(\Delta \pi_t\)` (the annual percentage change in a chain-weighted GDP price index), the unemployment rate `\(u_t\)` (seasonally adjusted civilian unemployment rate, all workers over age 16) and the interest rate rt (yield on the three month Treasury bill rate). Thus `\(y_t\)` = `\((∆π_t,u_t,r_t)\)`. The sample runs from 1953Q1 to 2006Q3. 


``` r
library(BVARWGC)
library(vars)
library(tsaccessories)
library(readxl)
data&lt;-read_excel("datawork/data.xlsx", sheet = "Sheet1")
data_qt &lt;- tsconvert(data, start = c(1953, 1), period = "quarterly")
tsplot(
  data = data_qt,
  vars = c("inflation", "unemployment_rate", "interest_rate"),
  freq = "quarterly",
  start_date = "1953-01-01"
)

tspanel_plot(
  data = data_qt,
  vars = c("inflation", "unemployment_rate", "interest_rate"),
  freq = "monthly",
  start_date =  "1953-01-01"
)

# Panel plot
panel_plot &lt;- tspanel_plot(
  data = data_qt,
  vars = c("inflation", "unemployment_rate", "interest_rate"),
  freq = "monthly",
  start_date =  "1953-01-01"
)
print(panel_plot)
```

---
# Example BVAR Model
&lt;div style="font-size: 23px;"&gt;

![](Main-Slides_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;


---
# Example BVAR Model
&lt;div style="font-size: 20px;"&gt;

.pull-left[

``` r
y &lt;- data_qt[, c("inflation", "unemployment_rate", "interest_rate")]
# Use VARselect to choose lag length
lag_selection &lt;- VARselect(y, lag.max = 4, type = "const")
# The suggested lag orders (per criterion: AIC, HQ, SC, FPE)
lag_selection$selection
```

```
## AIC(n)  HQ(n)  SC(n) FPE(n) 
##      3      2      2      3
```

``` r
#Minnesota
fit &lt;- BVARWGC::bvar(data_qt, y_vars = c("inflation", "unemployment_rate", "interest_rate"),
             p = 2, prior = "mn", backend = "auto", draws = 8000, burn = 4000)
```
]


.pull-right[

``` r
bvarsum(fit, holdout = 8)   
postbvar(fit, lb_lags = 12, arch_lags = 2)
bvarplots(fit, show = c("mcmc","residuals","acf","qq","roots","fit"))

bvarirf(fit, H = 20, method = "cholesky", level = 0.90, plot = TRUE)
bvarfevd(fit, H = 10, method = "cholesky", plot = TRUE)
```

]

---
# Example BVAR Model
&lt;div style="font-size: 23px;"&gt;



---
# Example BVAR Model
&lt;div style="font-size: 25px;"&gt;
- Interpretation of Posterior Medians of Coefficients (from bvarsum):
- **Inflation equation**
  - Strongly persistent: lag(1) inflation = 1.451 (inflation today depends heavily on past inflation).
  - Negative lag(2) inflation = -0.455 (dampening effect after two periods).
  - Some small positive effect from unemployment (lag 2).
- **Unemployment equation**
  - Strong persistence: lag(1) unemployment = 1.262.
  - Lag(2) unemployment = -0.362 (mean-reverting).
- **Interest rate equation**
  - Lag(1) interest rate = 0.817 (very persistent).
  - Inflation (lag 1) raises interest rates (0.466) → consistent with monetary policy reaction.
  - Unemployment (lag 1) lowers rates (-0.531) → possible policy easing during high unemployment.

---
# Example BVAR Model
&lt;div style="font-size: 25px;"&gt;
- Overall, the model captures key macroeconomic dynamics:
  - Inflation persistence with some mean-reversion.
  - Unemployment persistence and mean-reversion.
  - Interest rates responding to inflation and unemployment, consistent with monetary policy behavior.
- The Minnesota prior effectively shrinks many coefficients toward zero, reducing overfitting and improving forecast stability.

- Interpretation of Variance-Covariance Matrix (Shocks)
  - Diagonal elements show the variance of shocks to each variable.
  - Off-diagonal elements show covariances, indicating how shocks to one variable relate to shocks in another.
   - Inflation shock variance = 0.099 (moderate).
   - Interest rate shocks have the largest variance (0.583) → more volatile.
   - Negative covariance between unemployment &amp; interest rate (-0.092) → when unemployment shocks rise, interest rate shocks tend to fall.

---
# Example BVAR Model
&lt;div style="font-size: 25px;"&gt;
.pull-left[

``` r
postbvar(fit, lb_lags = 2, arch_lags = 2)
```

```
## $lb_pvalue
##         inflation unemployment_rate     interest_rate 
##        0.59184337        0.38157864        0.08175051 
## 
## $jb_pvalue
##         inflation unemployment_rate     interest_rate 
##      0.0004331099      0.0000000000      0.0000000000 
## 
## $arch_pvalue
##         inflation unemployment_rate     interest_rate 
##      3.643603e-04      5.179785e-01      4.249538e-11 
## 
## $roots_mod
## [1] 0.9509335 0.9509335 0.7036067 0.7036067 0.3724826 0.1218078
## 
## $is_stable
## [1] TRUE
## 
## attr(,"class")
## [1] "bvar_diagnostics"
```
]

.pull-right[
- Ljung–Box (Serial Correlation) - No Serial Correlation Detected

- Jarque–Bera (Normality) - Residuals Appear not to be Normally Distributed

- ARCH Test (Conditional Heteroskedasticity) - Evidence of ARCH Effects Detected except for unemployment equation.

- Stability - All Roots Inside Unit Circle, VAR is Stable
]

---
# Example BVAR Model
&lt;div style="font-size: 15px;"&gt;
.pull-left[
&lt;img src="img/density.png" width="450"/&gt;
]


.pull-right[
**Overall MCMC Diagnostics**
- ***Mixing:*** Both parameters show chains that move freely → good evidence of convergence.
- ***Stationarity:*** No drifts or trends → the chain has reached equilibrium.
- ***Posterior densities:*** Unimodal and smooth → no multimodality problems.
- ***Effective sample size:*** Since you had ~3870 accepted draws, you likely have enough effective samples for inference.
- The MCMC diagnostics indicate that the chains for both the marginal likelihood proxy (ml) and the shrinkage hyperparameter (λ) mix well and converge to stable distributions. The posterior distribution of λ is centered around 0.6, consistent with the optimized hyperparameter values reported earlier. This suggests moderate shrinkage of the Minnesota prior, providing a balance between parsimony and data-driven flexibility. The trace and density plots provide no evidence of non-convergence or multimodality.
]

---
# Example BVAR Model
&lt;div style="font-size: 20px;"&gt;

.pull-left[
.center[&lt;img src="img/irf.png" width="800"/&gt;]

- IRF shows that inflation shocks generate strong and persistent increases in both interest rates and unemployment, consistent with a monetary tightening response.
]

.pull-right[

- Interest rate shocks reduce inflation but raise unemployment, highlighting the trade-off faced by policymakers.

- Unemployment shocks, by contrast, are highly persistent and deflationary, leading to countercyclical monetary easing. 

- Together, these results confirm the presence of a Phillips curve relationship and a systematic Taylor-rule type policy response, while also underscoring the rigidity of unemployment dynamics relative to inflation and interest rates.
]
---
# Example BVAR Model
&lt;div style="font-size: 18px;"&gt;

.pull-left[
.center[&lt;img src="img/fevd.png" width="800"/&gt;]

- Inflation is largely self-driven, though unemployment shocks account for up to 15% of its forecast variance at longer horizons.

]

.pull-right[
- Interest rates, by contrast, become increasingly explained by inflation and unemployment shocks over time, consistent with systematic policy responses to macroeconomic conditions.
- Unemployment remains mostly self-driven, with only modest contributions from inflation and interest rate shocks.
- These findings highlight the persistence of unemployment, the self-reinforcing nature of inflation, and the endogenous nature of monetary policy.
- Overall, the FEVD results highlight the dominant role of inflation shocks in driving interest rate dynamics, while unemployment appears more insulated from shocks to the other variables. 
- This suggests that monetary policy primarily reacts to inflation developments, while unemployment follows a more persistent and less responsive path.
]

---
# FA-BVAR
&lt;div style="font-size: 27px;"&gt;

--

- FA-BVAR combines the strengths of factor models and Bayesian VARs, allowing for the incorporation of a large number of variables while maintaining a parsimonious representation.


--

- FA-BVAR models are designed to handle large datasets by extracting a few unobserved "factors" (common underlying forces) that explain the co-movement of many macroeconomic variables.


--

- It builds on Bernanke et al. (2005)’s FAVAR by incorporating Bayesian shrinkage (e.g., Minnesota prior) to control overfitting.

--

- The model assumes that the observed variables can be expressed as a linear combination of a small number of latent factors and idiosyncratic errors.

--

- Basically the FA-BVAR model replaces a high-dimensional dataset `\(X_t\)` with a smaller set of latent factors `\(F_t\)` that capture the common variation in the data, allowing for more efficient estimation and forecasting of the VAR. `\([Y_t^{'}, F_t^{'}]\)` model. 


???
The FA-BVAR model is a powerful tool for analyzing large datasets in macroeconomics, as it allows researchers to focus on the underlying factors driving the data rather than being overwhelmed by the sheer number of variables. By combining factor analysis with Bayesian VAR techniques, it provides a flexible and robust framework for understanding complex economic relationships.


 Imagine trying to predict the weather. A standard VAR might use temperature, humidity, and wind speed for a single location. A BFAVAR would be like also incorporating data from hundreds of weather stations globally, but summarizing that global data into a few "climate factors" before feeding them into your local prediction model. This provides a much richer information set without making your local model unwieldy.

# FA–BVAR — presenter notes

• What it is
  – **Factor-Augmented Bayesian VAR (FA-BVAR)** = a VAR for a small set of target variables (Y*) that is **augmented with a few latent factors (F_t)** extracted from a large dataset (X_t). It blends FAVAR’s factor dimension reduction with BVAR’s shrinkage. :contentReference[oaicite:0]{index=0}

• Why factors?
  – A handful of **common factors summarize co-movement** in many macro series (e.g., activity, prices, credit). Using F_t captures broad information without exploding parameters. This is the standard rationale from dynamic factor models. :contentReference[oaicite:1]{index=1}

• Basic structure (talk it, no derivations)
  – Measurement: large panel loads on factors,  `\((X_t \approx \Lambda F_t + e_t)\)`.
  – State/VAR: stack the factors with key observables and run a VAR on `\(([F_t',\, Y_t^{*'}]')\)`.
  – Estimation is typically two-step (PCA for F_t, then VAR) or fully Bayesian (state-space + Gibbs). :contentReference[oaicite:2]{index=2}

• Why *Bayesian* here?
  – With many predictors, **shrinkage priors** (e.g., Minnesota) stabilize coefficients and improve forecasts; FA-BVAR combines this shrinkage with factor compression. Tune tightness to cross-sectional size for large systems. :contentReference[oaicite:3]{index=3}

• Practical payoff
  – Works well in **data-rich environments**: factors bring in broad information; Bayesian shrinkage prevents overfitting → cleaner IRFs and better OOS forecasts in large VARs. :contentReference[oaicite:4]{index=4}

• One-liner to close
  – “FA-BVAR = **FAVAR’s information content + BVAR’s regularization**—compact, stable, and forecast-oriented for big macro datasets.” 
 
 
---
# FA-BVAR Model - What it seeks to Solve
&lt;div style="font-size: 27px;"&gt;

--

- It was designed to overcome the limitations of traditional BVAR models by incorporating rich datasets via latent factors.

--

- FA-BVAR was originally introduced by Bernanke, Boivin &amp; Eliasz (2005) and later extended by Giannone, Lenza &amp; Primiceri (2015).

--

- Standard VAR models are flexible but overparameterized when many variables; high variance in estimates.

--

- Dynamic Factor Models (DFM) reduce dimension but may ignore dynamics in observables

--

- FA-BVAR combines the strengths of both approaches, allowing for a parsimonious representation of large datasets while capturing dynamic relationships.

???
# FA–BVAR Model — What it seeks to solve (presenter notes)

• Motivation:
  – Standard VAR/BVARs explode in parameters as we add variables and lags → high estimation variance / overfitting (the “curse of dimensionality”). Bayesian shrinkage helps but can still struggle when k is large. :contentReference[oaicite:0]{index=0}

• Data-rich insight:
  – Many macro series move together because a few **latent factors** summarize the common variation; Dynamic Factor Models use those factors to reduce dimension. :contentReference[oaicite:1]{index=1}

• FAVAR building block:
  – Bernanke, Boivin &amp; Eliasz (2005) show how to **augment a VAR with estimated factors** from a large dataset—bringing broad information into a compact VAR. This is the core idea FA-BVAR builds on. :contentReference[oaicite:2]{index=2}

• FA-BVAR idea:
  – Combine the factor augmentation of FAVAR with **Bayesian shrinkage** on the VAR coefficients: run a VAR on \([F_t, Y_t^*]\) and use priors to control overfitting. Hyperparameters can be chosen/data-driven as in Giannone–Lenza–Primiceri (2015). :contentReference[oaicite:3]{index=3}

• What it solves:
  – Compared with a big BVAR, FA-BVAR is **parsimonious** (few factors instead of hundreds of series) yet still captures **dynamic relationships** among the key observables and the broad macro environment. Empirically, factor augmentation plus shrinkage often improves forecasts. :contentReference[oaicite:4]{index=4}

• One-liner:
  – “FA-BVAR = FAVAR’s information compression + BVAR’s regularization—built to handle rich datasets without drowning in parameters.” 

---
# Comparison of VAR, BVAR, FAVAR, and BFAVAR

&lt;div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:400px; overflow-x: scroll; width:100%; "&gt;&lt;table class="table table-striped table-hover table-condensed table-responsive" style="font-size: 13px; width: auto !important; "&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;"&gt; Model Type &lt;/th&gt;
   &lt;th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;"&gt; Key Characteristics &lt;/th&gt;
   &lt;th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;"&gt; Primary Problem Addressed &lt;/th&gt;
   &lt;th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;"&gt; Advantages &lt;/th&gt;
   &lt;th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;"&gt; Disadvantages / Limitations &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; VAR &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Linear multivariate time-series model; captures joint dynamics; parameters are fixed values. &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; None (base model) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Flexible, atheoretical, easy to interpret for small systems. &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; "Curse of dimensionality" (over-parameterization), "omitted variable bias" for large systems, unstable estimates, poor forecasts with many variables. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; BVAR &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; VAR with Bayesian methods; parameters are random variables; uses informative priors for shrinkage. &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; "Curse of dimensionality" (over-parameterization) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Overcomes dimensionality, reduces parameter uncertainty, improves forecast accuracy, handles large systems (100+ variables). &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Can still suffer from omitted variable bias if key information is not in the included variables; prior choice can be subjective. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; FAVAR &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Extends VAR by incorporating latent factors extracted from a large dataset. &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; "Omitted variable bias" (limited information set) &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Incorporates large information sets parsimoniously, provides richer understanding of economic shocks, improves structural analysis. &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Two-step estimation can complicate inference (generated regressors); factors may lack clear economic interpretation without further restrictions. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; BFAVAR &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; FAVAR estimated using Bayesian methods; jointly estimates factors and parameters. &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Both "Curse of dimensionality" and "Omitted variable bias" &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Combines strengths of BVAR and FAVAR; robust inference by addressing uncertainty in factors &amp;amp; parameters; single-step estimation; superior density forecasts, particularly at short horizons; better for structural analysis with rich information. &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Computationally more intensive than two-step FAVAR; still requires careful prior elicitation and identification of factors. &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;

???
# Presenter notes for comparison table (VAR vs BVAR vs FAVAR vs BFAVAR)

• Set-up (what the audience is seeing)
  – A side-by-side of four cousins: **VAR**, **BVAR**, **FAVAR**, **BFAVAR**.
  – Rows = model families; columns = essence, what problem they tackle, pros/cons.

• VAR — baseline
  – **What it is:** plain multivariate regression in lags; parameters are fixed numbers.
  – **Strength:** simple, transparent, good for *small* systems.
  – **Weakness:** blows up in parameters as k×p grows → “curse of dimensionality”; can miss information not in Y (omitted-variable risk).

• BVAR — shrinkage version of VAR
  – **What changes:** treat coefficients as random; add **priors** to shrink toward reasonable values.
  – **Problem addressed:** over-parameterization; stabilizes estimates and forecasts in medium/large systems.
  – **Watch-outs:** if your information set is narrow, you can still have omitted-variable bias; priors must be chosen/tuned (λ, θ, etc.).

• FAVAR — factor-augmented VAR (two-step)
  – **What changes:** extract a few **latent factors** from a big panel X_t (PCA or similar), then augment a VAR for targets Y*.
  – **Problem addressed:** limited information set; factors summarize broad co-movement.
  – **Pros:** data-rich insight with few parameters; useful for structural analysis (impulse responses to broad shocks).
  – **Cons:** two-step “generated regressors” complicate inference; factors may lack clear economic labels without extra structure.

• BFAVAR — Bayesian FAVAR (single framework)
  – **What changes:** estimate **factors and VAR** jointly under Bayesian shrinkage.
  – **Problem addressed:** both parameter explosion and limited information.
  – **Pros:** coherent uncertainty, better density forecasts (esp. short horizons), richer structural analysis; avoids two-step pitfalls.
  – **Cons:** more computation; still need careful priors and identification for factors/shocks.

• Quick “when to use which” rule-of-thumb
  – **Small k, plenty of T:** VAR is fine.
  – **Medium/large k or short T:** **BVAR**.
  – **Need broad information beyond Y:** **FAVAR**.
  – **Big data + parsimony + full Bayesian inference:** **BFAVAR**.

• Closing line
  – “Think of BVAR as taming parameter growth; FAVAR as importing information; BFAVAR does **both at once** with a unified Bayesian treatment.”

---
# Motivation for FA-BVAR
&lt;div style="font-size: 27px;"&gt;

--

- Use FA-BVAR when you have a large number of variables, but limited observations (e.g., 100–200) and want to avoid overfitting.

--

- Use it when you want to incorporate a wide range of macroeconomic indicators without losing the ability to estimate dynamic relationships.

--

- FA-BVAR is highly appropriate for structural analysis, especially for identifying and assessing the dynamic effects of economic shocks, such as monetary policy shocks. Traditional VARs often suffer from omitted variable bias, which can lead to puzzling results like the "price puzzle" or "liquidity puzzle". 

--

- FAVAR is designed to handle large information sets while maintaining the dynamic interpretability of VAR models, overcoming the limitations of both standard VARs (dimensionality) and pure factor models (lack of dynamic interaction modeling). 

--

- It provides a more robust and informative framework for macroeconomic forecasting and structural analysis.


???
Point 1 &amp; 2. 

- FA-BVAR models are particularly well-suited for applications where researchers and policymakers need to analyze macroeconomic dynamics by leveraging a vast amount of information. 

- In contemporary economies, a multitude of indicators—ranging from detailed sectoral data and financial market variables to consumer sentiment indices—contain crucial information about the current state and future evolution of the economy. 

- BFAVAR's unique ability to process and incorporate the informational content of many time series, often hundreds, allows for a comprehensive understanding of complex interrelationships that would be intractable with smaller models. 

- This capacity is invaluable for institutions like central banks, which rely on a wide array of data to infer the state of the economy, understand the main forces driving economic movements, and make informed policy decisions. 

- By efficiently distilling information from a large number of variables, BFAVAR provides a coherent forecasting framework that can simultaneously forecast a larger set of core variables, offering a holistic view of the economy.



Point 3.
- By incorporating a rich information set through latent factors, FA-BVAR provides a more accurate and comprehensive picture of the monetary transmission mechanism.

- The Bayesian framework also allows for flexible identification strategies, such as sign restrictions on impulse response functions, which can be imposed on a large number of variables to more precisely pin down the impact of policy shocks. 

- This leads to more reliable and economically sensible impulse responses, as the larger information set helps to distinguish true structural shocks from model misspecification errors. 

- The ability of BFAVAR to provide robust structural analysis, particularly for monetary policy shocks, makes it a practical, operational tool for informing monetary and fiscal policy decisions, allowing for more nuanced and data-driven interventions. 



General 
- Modern economies have hundreds of time series (output, prices, employment, finance, etc.). .my-coral[*Factor-Augmented VAR*] is proposed to exploit this rich information.



- FAVAR use a few latent factors to summarize the co-movement in a large number of series. These factors, plus any key observed variables, enter a VAR. This .my-coral[*augmented VAR *]retains a manageable size but with a much *broader information set*.


- FAVAR models are designed to handle large datasets by extracting a few unobserved "factors" (common underlying forces) that explain the co-movement of many macroeconomic variables.

- FAVAR basically helps to:

  - reduce omitted variable bias by including common factors that capture many variables’ information, 
  
  - mitigate arbitrary proxy choices by effectively using multiple indicators for each concept, and 
  
  - produce impulse responses for any variable in the large dataset (via the factors)


Speaker Notes:
The Factor-Augmented VAR (FAVAR) was developed as a direct response to the limitations of small VARs. Instead of throwing out most of the data, why not use it intelligently? The idea is to leverage the fact that in a large dataset, many series move together due to common underlying drivers (for instance, dozens of economic activity indicators might all load on a “business cycle” factor). Rather than adding 100 series into a VAR (impossible to estimate), we compress the information into a few latent factors. A FAVAR model typically includes:

- A set of unobserved common factors F that summarize broad information (e.g., an “output factor,” “price factor,” “financial factor,” etc.), and

- Possibly some observed key variables Y (especially those we don’t want to treat as latent, like the policy interest rate which is often included as an observed factor).


These factors and observed variables together form an augmented state vector that follows a VAR. By doing this, we get a model that is roughly the same size as a traditional small VAR in terms of equations, but it implicitly includes the information from potentially hundreds of series through the factors. Bernanke et al.’s original FAVAR paper demonstrated that this approach indeed improved identification of monetary policy effects. They found that using the extra information from factors was important to properly uncover the monetary transmission mechanism and yielded a more comprehensive and coherent picture of policy effects


. Essentially, FAVAR resolved some puzzles: for example, the price puzzle was greatly reduced when a broad price index factor (incorporating commodity prices and other forward-looking prices) was included, as we’ll see later. It also allowed them to compute impulse responses for variables that were not in the original small VAR – because once you have estimated factors and factor loadings, you can infer how any included series responds. To sum up, FAVAR is an elegant solution that bridges large information sets with tractable VAR analysis: it retains the flavor of VAR (data-driven, relatively flexible) but with the informational richness of factor models. Next, we’ll dig a bit deeper into what these “factors” are and how we extract them, which falls under Dynamic Factor Models.




---
# Specification of the FA-BVAR Model
&lt;div style="font-size: 27px;"&gt;

--
#### FAVAR Structure
- FA-BVAR involves two broad set of variables 


  - ***.my-coral[Latent Factors]***: `\(F_t\)` (dimensionality `\(r\)`) Unobserved common factors extracted from a large dataset of macroeconomic variables.
  
  
  - ***.my-coral[A Vector of observed Variables]***: Key variables `\(Y_t\)` (dimensionality `\(m\)`)  that are directly included in the model (e.g., policy interest rates, inflation rates).

- The model assumes that the observed variables can be expressed as a linear combination of the latent factors and idiosyncratic errors.

--

- The latent factors `\(F_t\)` capture the common variation in the large dataset, while the observed variables `\(Y_t\)` provide additional information that is not captured by the factors alone.

--

- Together, `\(Z_t = [F_t^\prime,; Y_t^\prime]^\prime\)` (dimension `\(r+m\)`) follows a VAR.

???

# Specification of the FA-BVAR model — FAVAR structure (presenter notes)

• Big picture
  – We split information into **(i) latent factors F_t** from a large panel X_t and **(ii) a small vector of observed targets Y_t** (policy rate, inflation, etc.).
  – Then we run a **VAR on [F_t', Y_t']'** with Bayesian shrinkage on its coefficients.

• What are the factors F_t?
  – Unobserved “common drivers” (dimension r ≪ number of series N) extracted from X_t (e.g., via PCA or a dynamic factor model).
  – Measurement idea (say, not deriving):  X_t ≈ Λ F_t + e_t, where Λ are loadings and e_t are idiosyncratic noises.
  – Intuition: F_t captures the broad comovement in macro data (activity, prices, credit, external sector).

• Why keep Y_t as observed variables?
  – Y_t are the **policy-relevant targets** we care to forecast/interpret; they may contain information **beyond** the common factors (e.g., specific policy frictions).
  – Putting Y_t directly in the VAR with F_t improves interpretability of IRFs and policy analysis.

• Modeling assumption (speak it)
  – Each series can be written as **factor part + idiosyncratic error**.
  – The **latent factors summarize** most variation; **idiosyncratic components** are left to the shocks in the VAR.

• How FA-BVAR ties it together
  – Step 1 (conceptually): estimate F_t (PCA/DFM), choose r by standard criteria.
  – Step 2: estimate a **Bayesian VAR** on [F_t, Y_t] with shrinkage (e.g., Minnesota or global-local).
  – Fully Bayesian versions estimate {F_t, Λ, VAR parameters} **jointly** for coherent uncertainty.

• Practical notes to flag
  – Choose r small (parsimony) but large enough to capture cross-sectional variance.
  – Standardize X_t before extracting factors; include domain-specific blocks if needed.
  – Identification for structural analysis still required (Cholesky, sign, long-run, etc.).

• One-liner
  – “FA-BVAR = few latent factors for **breadth of information** + Bayesian VAR for **regularized dynamics** on the policy-relevant Y_t.”

Latent Factors  `\(F_t\)`

 
Definition: These are unobserved (hidden) common factors extracted from a large panel of macroeconomic time series data.

Purpose: Since macroeconomic datasets often contain hundreds of interrelated variables, factor models reduce this information into a smaller number of latent variables that capture the main dynamics.

Dimensionality: Denoted as r, where r≪N (N being the total number of macroeconomic indicators).

Estimation: Typically obtained using factor analysis or principal component analysis (PCA) applied to standardized macroeconomic data.

Example: A latent factor might represent a general “business cycle” trend that influences many observed macro variables such as GDP, employment, consumption, etc.


---
# Specification of the FA-BVAR Model
&lt;div style="font-size: 23px;"&gt;

#### Measurement Equation

$$X_t=\Lambda F_t + e_t, \ \ e_t \sim \mathcal{N}(0, \Psi) $$

- Where 
  - `\(X_t\)` is the `\(N\times1\)` vector of observed macroeconomic variables at time `\(t\)`.
  - `\(\Lambda\)` is the `\(N\times r\)` matrix of factor loadings, capturing how each variable relates to the latent factors.
  - `\(F_t\)` is the `\(r\times1\)` vector of latent factors at time `\(t\)`.
  - `\(e_t\)` is the `\(N\times1\)` vector of idiosyncratic errors, assumed to be normally distributed with mean zero and covariance matrix `\(\Psi\)`.
- The measurement equation captures how the observed variables `\(X_t\)` are linearly related to the latent factors `\(F_t\)` and their idiosyncratic errors `\(e_t\)`.
- The factor loadings `\(\Lambda\)` indicate the strength and direction of the relationship between each observed variable and the latent factors.
- Initial Factor Extraction
  - Before Bayesian estimation, factors `\(F_t\)` are often initialized via Principal Components (Stock–Watson) or EM algorithms; these provide starting values for the joint sampler
  
  
???
## Measurement Equation

&gt; **Presenter note (intuition first):**  
&gt; Think of a large orchestra:  
&gt; - The **latent factors** `\(F_t\)` play the role of the **conductor’s tempo and thematic motifs**, setting the underlying rhythm.  
&gt; - The **loadings** `\(\Lambda\)` describe **how each musician (observed series) follows** the conductor—some stick closely to the theme, others less so.  
&gt; - The **idiosyncratic errors** `\(e_t\)` are each musician’s **personal flourishes and timing quirks**, adding texture beyond the common beat.  
&gt;
&gt; This analogy captures how a handful of **hidden forces** can drive the collective performance of many observed variables.

&gt; **Presenter note (equation):**  
&gt; $$
&gt; X_t = \Lambda\,F_t + e_t,\quad e_t \sim \mathcal{N}(0, \Psi)
&gt; $$
&gt; Here, `\(X_t\)` is the **full panel** of `\(N\)` observed macro‑financial indicators at time `\(t\)`, such as GDP growth, inflation, credit spreads, and interest rates.

&gt; **Presenter note (`\(F_t\)`):**  
&gt; `\(F_t \in \mathbb{R}^r\)` is a **low‑dimensional vector of latent factors** (`\(r \ll N\)`).  
&gt; These factors capture the **unobserved common drivers**—for example, aggregate economic sentiment or financial conditions—extracted via PCA or factor analysis.

&gt; **Presenter note (`\(\Lambda\)`):**  
&gt; `\(\Lambda\)` is the `\(N\times r\)` **factor loading matrix**.  
&gt; Each element `\(\lambda_{ij}\)` measures **how strongly** observed variable `\(X_{i,t}\)` “loads on” factor `\(j\)`.  
&gt; A large magnitude indicates that `\(X_{i,t}\)` moves closely with `\(F_{j,t}\)`.

&gt; **Presenter note (`\(e_t\)` and `\(\Psi\)`):**  
&gt; `\(e_t\)` is the **idiosyncratic error vector**, assumed  
&gt; `$$e_t \sim \mathcal{N}(0, \Psi)$$`  
&gt; with `\(\Psi\)` **diagonal**. This implies each series `\(X_{i,t}\)` has its own residual variance `\(\psi_i\)`, and residuals are uncorrelated across series.

&gt; **Presenter note (putting it together):**  
&gt; - `\(\Lambda\,F_t\)` captures the **common component**—the part of `\(X_t\)` explained by **shared latent forces**.  
&gt; - `\(e_t\)` captures **series‑specific “noise”** or measurement error.  
&gt; When factors account for most of the shared dynamics, `\(\Psi\)` shrinks, leaving little idiosyncratic variance.

&gt; **Presenter note (advantages):**  
&gt; 1. **Dimensionality reduction:** We summarize `\(N\)` series with just `\(r\)` factors, making estimation tractable.  
&gt; 2. **Signal vs. noise separation:** Clear partition between common drivers and individual shocks.  
&gt; 3. **Forecasting &amp; policy analysis:** If `\(F_t\)` follows a VAR, we can trace how policy shocks propagate through the entire dataset.
• Intuition to emphasize:
  – Factors capture the **shared macro forces**; e_t absorbs **series-specific noise**.
  – If Ψ looks large or correlated, that’s a sign we may need **more factors** or a richer structure.

• Identification / normalization (one sentence):
  – Because factors/loadings are subject to **scale/rotation indeterminacy**, we impose simple constraints (e.g., Var(F_t)=I_r or orthogonal factors) — PCA enforces this automatically; in full Bayesian estimation we fix a normalization before sampling.

• Initial factor extraction (practical step):
  – Before joint Bayesian estimation, initialize F_t via **PCA (Stock–Watson)** or **EM**; these provide starting values and a sensible r (use ICs/scree plot/forecast performance).

• Good practice tips:
  – Remove means and standardize X_t before factor extraction.
  – Choose r small but sufficient to capture most variance; check robustness of r via forecasts/IRFs.
  – Missing data → EM or state-space PCA handles it cleanly.

• Bridge to the VAR part (preview):
  – After we have F_t, we stack \([F_t', Y_t']'\) and estimate a **BVAR** for the dynamics; today’s slide is only the **measurement** side that links the big panel to the factors.

---
# Specification of the FA-BVAR Model
&lt;div style="font-size: 23px;"&gt;
#### State Equation

`$$\underbrace{\begin{pmatrix} Y_t \\ F_t \end{pmatrix}}_{Z_t}= \sum_{i=1}^{p} A_i Z_{t-i} + u_t \quad u_t \sim N(0, \Sigma)$$`

- Where 
  - `\(Y_t\)` is the `\(m\times1\)` vector of observed variables (e.g., policy rates, inflation) at time `\(t\)`.
  - `\(F_t\)` is the `\(r\times1\)` vector of latent factors at time `\(t\)`.
  - `\(Z_t = [Y_t^\prime, F_t^\prime]^\prime\)` is the `\((m+r)\times1\)` vector of all variables at time `\(t\)`.
  - `\(A_i\)` is the `\((m+r)\times(m+r)\)` coefficient matrix for lag `\(i\)`.
  - `\(u_t\)` is the `\((m+r)\times1\)` vector of errors, assumed to be normally distributed with mean zero and covariance matrix `\(\Sigma\)`.
???
• Intuition:
  – Once we compress big data into F_t, we treat **[F_t, Y_t]** like a standard VAR(p).
  – Coefficients in A_i capture how today’s targets and factors depend on their own and each other’s lags.

• Role of Σ:
  – Σ governs contemporaneous co-movement of shocks across Y and F.
  – For forecasting, Σ delivers uncertainty; for structural analysis we’ll add an identification (e.g., Cholesky/sign restrictions).

• Bayesian layer (remind briefly):
  – Place shrinkage priors on vec(A_i) (e.g., Minnesota or global–local).
  – Prior on Σ (e.g., Inverse-Wishart or stochastic volatility) for stability and sensible uncertainty.

• Estimation flow (1 sentence):
  – Either two-step (PCA factors → BVAR on Z_t) or fully Bayesian joint estimation of {F_t, Λ, A_i, Σ} via Gibbs/EM; joint approach propagates factor uncertainty.

• Practical checks:
  – Choose lag order p (ICs/OOS forecasts).
  – Standardize variables; monitor stability (eigenvalues inside unit circle).
  – Inspect IRFs/forecast accuracy; tune shrinkage if dynamics look noisy.

- The state equation captures the dynamic relationships between the observed variables and the latent factors, allowing for lagged interactions and contemporaneous effects.

- The model assumes that the joint dynamics of the observed variables and latent factors can be described by a VAR process, where the coefficients `\(A_i\)` capture the relationships across all variables at different lags.

- The errors `\(u_t\)` represent the joint shocks to both the observed variables and the latent factors, allowing for contemporaneous correlations between them.

- The state equation is estimated jointly with the measurement equation to obtain the posterior distributions of the parameters and latent factors.

---
# Shrinkage Priors in FA-BVAR
&lt;div style="font-size: 27px;"&gt;

- In FA-BVAR, we place priors on the coefficients `\(A_i\)` and the covariance matrix `\(\Sigma\)` to control overfitting and improve estimation accuracy.

- These priors include 

  - Minnesota Prior : 
    - Coefficients for own lags are set to 1 (random walk belief), while others are shrunk toward zero.
  - Hierarchical Minnesota Prior
    - Coefficients are shrunk based on their lag length, with stronger shrinkage for longer lags.
  - Adaptive Hierarchical Priors (AHPs)
    - Coefficients are shrunk adaptively based on the data, allowing for different degrees of shrinkage across coefficients.

???
# Shrinkage Priors in FA-BVAR — presenter notes

• Framing:
  – In FA-BVAR we estimate a VAR on Z_t = [Y_t, F_t]. To prevent overfitting we put priors on the
    **state coefficients** A_i and on the **shock covariance** Σ.
  – Same logic as BVARs, but now the system includes **factors and observables**.

• What we shrink (intuition):
  – Allow **own first lags** to be relatively free (persistence), especially for factors and key Y’s.
  – **Cross-lags** (F → Y, Y → F, and across variables) get tighter shrinkage.
  – **Higher lags** get progressively tighter (lag-decay).

• Minnesota prior (baseline):
  – Means: own lag-1 ≈ 1 (random-walk flavor), others ≈ 0.
  – Variances: overall tightness λ, cross-lag penalty θ, lag-decay (∝ 1/ℓ²), scale by residual std devs.
  – Fast, transparent; good default for FA-BVAR dynamics.

• Hierarchical Minnesota:
  – Same structure, but the tightness parameters are **learned** from the data (hyperpriors).
  – Adapts overall shrinkage level to sample size/system dimension.

• Adaptive Hierarchical Priors (global–local):
  – Add a **global** tightness (controls sparsity system-wide) and **local** tightness per coefficient.
  – Result: irrelevant coefficients collapse toward 0; truly important ones **escape** shrinkage.
  – Often improves forecasts in large FA-BVARs.

• Σ prior (one sentence):
  – Use an Inverse-Wishart or stochastic-volatility prior; diagonal or block-diagonal options are common
    for forecasting, with identification added later for structural work.

• Practical tips:
  – Start with Minnesota; tune λ (and θ) by out-of-sample forecasts.
  – If dynamics differ a lot across blocks (F vs Y), use **block-specific tightness**.
  – Consider global–local when k and p are large; check IRF/forecast sensitivity.

• Take-away line:
  – “FA-BVAR = rich information via factors + **regularized** dynamics via shrinkage priors; own lags breathe,
    cross and distant lags stay humble unless the data insist.”

---
# Bayesian Estimation via Gibbs Sampling
&lt;div style="font-size: 27px;"&gt;

- The FA-BVAR model is estimated using Bayesian methods, typically via Gibbs sampling, which allows for efficient sampling from the posterior distributions of the parameters and latent factors.

- The Gibbs sampler iteratively samples from the conditional distributions of the parameters and latent factors, updating them based on the observed data and the specified priors.

- The sampling process involves:

  - Sampling the latent factors `\(F_t\)` given the observed data and current parameter estimates.
  - Sampling the coefficients `\(A_i\)` and covariance matrix `\(\Sigma\)` given the sampled latent factors.
  - Updating the priors based on the sampled parameters and latent factors.

???
# Bayesian Estimation via Gibbs Sampling — presenter notes

• Core message
  – FA-BVAR is estimated with **Gibbs sampling**: draw each block from its **conditional posterior**, cycle, and collect draws → full posterior.

• Model blocks we use
  – **Measurement**:  `\(X_t = Λ F_t + e_t,  e_t ~ N(0, Ψ)\)`
  – **State (VAR)**:  `\(Z_t = [Y_t', F_t']' = Σ_{i=1}^p A_i Z_{t−i} + u_t, \ \ u_t ~ N(0, Σ)\)`
  – **Priors**: shrinkage on A (Minnesota / NG / hierarchical), IW (or SV) for Σ; Normal–IW for `\((Λ, Ψ)\)`.

• One Gibbs iteration (repeat G times)
  1) **Sample factors**   `\(F_{1:T} | X, Y, Λ, Ψ, A, Σ\)`
     – Use Kalman filter/smoother (Carter–Kohn) or state-space sampler.
  2) **Sample loadings** `\(Λ | X, F, Ψ\)` (row-wise Normal regressions);  
     **sample** `\(Ψ | X, F, Λ\)` (Inverse-Gamma / IW).
  3) **Form** `\(Z = [Y, F] and sample A | Σ, Z\)`  
     – Matrix-Normal update with shrinkage prior; if global–local, also draw local scales (ψ) and global tightness (τ/κ).
  4) **Sample** `\(Σ | A, Z\)`
     – Inverse-Wishart (or SV block if using stochastic volatility).
  5) **Sample hyperparameters** `\(| A (and ψ)\)`  
     – e.g., λ, θ (Minnesota), τ/κ (NG), etc.

• Housekeeping / workflow
  – Initialize F with PCA (and choose r); choose p via ICs/OOS.  
  – Run burn-in (e.g., first 2–5k draws), then keep draws; check **trace/R-hat/ESS**.  
  – Summarize posteriors (means/medians, 68/90% bands).

• Using the draws
  – **Forecasts**: simulate `\(Z_{T+h}\)` recursively per draw → density forecasts for Y.  
  – **IRFs/FEVDs**: for each draw, identify `\(Σ (Cholesky/sign)\)`, compute IRFs, then average and band them.

• Talk-track punchline
  – “Gibbs just alternates: **sample** `\(F → Λ/Ψ → A → Σ → hyperparameters\)`.  
     Joint estimation **propagates factor uncertainty** and shrinkage keeps high-dimensional dynamics stable.”

## 1. Role of Gibbs Sampling in BFAVAR  
- **Joint estimation**: Instead of a two-step PCA + VAR, Gibbs sampling estimates factors `\((F_{1:T})\)` and parameters `\((\{\Lambda,\Phi,\Sigma\})\)` together, capturing sampling uncertainty at every stage :contentReference[oaicite:3]{index=3}.  
- **Conjugacy**: With normal–inverse-Wishart and related priors, each conditional posterior has a known form, enabling closed-form sampling rather than Metropolis proposals :contentReference[oaicite:4]{index=4}.  
- **Ragged panels &amp; mixed frequencies**: The Kalman forward-filter/backward-sample step handles missing or asynchronous observations naturally within the Gibbs cycle :contentReference[oaicite:5]{index=5}.



## 2. Step-by-Step Conditional Draws  

1. **Sample latent factors **  `\((F_{1:T})\)`
   - Use the current loadings \(\Lambda\), VAR coeffs \(\{\Phi_i\}\), and covariance `\((\Sigma)\)` in a state-space Kalman smoother to draw full factor paths :contentReference[oaicite:6]{index=6}.  

2. **Sample VAR parameters ** `\((\{\Phi_i\}\) and \(\Sigma)\)`  
   - Treating `\((Z_t=[Y_t;F_t])\)` as observed, draw coefficients `\((\Phi_i)\)` and covariance `\((\Sigma)\)` from a matrix-normal–inverse-Wishart posterior :contentReference[oaicite:7]{index=7}.  

3. **Sample factor loadings \(\Lambda\)**  
   - Given `\((F_{1:T})\)` and `\((X_{1:T})\)`, each row of `\((\Lambda)\)` is sampled via a standard Bayesian regression with shrinkage priors (e.g.\ horseshoe, Dirichlet–Laplace) :contentReference[oaicite:8]{index=8}.  

4. **Update hyper-parameters (shrinkage scales)**  
   - Draw variance components (Minnesota tightness, local–global scales) from Gamma or inverse-Gamma conditionals, adapting the degree of shrinkage :contentReference[oaicite:9]{index=9}.



## 3. Conveying Convergence &amp; Diagnostics  

- **Trace plots**: Show parameter chains mixing without trends, resembling a “hairy caterpillar” :contentReference[oaicite:10]{index=10}.  
- **Gelman–Rubin `\((\hat R)\)`**: Values near 1.0 across multiple chains indicate convergence :contentReference[oaicite:11]{index=11}.  
- **Effective sample size**: Ensure sufficient independent draws for reliable credible intervals :contentReference[oaicite:12]{index=12}.  
- **Burn-in &amp; thinning**: Explain that burn-in discards initial transient draws; thinning can reduce autocorrelation but is optional if chains are long enough :contentReference[oaicite:13]{index=13}.


## 4. Using Posterior Draws  

- **Point &amp; density estimates**: Compute posterior means, medians, and 90% credible intervals for parameters and factors :contentReference[oaicite:14]{index=14}.  
- **Density forecasts**: For each draw of `\(((\Phi,\Sigma))\)`, simulate future `\((Z_{t+h})\)` to build fan charts for `\((Y)\)` and `\((F)\)` :contentReference[oaicite:15]{index=15}.  
- **Impulse response functions**: Identify structural shocks (e.g.\ Cholesky or sign restrictions), then average IRFs over all draws for credible bands :contentReference[oaicite:16]{index=16}.  
- **Variance decompositions**: Decompose forecast-error variance into contributions from common factors versus idiosyncratic shocks :contentReference[oaicite:17]{index=17}.


## 5. Presenter Tips  

- **Visual metaphor**: Compare Gibbs sampling to an assembly line with stations for factors, VAR, loadings, and hyper-parameters—each learning from the last.  
- **Interactive demonstration**: Show students trace-plot animations across iterations, highlighting how chains “settle” into their stationary distribution.  
- **Hands-on intuition**: Encourage students to think of each conditional draw as “filling in” one piece of a puzzle given the current picture of the rest.  

---
# Bayesian Estimation via Gibbs Sampling
&lt;div style="font-size: 27px;"&gt;

- The Gibbs sampler continues until convergence, producing samples from the posterior distribution of the parameters and latent factors.

- The posterior samples can then be used to compute point estimates (e.g., means, medians) and credible intervals for the parameters, as well as to generate forecasts and impulse response functions.

- The Bayesian framework allows for uncertainty quantification and robust inference, making FA-BVAR a powerful tool for macroeconomic analysis.

- The Gibbs sampling approach is particularly useful in FA-BVAR because it allows for the joint estimation of the latent factors and the VAR parameters, which is crucial for capturing the dynamic relationships in large datasets.


???
# Bayesian Estimation via Gibbs Sampling — presenter notes (conclusion)

• What “until convergence” means
  – Run the Gibbs cycles until draws look *stationary* and well-mixed.
  – Check **diagnostics**: trace plots (no drift), **R-hat ≲ 1.01**, **effective sample size** (ESS) large enough, low autocorrelation.
  – Use multiple chains with dispersed starts; discard **burn-in**.

• After convergence: what we do with the draws
  – Compute **posterior summaries**: means/medians, **credible intervals** for A’s, Σ, Λ, and F_t.
  – Produce **posterior predictive** objects: density forecasts (fan charts), predictive quantiles.
  – For **IRFs/FEVDs**: identify Σ per draw (e.g., Cholesky/sign), compute IRFs, then average and form credible bands.

• Why Bayesian helps here
  – We get **full uncertainty quantification**: parameters **and** latent factors with coherent intervals.
  – Robust inference in data-rich settings: shrinkage + joint estimation prevent overfitting and propagate factor uncertainty.

• Why Gibbs is handy for FA-BVAR
  – Conditionals are available/tractable (e.g., Matrix-Normal/IW blocks), so sampling is **modular and efficient** even in large systems.
  – Captures the **dynamic relationships** among targets and factors without two-step generated-regressor issues.

• Practical reminders
  – Save draws of A, Σ, Λ, and F_t; monitor stability (VAR roots inside unit circle) draw-by-draw.
  – Thin only if storage/autocorrelation is severe; otherwise keep all post-burn-in draws.
  – Report diagnostics and **sensitivity** to prior tightness and factor count r.

• One-liner
  – “Gibbs gives us a factory for posterior draws; from those we get **estimates, intervals, forecasts, and IRFs**—all with factor uncertainty baked in.”

---
# FA-BVAR Example
&lt;div style="font-size: 27px;"&gt;

.pull-left[

``` r
library(BVARWGC)
library(vars)
library(tsaccessories)
library(readxl)
library(BFAVAR)

Y&lt;-read_excel("datawork/FAVAR.xlsx", sheet = "Sheet1")
X&lt;-read_excel("datawork/FAVAR.xlsx", sheet = "Sheet2")

df &lt;- X[, sapply(X, is.numeric)]

bbe_obj &lt;- pca(data = df, y_var = "Interest rate",
               x_vars = colnames(df)[1:116],
               method = "BBE", ncomp_max = 5)
F_bbe   &lt;- select_factors(bbe_obj, r = 1)


y_vars &lt;- c("Inflation rate", "Unemployment rate", "Interest rate" )
# 2) Compute FEVDs up to 10 horizons
fit &lt;- bfavar(Y, y_vars = y_vars, F_hat = F_bbe,
              p = 4, prior = "GLP", backend = "auto", draws = 8000, burn = 4000, thin = 2)
bfavarsum(fit) 
irf &lt;- bfavar_irf(fit, H = 20) # BVAR: bvar_irf object (use plot(irf)); bayesianVARs: list with IRFs
#plot(irf)
fe &lt;- bfavarfevd(fit, H = 5)

# 3) Plot (stacked bars + % labels)
p  &lt;- bfavarfevd2(fe)  
print(p)
```
]

.pull-right[

```
## 
## === Factor Extraction (BBE) ===
##   PC     Eigen PropPct CumPct
##  PC1 71.867859   79.43  79.43
##  PC2  8.173190    9.03  88.47
##  PC3  5.890206    6.51  94.98
##  PC4  3.669286    4.06  99.03
##  PC5  0.874812    0.97 100.00
## 
## Use select_factors(obj, r = ...) to choose r factors for your FA-BVAR.
```

]

---
# FA-BVAR Example
&lt;div style="font-size: 18px;"&gt;


.pull-left[

``` r
fit &lt;- bfavar(Y, y_vars = y_vars, F_hat = F_bbe,
              p = 4, prior = "GLP", backend = "auto", draws = 8000, burn = 4000, thin = 2)
```

```
## Optimisation concluded.
## Posterior marginal likelihood: -262.553
## Hyperparameters: lambda = 0.49983; soc = 0.36911; sur = 0.38366
## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |                                                                      |   1%
  |                                                                            
  |=                                                                     |   1%
  |                                                                            
  |=                                                                     |   2%
  |                                                                            
  |==                                                                    |   2%
  |                                                                            
  |==                                                                    |   3%
  |                                                                            
  |==                                                                    |   4%
  |                                                                            
  |===                                                                   |   4%
  |                                                                            
  |===                                                                   |   5%
  |                                                                            
  |====                                                                  |   5%
  |                                                                            
  |====                                                                  |   6%
  |                                                                            
  |=====                                                                 |   6%
  |                                                                            
  |=====                                                                 |   7%
  |                                                                            
  |=====                                                                 |   8%
  |                                                                            
  |======                                                                |   8%
  |                                                                            
  |======                                                                |   9%
  |                                                                            
  |=======                                                               |   9%
  |                                                                            
  |=======                                                               |  10%
  |                                                                            
  |=======                                                               |  11%
  |                                                                            
  |========                                                              |  11%
  |                                                                            
  |========                                                              |  12%
  |                                                                            
  |=========                                                             |  12%
  |                                                                            
  |=========                                                             |  13%
  |                                                                            
  |=========                                                             |  14%
  |                                                                            
  |==========                                                            |  14%
  |                                                                            
  |==========                                                            |  15%
  |                                                                            
  |===========                                                           |  15%
  |                                                                            
  |===========                                                           |  16%
  |                                                                            
  |============                                                          |  16%
  |                                                                            
  |============                                                          |  17%
  |                                                                            
  |============                                                          |  18%
  |                                                                            
  |=============                                                         |  18%
  |                                                                            
  |=============                                                         |  19%
  |                                                                            
  |==============                                                        |  19%
  |                                                                            
  |==============                                                        |  20%
  |                                                                            
  |==============                                                        |  21%
  |                                                                            
  |===============                                                       |  21%
  |                                                                            
  |===============                                                       |  22%
  |                                                                            
  |================                                                      |  22%
  |                                                                            
  |================                                                      |  23%
  |                                                                            
  |================                                                      |  24%
  |                                                                            
  |=================                                                     |  24%
  |                                                                            
  |=================                                                     |  25%
  |                                                                            
  |==================                                                    |  25%
  |                                                                            
  |==================                                                    |  26%
  |                                                                            
  |===================                                                   |  26%
  |                                                                            
  |===================                                                   |  27%
  |                                                                            
  |===================                                                   |  28%
  |                                                                            
  |====================                                                  |  28%
  |                                                                            
  |====================                                                  |  29%
  |                                                                            
  |=====================                                                 |  29%
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |=====================                                                 |  31%
  |                                                                            
  |======================                                                |  31%
  |                                                                            
  |======================                                                |  32%
  |                                                                            
  |=======================                                               |  32%
  |                                                                            
  |=======================                                               |  33%
  |                                                                            
  |=======================                                               |  34%
  |                                                                            
  |========================                                              |  34%
  |                                                                            
  |========================                                              |  35%
  |                                                                            
  |=========================                                             |  35%
  |                                                                            
  |=========================                                             |  36%
  |                                                                            
  |==========================                                            |  36%
  |                                                                            
  |==========================                                            |  37%
  |                                                                            
  |==========================                                            |  38%
  |                                                                            
  |===========================                                           |  38%
  |                                                                            
  |===========================                                           |  39%
  |                                                                            
  |============================                                          |  39%
  |                                                                            
  |============================                                          |  40%
  |                                                                            
  |============================                                          |  41%
  |                                                                            
  |=============================                                         |  41%
  |                                                                            
  |=============================                                         |  42%
  |                                                                            
  |==============================                                        |  42%
  |                                                                            
  |==============================                                        |  43%
  |                                                                            
  |==============================                                        |  44%
  |                                                                            
  |===============================                                       |  44%
  |                                                                            
  |===============================                                       |  45%
  |                                                                            
  |================================                                      |  45%
  |                                                                            
  |================================                                      |  46%
  |                                                                            
  |=================================                                     |  46%
  |                                                                            
  |=================================                                     |  47%
  |                                                                            
  |=================================                                     |  48%
  |                                                                            
  |==================================                                    |  48%
  |                                                                            
  |==================================                                    |  49%
  |                                                                            
  |===================================                                   |  49%
  |                                                                            
  |===================================                                   |  50%
  |                                                                            
  |===================================                                   |  51%
  |                                                                            
  |====================================                                  |  51%
  |                                                                            
  |====================================                                  |  52%
  |                                                                            
  |=====================================                                 |  52%
  |                                                                            
  |=====================================                                 |  53%
  |                                                                            
  |=====================================                                 |  54%
  |                                                                            
  |======================================                                |  54%
  |                                                                            
  |======================================                                |  55%
  |                                                                            
  |=======================================                               |  55%
  |                                                                            
  |=======================================                               |  56%
  |                                                                            
  |========================================                              |  56%
  |                                                                            
  |========================================                              |  57%
  |                                                                            
  |========================================                              |  58%
  |                                                                            
  |=========================================                             |  58%
  |                                                                            
  |=========================================                             |  59%
  |                                                                            
  |==========================================                            |  59%
  |                                                                            
  |==========================================                            |  60%
  |                                                                            
  |==========================================                            |  61%
  |                                                                            
  |===========================================                           |  61%
  |                                                                            
  |===========================================                           |  62%
  |                                                                            
  |============================================                          |  62%
  |                                                                            
  |============================================                          |  63%
  |                                                                            
  |============================================                          |  64%
  |                                                                            
  |=============================================                         |  64%
  |                                                                            
  |=============================================                         |  65%
  |                                                                            
  |==============================================                        |  65%
  |                                                                            
  |==============================================                        |  66%
  |                                                                            
  |===============================================                       |  66%
  |                                                                            
  |===============================================                       |  67%
  |                                                                            
  |===============================================                       |  68%
  |                                                                            
  |================================================                      |  68%
  |                                                                            
  |================================================                      |  69%
  |                                                                            
  |=================================================                     |  69%
  |                                                                            
  |=================================================                     |  70%
  |                                                                            
  |=================================================                     |  71%
  |                                                                            
  |==================================================                    |  71%
  |                                                                            
  |==================================================                    |  72%
  |                                                                            
  |===================================================                   |  72%
  |                                                                            
  |===================================================                   |  73%
  |                                                                            
  |===================================================                   |  74%
  |                                                                            
  |====================================================                  |  74%
  |                                                                            
  |====================================================                  |  75%
  |                                                                            
  |=====================================================                 |  75%
  |                                                                            
  |=====================================================                 |  76%
  |                                                                            
  |======================================================                |  76%
  |                                                                            
  |======================================================                |  77%
  |                                                                            
  |======================================================                |  78%
  |                                                                            
  |=======================================================               |  78%
  |                                                                            
  |=======================================================               |  79%
  |                                                                            
  |========================================================              |  79%
  |                                                                            
  |========================================================              |  80%
  |                                                                            
  |========================================================              |  81%
  |                                                                            
  |=========================================================             |  81%
  |                                                                            
  |=========================================================             |  82%
  |                                                                            
  |==========================================================            |  82%
  |                                                                            
  |==========================================================            |  83%
  |                                                                            
  |==========================================================            |  84%
  |                                                                            
  |===========================================================           |  84%
  |                                                                            
  |===========================================================           |  85%
  |                                                                            
  |============================================================          |  85%
  |                                                                            
  |============================================================          |  86%
  |                                                                            
  |=============================================================         |  86%
  |                                                                            
  |=============================================================         |  87%
  |                                                                            
  |=============================================================         |  88%
  |                                                                            
  |==============================================================        |  88%
  |                                                                            
  |==============================================================        |  89%
  |                                                                            
  |===============================================================       |  89%
  |                                                                            
  |===============================================================       |  90%
  |                                                                            
  |===============================================================       |  91%
  |                                                                            
  |================================================================      |  91%
  |                                                                            
  |================================================================      |  92%
  |                                                                            
  |=================================================================     |  92%
  |                                                                            
  |=================================================================     |  93%
  |                                                                            
  |=================================================================     |  94%
  |                                                                            
  |==================================================================    |  94%
  |                                                                            
  |==================================================================    |  95%
  |                                                                            
  |===================================================================   |  95%
  |                                                                            
  |===================================================================   |  96%
  |                                                                            
  |====================================================================  |  96%
  |                                                                            
  |====================================================================  |  97%
  |                                                                            
  |====================================================================  |  98%
  |                                                                            
  |===================================================================== |  98%
  |                                                                            
  |===================================================================== |  99%
  |                                                                            
  |======================================================================|  99%
  |                                                                            
  |======================================================================| 100%
## Finished MCMC after 1.05 secs.
```
]


.pull-right[

```
## Bayesian VAR consisting of 188 observations, 4 variables and 4 lags.
## Time spent calculating: 1.05 secs
## Hyperparameters: lambda, soc, sur 
## Hyperparameter values after optimisation: 0.49983, 0.36911, 0.38366
## Iterations (burnt / thinning): 8000 (4000 / 2)
## Accepted draws (rate): 114 (0.028)
## 
## Numeric array (dimensions 17, 4) of coefficient values from a BVAR.
## Median values:
##                        Inflation rate Unemployment rate Interest rate     F1
## constant                        0.195             0.012         0.151  0.166
## Inflation rate-lag1             1.338             0.040         0.400  0.191
## Unemployment rate-lag1         -0.195             1.216        -0.595 -0.189
## Interest rate-lag1             -0.018            -0.061         0.967  0.004
## F1-lag1                         0.012             0.132        -0.620  0.753
## Inflation rate-lag2            -0.237             0.031        -0.222 -0.107
## Unemployment rate-lag2          0.122            -0.151         0.346  0.105
## Interest rate-lag2              0.004             0.034        -0.159 -0.076
## F1-lag2                         0.035            -0.137         0.513  0.186
## Inflation rate-lag3            -0.076            -0.021         0.127  0.057
## Unemployment rate-lag3          0.008            -0.073         0.037  0.011
## Interest rate-lag3              0.003             0.063         0.060  0.034
## F1-lag3                        -0.020             0.018         0.144  0.095
## Inflation rate-lag4            -0.031            -0.041        -0.172 -0.074
## Unemployment rate-lag4          0.042            -0.030         0.204  0.069
## Interest rate-lag4              0.004            -0.008         0.035 -0.002
## F1-lag4                        -0.031            -0.015        -0.037 -0.023
## 
## Numeric array (dimensions 4, 4) of variance-covariance values from a BVAR.
## Median values:
##                   Inflation rate Unemployment rate Interest rate     F1
## Inflation rate             0.102            -0.004         0.043  0.022
## Unemployment rate         -0.004             0.073        -0.083 -0.020
## Interest rate              0.043            -0.083         0.561  0.190
## F1                         0.022            -0.020         0.190  0.089
## 
## Log-Likelihood: -156.9765
```
]



---
# FA-BVAR Example
&lt;div style="font-size: 22px;"&gt;


.pull-left[
- Inflation is highly persistent and largely self-driven, but reacts negatively to unemployment (Phillips curve logic).

- Unemployment is also persistent, with some influence from F1, suggesting common shocks captured by the factor (e.g., productivity shocks).

- Interest rate responds to inflation and unemployment in a Taylor-rule fashion (tightens with inflation, loosens with unemployment).

- F1 (latent factor) plays a meaningful role in both unemployment and interest rate equations → it may represent a common macro driver such as global conditions, commodity prices, or productivity trends.

]


.pull-right[

``` r
irf &lt;- bfavar_irf(fit, H = 20) # BVAR: bvar_irf object (use plot(irf)); bayesianVARs: list with IRFs
#plot(irf)
fe &lt;- bfavarfevd(fit, H = 10)
plt_hs &lt;- bfavarfevd4(fe, horizons = 1:10, others_only = TRUE,
                      palette = "viridis", label_min = 0.03, label_size_mm = 2)
#print(plt_hs)
```
]

---
# FA-BVAR Example
&lt;div style="font-size: 12px;"&gt;
.pull-left[
.center[&lt;img src="img/irf2.png" width="900"/&gt;]
- Inflation dynamics: Mostly self-driven but reduced by interest rate hikes and unemployment shocks.
- Unemployment rigidity: Strong persistence; reacts to policy and inflation shocks, but very sluggishly.
- Monetary policy rule: Interest rate reacts positively to inflation and negatively to unemployment, consistent with a Taylor rule.
- F1 (latent factor): Influences unemployment and interest rates significantly.
]

.pull-right[
.center[&lt;img src="img/fevd2.png" width="900"/&gt;]
- The FEVD results from the BFAVAR indicate that the latent factor (F1) explains a substantial share (30–35%) of the forecast error variance of all variables, particularly inflation and unemployment.
- This stands in contrast to the standard BVAR, where inflation and unemployment were almost entirely self-driven.
- Policy Behavior: Interest rates are heavily influenced by F1, suggesting the central bank indirectly reacts to global/latent forces, not just inflation and unemployment.
]
---
# FA-BVAR Example
&lt;div style="font-size: 20px;"&gt;

``` r
library(FAVAR)
library(readxl)
library(dplyr)
library(stringr)
library(ggplot2)
library(patchwork)
Y&lt;-read_excel("datawork/FAVAR.xlsx", sheet = "Sheet1")
X&lt;-read_excel("datawork/FAVAR.xlsx", sheet = "Sheet2")
X &lt;- X[, sapply(X, is.numeric)]

  "CBI","GDPC96","FINSLC96","CIVA","CP","CNCF","GDPCTPI","FPI","GSAVE", "PRFI",   "CMDEBT", "INDPRO",
  "NAPM",   "HCOMPBS", "HOABS",  "RCPHBS", "ULCBS", "COMPNFB", "HOANBS", "COMPRNFB", "ULCNFB", "UEMPLT5", "UEMP5TO14", 
  "UEMP15OV",  "UEMP15T26", "UEMP27OV",  "NDMANEMP", "MANEMP", "SRVPRD", "USTPU",  "USWTRADE",  "USTRADE", "USFIRE", 
  "USEHS",  "USPBS",  "USINFO" , "USSERV", "USPRIV", "USGOVT", "USLAH", "AHECONS" , "AHEMAN", "AHETPI", "AWOTMAN" , 
  "AWHMAN", "HOUST",  "HOUSTNE", "HOUSTMW", "HOUSTS", "HOUSTW", "HOUST1F", "PERMIT", "NONREVSL",  "USGSEC", "OTHSEC", 
  "TOTALSL", "BUSLOANS",  "CONSUMER",  "LOANS",  "LOANINV", "INVEST", "REALLN", "BOGAMBSL", "TRARR",  "BOGNONBR",  
  "NFORBRES",  "M1SL",   "CURRSL", "CURRDD", "DEMDEPSL",  "TCDSL",  "TB3MS", "TB6MS",  "GS1", "GS3", "GS5", "GS10",   
  "MPRIME", "AAA","BAA","sTB3MS",  "sTB6MS", "sGS1",   "sGS3",   "sGS5",   "sGS10",  "sMPRIME", "sAAA",   "sBAA",   "EXSZUS", 
  "EXJPUS", "PPIACO", "PPICRM", "PPIFCF", "PPIFCG", "PFCGEF", "PPIFGS", "PPICPE", "PPIENG", "PPIIDC", "PPIITM", 
  "CPIAUCSL",  "CPIUFDSL",  "CPIENGSL",  "CPILEGSL",  "CPIULFSL",  "CPILFESL",  "OILPRICE", "HHSNTN", "PMI","PMNO",   
  "PMDEL",  "PMNV",   "MOCMQ",  "MSONDQ", "Interest rate"
  )

# Build the logical factors vector required by FAVAR() (must align to columns of X)
slowcode &lt;- colnames(X) %in% factors

# Settings mirroring your frequentist VAR: 1 factors, 2 lags
K    &lt;- 1
plag &lt;- 2
iter &lt;- 15000  # saved draws (nrep)
burn &lt;- 5000   # burn-in (nburn)

# Priors:
#   factorprior: diffuse Normal-Gamma on factor eq.

factor_prior &lt;- list(b0 = 0, vb0 = NULL, c0 = 0.01, d0 = 0.01)
var_prior    &lt;- list(mn = list(kappa0 = 0.2, kappa1 = 0.5))  # classic, moderate shrink

k_var &lt;- K + ncol(Y)          # number of VAR equations = K factors + |Y|
var_prior &lt;- list(
  b0  = 0,                    # coeff prior mean (0 = random walk around 0)
  vb0 = 10,                   # coeff prior variance (bigger = looser)
  nu0 = k_var + 2,            # dof for Sigma prior (min k+1; k+2 is weakly informative)
  s0  = diag(k_var)           # scale matrix for Sigma prior
)

fit &lt;- FAVAR(
  Y = Y, X = X,
  fctmethod  = "BBE",
  slowcode   = slowcode,
  K          = K,
  plag       = plag,
  factorprior= list(b0 = 0, vb0 = NULL, c0 = 0.01, d0 = 0.01),
  varprior   = var_prior,     # &lt;-- now has nu0 and s0
  nburn      = burn,
  nrep       = iter,
  standardize= TRUE,
  ncores     = 1
)

# In the internal VAR, variables are stacked as [F1..FK, Y].
K_var   &lt;- K + ncol(Y)       # total VAR dimension
imp_pos &lt;- K + 1             #  (K+1)-th variable (impulse position)

vars_to_show &lt;- c("Inflation rate", "Unemployment rate", "Interest rate" )

# Map names to indices in the [X, Y] space used by irf()
XY_names &lt;- c(colnames(X), colnames(Y))
res_idx  &lt;- match(vars_to_show, XY_names)
res_idx  &lt;- res_idx[!is.na(res_idx)]   # drop any not-found names

irf(
  fit,
  irftype  = "orth",
  tcode    = "level",   # change if some series are in log/diff (vector also accepted)
  resvar   = res_idx,
  impvar   = imp_pos,
  nhor     = 48,
  ci       = 0.90,
  showplot = TRUE
)
```


---
# Time-Varying Parameter BVARs (TVP-BVARs)
&lt;div style="font-size: 27px;"&gt;

--

- Standard VARs impose the strong assumption of constant parameters over time, which may not hold in practice.

--

- In macroeconomics, such constancy is often unrealistic due to evolving economic conditions and policy regimes.

???
# Time-Varying Parameter BVARs (TVP-BVARs) — presenter notes

* **Motivation**

 Standard VARs assume **constant coefficients**, which can be unrealistic when policies/regimes evolve. TVP-VARs relax this by letting relationships change over time. 

--


- Time-Varying Parameter BVARs (TVP-BVARs) relax this assumption by allowing the coefficients to change over time, capturing structural breaks and evolving relationships.

???
* **What a TVP-BVAR does**

 Writes the VAR in **state-space form** with coefficients that follow simple dynamics (often **random walks**) and typically **stochastic volatility** for shocks. This captures gradual drifts and structural breaks. 

--

- TVP-BVARs are particularly useful in situations where the underlying economic relationships are expected to change, such as during financial crises, policy regime shifts, or structural changes in the economy.

???
* **When it’s particularly useful**

 Periods with **policy shifts, crises, or regime changes** where elasticities/propagation mechanisms move over time (e.g., Great Inflation/Moderation, GFC, pandemic). 

--

- They can be estimated using Bayesian methods, often employing a random walk prior on the coefficients to allow for gradual changes over time.

???

* **Estimation (how we do it)**

 **Bayesian MCMC (Gibbs)** with Kalman filter/smoother steps; widely used algorithms and corrections 
are documented (Del Negro &amp; Primiceri, 2013/2015). Software implementations (e.g., *bvarsv* in R) follow this setup. 

* **Evidence &amp; practice**

 For forecasting and inference in **large systems**, TVP-VARs with shrinkage priors are feasible and often perform well; see overviews and scalable methods in **Koop &amp; Korobilis (2013)** and recent general frameworks.


* **One-liner to close**

 “TVP-BVARs let the **data update the model’s relationships through time**—coefficients drift, volatility changes, and we estimate it all coherently in a Bayesian, state-space framework.” 

---
# Nigeria Monetary Policy:Bad Policy or Bad Luck?

&lt;div style="font-size: 25px;"&gt;

--

- High inflation and real-GDP volatility in Nigeria (1970–2014): was it central-bank policy shifts or changing external shocks? (Abdullahi, 2016)

???
* **Framing the question**

Nigeria (1970–2014) saw high inflation and volatile real GDP. Are the swings due to **changing policy behavior** (bad policy) or **changing shocks**, especially oil-market shocks (bad luck)? This slide contrasts both stories with evidence.

--

#### “Bad Policy” story

- TVP-VAR estimates reveal that the *immediate responses* of real GDP, FX reserves and inflation to a policy-rate shock vary significantly over time, suggesting that the CBN’s reaction function (i.e., the VAR coefficients) *changed under different governors*.

???
* **“Bad Policy” story (time-varying reaction function)**

1. TVP-VAR evidence shows that the **impact of a policy-rate shock on real GDP, FX reserves, and inflation** changed notably over time, consistent with shifts in the CBN’s reaction function across regimes/governors

2. Interpretation: coefficients in the VAR **drift**—policy transmission varies—so different regimes imply different immediate responses.

--

#### “Bad Luck” story

- Exogenous shocks—especially, *oil-market demand shocks*, exhibit *time-varying volatility*, which can *mimic* changes in policy effectiveness. Oil-shock variance spikes coincide with apparent regime shifts in impulse responses, implying that *stochastic volatility alone* can generate the look of structural breaks (Bello, 2024)

???

* **“Bad Luck” story (time-varying shock volatility)**

“Bad Luck” story (time-varying shock volatility)

1. Oil-market shocks to Nigeria’s economy display **time-varying volatility**; in a TVP-VAR, oil-specific demand shocks materially affect exchange rate, output growth, and price stability, and their effects **vary over time**. Spikes in shock variance can make IRFs look as if policy changed. 

2. Interpretation: if **shock variances** (especially oil-related) spike at times that align with apparent regime changes, then **volatility alone** can mimic changes in policy effectiveness.

3. Broader literature warns that **changes in volatility alone** can mimic regime shifts or apparent changes in transmission—so one can mistake “bad luck” for “bad policy” without modeling stochastic volatility. 

**How to discriminate in practice (what you’d say you did)**
  – Use a **TVP-VAR with stochastic volatility**: let both **coefficients** and **shock variances** evolve. Then inspect posteriors:
     1) Do coefficients (policy reaction) **move** materially?  
     2) Or do **shock variances** account for most of the instability?  
     3) Are the timing of volatility spikes aligned with global oil episodes? 
     
• Talking points to close
  – If the model finds **stable coefficients** but **volatile shocks**, the evidence favors “bad luck.” If **coefficients drift** (especially around leadership or framework changes), that supports “bad policy.” In Nigeria, studies find elements of both: **time-varying responses** (policy) and **time-varying oil-shock volatility** (luck). 
  
---
# Nigeria Monetary Policy:Bad Policy or Bad Luck?

&lt;div style="font-size: 27px;"&gt;

--

### Methodological note

- A TVP-VAR with stochastic volatility nests both stories by letting coefficients and shock variances evolve continuously, avoiding arbitrary break-date assumptions 


### Recent policy context

- After holding rates steady at 27.5%, the CBN has signaled a tight stance amid sticky inflation; understanding whether this reflects a deliberate policy shift or merely a response to more volatile shocks is crucial for future rate decisions.

- The TVP-VAR framework allows for a nuanced analysis of the CBN's policy response, providing insights into whether recent rate decisions are driven by changing economic conditions or a shift in the central bank's reaction function.

???
# Nigeria Monetary Policy: Bad Policy or Bad Luck? — presenter notes

* Recent policy context

– After holding the policy rate around **27.5%**, the CBN has maintained a tight stance amid sticky inflation. The key question is whether this reflects a deliberate **policy shift** or simply a response to more volatile shocks.

* Interpreting “Bad Policy” vs “Bad Luck”
  
– **Bad Policy:** If the **impulse responses to a policy-rate shock** change materially through time,
    that suggests the central bank’s **reaction function** shifted across regimes.
  
– **Bad Luck:** If **shock variances** (e.g., oil-related disturbances) spike at times that coincide with apparent regime changes, volatility alone can **mimic** changes in policy effectiveness.


* **What the TVP-VAR/SV delivers**

– Jointly lets the data indicate whether instability is mostly in **βₜ** (coefficients) or in **Σₜ** (volatility).

– Estimation proceeds with a Gibbs sampler that alternates draws for factors/parameters/volatility.

* **Take-away line**
  – If coefficients look stable but shocks are volatile → **bad luck** dominates.
    If coefficients drift around leadership/framework changes → **bad policy** is implicated.
    In practice, both forces can matter; the TVP-VAR/SV framework helps separate them.

---
# Empirical Evidence: Both Transmission and Volatility Change
&lt;div style="font-size: 20px;"&gt;

- Studies (Primiceri, 2005; Koop et al., 2009) show that **both** transmission mechanisms and shock variances vary over time.

- Primiceri (2005) finds significant shifts in U.S. monetary policy transmission and volatility using a TVP-SVAR with stochastic volatility.

- Policy implications require models where both VAR coefficients and error covariances can change.
- Extensive macro literature documents structural breaks and parameter changes (Stock \&amp; Watson, 1996).

--

### Alternative specifications:

  - Markov switching VARs (Paap &amp; van Dijk, 2003; Sims &amp; Zha, 2006)

  - Other regime-switching VARs (Koop &amp; Potter, 2006)


- TVP–VARs have emerged as the most popular approach for modelling gradual parameter evolution.

???
# Empirical Evidence: Both Transmission and Volatility Change — presenter notes

* **Policy implication**

– For credible inference and policy analysis, we need models where **VAR coefficients and error covariances** can change—TVP-VARs with stochastic volatility are designed for this. (Tie back to Primiceri-style setups.) 

* **Alternative specifications to mention**

1 **Markov-switching VARs:** regime changes captured by discrete states; e.g., **Sims &amp; Zha (2006)** show a best-fit specification with **time-varying disturbance variances** (and sometimes rule changes). :contentReference[oaicite:5]{index=5}

* **Why TVP-VARs are now standard**

2. For gradual evolution (not just discrete regimes), **TVP-VARs** have become a **popular workhorse**, including in large systems with shrinkage. (Koop &amp; Korobilis’s survey/results.) 

* **Wrap-up line**

3.  “The literature points to **two moving parts**—transmission and volatility. Good practice is to let **both** vary and let the data tell us which one drives the instability.”

---
# TVP–VAR: Model Structure
&lt;div style="font-size: 25px;"&gt;
## Formulation 

 `$$y_t= {X}_t \beta_t + \varepsilon_t, \quad \varepsilon_t \sim N(\mathbf{0, \Sigma}) \\
    \beta_t = \beta_t-1 + \eta_t, \quad \eta_t \sim N(\mathbf{0}, \mathbf{Q})$$`

- Where
  - `\(y_t\)` is the `\(N\times1\)` vector of observed variables at time `\(t\)`.
  - `\({X}_t\)` is the `\(N\times K\)` matrix of predictors (including lags).
  - `\(\beta_t\)` is the `\(K\times1\)` vector of time-varying coefficients at time `\(t\)`.
  - `\(\varepsilon_t\)` is the `\(N\times1\)` vector of errors, assumed to be normally distributed with mean zero and covariance matrix `\(\Sigma\)`.
  - `\(\eta_t\)` is the `\(K\times1\)` vector of innovations to the coefficients, assumed to be normally distributed with mean zero and covariance matrix `\(Q\)`.

- The model allows the coefficients `\(\beta_t\)` to evolve over time according to a random walk process, capturing gradual changes in the relationships between the variables.


???
# TVP–VAR: Model Structure — presenter notes

* **Big idea**

1. Same VAR logic, but **coefficients change over time** → written as a **state-space** model.

2. Lets us capture gradual structural change without hard break dates.


* **Objects to remember (no equations)**

- `\(y_t\)`: stacked vector of observed series (e.g., inflation, output, rate); equations estimated 

**jointly**.
  – `\(X_t\)`: predictors (lags of all variables + optional intercept/trend/exogenous controls). &lt;br&gt;
  
  – `\(β_t\)`: **time-varying** coefficients at time t; evolve gradually (random-walk style). &lt;br&gt;
  
  – `\(ε_t\)` with Σ: observation shocks and their contemporaneous covariance (co-movement across variables). &lt;br&gt;
  
  – `\(η_t\)` with Q: shocks that move `\(β_t\)` from `\(t−1\)` to `\(t\)`; **Q controls how fast** `\(β_t\)` is allowed to drift. &lt;br&gt;

* **How to read the model**
  – Observation part: today’s data = lags × **today’s** coefficients + a shock. &lt;br&gt;
  – State part: today’s coefficients = yesterday’s coefficients **+ a small step**.&lt;br&gt;
  – Many small steps ⇒ smooth drifts; occasional big steps ⇒ noticeable regime shifts.&lt;br&gt;

* **Tuning intuition**

  1. **Small Q** ⇒ coefficients nearly constant (prevents spurious time variation).&lt;br&gt;
  
  – **Larger Q** ⇒ coefficients can adapt faster to real change.&lt;br&gt;
  – Allowing **time-varying Σ (stochastic volatility)** is important—volatility shifts can otherwise look like parameter change.&lt;br&gt;

*  **Estimation (what we actually do)**
  – Bayesian **Gibbs** workflow: draw β-paths (Kalman smoother step) → draw variance blocks (Q, Σ; include SV if used) → repeat until convergence.&lt;br&gt;
  – Summarize posteriors; use draws for **density forecasts** and **time-varying IRFs** (identify per draw, then average &amp; band).&lt;br&gt;

* **Practical checklist**
  – Choose lag order p via ICs/OOS; standardize series if needed.&lt;br&gt;
  – Put sensible shrinkage on β_t and Q; check sensitivity.&lt;br&gt;
  – Diagnose MCMC (trace, R-hat, ESS) and **stability** (VAR roots) across draws before reporting results.&lt;br&gt;

* **One-liner**
  – “TVP-VARs let relationships **evolve** and shocks’ volatility **change**, giving policy-relevant IRFs and forecasts with coherent uncertainty.” &lt;br&gt;

---
# Bayesian Estimation of TVP–VAR
&lt;div style="font-size: 25px;"&gt;

- The TVP–VAR model is estimated using Bayesian methods, typically via Gibbs sampling or other MCMC techniques.

- The estimation process involves:

  - Specifying priors for the coefficients `\(\beta_t\)` and the covariance matrices `\(\Sigma\)` and `\(Q\)`.
  - Sampling from the posterior distributions of the parameters and latent variables using Gibbs sampling or other MCMC methods.
  - Updating the priors based on the observed data and the sampled parameters.

- Gibbs sampler alternates:
-  Sample `\(\{\beta_t\}\)` using FFBS (conditioned on `\({\Sigma}, \mathbf{Q}\)`).
- Sample `\(\Sigma\)` (inverse Wishart), `\(\mathbf{Q}\)` (inverse Wishart).
- Hyperpriors on initial coefficients and state noise variances.
- .my-coral[Key:] TVP–VAR is a high-dimensional, state space problem: Bayesian inference is efficient via MCMC.


???
# Bayesian Estimation of TVP–VAR — presenter notes

* **Big picture**

1. We estimate the TVP–VAR in a **Bayesian state-space** set-up. Priors are placed on the time-varying coefficients (β_t) and on the variance blocks (Σ for observation shocks, Q for coefficient innovations). Sampling is done with **Gibbs/MCMC**, which is efficient for these models. &lt;br&gt;
2. The key computational step is using **FFBS** (forward-filtering backward-sampling) to draw the entire path of coefficients {β_t} in one block, which improves efficiency over element-by-element sampling. &lt;br&gt;

3. The Gibbs sampler cycles through drawing the entire path of coefficients, then the variance blocks, iterating until convergence. &lt;br&gt;

* **What the Gibbs sampler cycles through (one iteration)**
  1) **Sample the entire path of coefficients `\({β_t} | Σ, Q\)`, data**  &lt;br&gt;
  
     – Use **FFBS** (forward-filtering backward-sampling): run a Kalman filter, then draw the states backward to obtain a full draw of `\(β₁:ₜ\)`. &lt;br&gt;
     
  2) **Sample Σ | β, data**  
     – With conjugate setup, draw from an **Inverse-Wishart** (or SV block if using stochastic volatility). Primiceri-style TVP-SV models treat volatilities as time-varying and sample them with a KSC (Kim-Shephard-Chib) step. &lt;br&gt;
     
  3) **Sample Q | β**  
     – Draw the evolution-variance(s) for β_t (Inverse-Gamma per element if Q is diagonal; IW if modeled as full). Tight Q ⇒ slow coefficient drift; loose Q ⇒ faster drift. &lt;br&gt;
     
  4) **(If SV) Sample the log-volatility states** and any auxiliary mixture indicators; Del Negro–Primiceri show the correct **ordering** of these steps for stable mixing. &lt;br&gt; 
  
  5) **Update hyperparameters** for priors on initial β and on the variance blocks as needed (hierarchical setup). &lt;br&gt;

* **What to emphasize to the audience**
  – **Why FFBS?** It gives full draws of the whole coefficient path in one block, which greatly improves efficiency over element-by-element sampling.&lt;br&gt;
  
  – **Why TVP-SV (time-varying Σ)?** Volatility changes on their own can mimic parameter change; modeling both drifting β_t and changing `\(Σ_t\)` is standard and policy-relevant.&lt;br&gt; 
  
  – **Key practical point:** TVP–VARs are **high-dimensional**; conjugate blocks + FFBS + the corrected sampler ordering make Gibbs **computationally tractable**. &lt;br&gt; 
  

* **Deliverables from the posterior draws**

  – **Point summaries &amp; credible intervals** for `\(β_t, Σ\)` (and Q).&lt;br&gt;  
  – **Density forecasts** by simulating forward from each draw. &lt;br&gt; 
  – **Time-varying IRFs/FEVDs**: identify shocks per draw (e.g., Cholesky/sign), compute IRFs, average and form credible bands. &lt;br&gt;


Forward Filtering Backward Sampling (FFBS) &lt;br&gt;

- Bayesian estimation of Time‐Varying Parameter VARs relies on Markov Chain Monte Carlo (MCMC), specifically a Gibbs sampler, to draw from the joint posterior of the entire path of time‐varying coefficients and covariances. &lt;br&gt;

- The core steps alternate between (1) sampling the latent coefficient trajectories `\(\{\beta_t\}\)` using Forward Filtering Backward Sampling (FFBS) conditioned on the covariance matrices `\(\Sigma\)` and `\(Q\)`, and (2) sampling the covariance matrices themselves from their posterior distributions.&lt;br&gt;

- The FFBS algorithm efficiently samples the entire path of time-varying coefficients `\(\{\beta_t\}\)` given the current estimates of the covariance matrices, allowing for a coherent estimation of the dynamic relationships in the data.&lt;br&gt;

- Sample `\(\{\Sigma, Q}\)` using conjugate priors (e.g., inverse Wishart for `\(\Sigma\)` and Wishart for `\(Q\)`).&lt;br&gt;

- The posterior samples can then be used to compute point estimates (e.g., means, medians) and credible intervals for the parameters, as well as to generate forecasts and impulse response functions.&lt;br&gt;



---
# Homoskedastic TVP–VARs: Model Structure
&lt;div style="font-size: 25px;"&gt;

--
##  Model equations:

`$$y_t \;=\; Z_t\,\beta_t \;+\;\varepsilon_t, \quad
\varepsilon_t \sim \mathcal{N}\bigl(0,\,\Sigma\bigr) \\ \beta_{t+1} \;=\; \beta_t \;+\; u_t, \quad
u_t \sim \mathcal{N}\bigl(0,\,Q\bigr)$$`

- Where
  - `\(y_t\)` is the `\(N\times1\)` vector of observed variables at time `\(t\)`.
  - `\(Z_t\)` is the `\(N\times K\)` matrix of predictors (including lags).
  - `\(\beta_t\)` is the `\(K\times1\)` vector of time-varying coefficients at time `\(t\)`.
  - `\(\varepsilon_t\)` is the `\(N\times1\)` vector of errors, assumed to be normally distributed with mean zero and covariance matrix `\(\Sigma\)`.
  - `\(u_t\)` is the `\(K\times1\)` vector of innovations to the coefficients, assumed to be normally distributed with mean zero and covariance matrix `\(Q\)`.

- TVP–VAR can allow for exogenous variables and (optionally) coefficients fixed over time

???
- The TVP–VAR’s state‐space formulation consists of two linked parts: an observation equation that ties the observed macroeconomic variables to a set of coefficients that are allowed to change over time, and a state equation that treats those coefficients themselves as evolving according to a simple “random‐walk” process. 

- By casting the model in this way, we can use the Kalman filter (or its Bayesian analogue) to infer the unobserved coefficient trajectories from the data, while hyperparameters governing the innovation variances determine how quickly those coefficients are permitted to drift. 

- In practice, one alternates between estimating the full path of coefficients and updating the covariance parameters—often via a Gibbs sampler that leverages conjugate inverse‐Wishart priors—which makes the approach both flexible enough to capture gradual structural change and computationally tractable even in moderately large systems.

# Homoskedastic TVP–VARs: Model Structure — presenter notes

• What “homoskedastic” means here
  – The **error covariance Σ is constant over time**. Coefficients are allowed to drift, but shock variances do not; this is the simplest TVP–VAR specification. &lt;br&gt;

• Read the blocks on the slide (no equations)
  – **$y_t$**: vector of observed variables (inflation, output, rate, …), modeled jointly.&lt;br&gt;
  – **$Z_t$**: predictor matrix (typically lags of all variables + possibly exogenous terms).&lt;br&gt;
  – **$β_t$**: **time-varying** coefficients at time t; they evolve gradually over time (random-walk style), allowing relationships to change. &lt;br&gt;
  – **$ε_t$ with Σ**: observation shocks; **Σ is fixed** under homoskedasticity. &lt;br&gt;
  – **$u_t$ with Q**: shocks that move β_t from one period to the next; **Q controls how fast coefficients can drift** (small Q → nearly constant parameters; larger Q → more flexibility). &lt;br&gt;

* **Estimation workflow (Gibbs in state-space form)**
  – **FFBS** to draw the full path of coefficients `\({β_t}\)` given `\((Σ, Q)\)` and data (forward filter, backward sample).&lt;br&gt;
  – Draw **Σ** (Inverse-Wishart in the homoskedastic case) and **Q** (IW/IG depending on parameterization). &lt;br&gt;
  – Iterate until convergence; summarize posteriors and produce density forecasts/IRFs from the draws. &lt;br&gt;

* **Why (and when) use the homoskedastic version**
  – **Pros:** simpler and faster than TVP–SV; fewer variance states to sample; good baseline when volatility appears roughly stable. &lt;br&gt;
  
  – **Caution:** if shock volatility actually changes over time, a homoskedastic TVP–VAR can **misattribute volatility shifts to coefficient drift**. Many studies find that allowing **stochastic volatility** improves fit/forecasts in macro data. &lt;br&gt;

* **Practical tips to mention**
  – Start with homoskedastic TVP–VAR for speed, then **test against TVP–SV** using marginal likelihood or OOS forecasts. &lt;br&gt;
  – Place shrinkage on **Q** to avoid spurious time variation; check MCMC diagnostics (trace, R-hat, ESS) and VAR stability across draws.&lt;br&gt;

* **One-liner**
  – “Homoskedastic TVP–VARs let **relationships evolve** while keeping **shock variance fixed**—a fast, transparent baseline before escalating to full TVP-SV.” 

---
# Heteroskedastic TVP–VARs: Model Structure

**Model equations (measurement &amp; states):**

`$$y_t \;=\; Z_t \beta_t \;+\; \varepsilon_t,\qquad \varepsilon_t \sim \mathcal{N}(0,\Sigma_t)$$`

`$$\beta_{t+1} \;=\; \beta_t \;+\; u_t,\qquad u_t \sim \mathcal{N}(0,Q)$$`

**Heteroskedasticity through stochastic volatility (SV) and structural shocks:**

`$$\varepsilon_t \;=\; A_t^{-1}\,\Lambda_t^{1/2}\,e_t,\qquad e_t \sim \mathcal{N}(0,I_N)$$`

`$$\Sigma_t \;=\; A_t^{-1}\,\Lambda_t\,(A_t^{-1})^\top ,\qquad 
\Lambda_t \;=\; \mathrm{diag}\!\big(e^{h_{1t}},\ldots,e^{h_{Nt}}\big)$$`

**Evolution of time‑varying components (random‑walk / AR(1) states):**

`$$a_{t+1} \;=\; a_t \;+\; \zeta_t,\qquad \zeta_t \sim \mathcal{N}(0,S) \quad \text{(free elements of }A_t\text{)}$$`

`$$h_{t+1} \;=\; \mu \;+\; \Phi\,(h_t-\mu) \;+\; \xi_t,\qquad \xi_t \sim \mathcal{N}(0,W)$$`

???
# Heteroskedastic TVP–VARs: Model Structure — presenter notes

* **What “heteroskedastic” adds**
  – Unlike the homoskedastic case, the **shock variances change over time** (stochastic volatility). We also allow the **contemporaneous relationships** among variables to evolve through a time-varying triangular impact matrix. Together, this lets both **transmission** and **volatility** vary.&lt;br&gt; 
  

* **Pieces of the decomposition (say, not derive)**
 1. `\(A_t\)` (lower-triangular, ones on the diagonal): maps **structural shocks** into reduced-form shocks; time variation in A_t means **instantaneous interactions** among variables can drift (policy/information structure can change). &lt;br&gt;
  
  2. `\(Λ_t = diag(exp(h_{1t}),…,exp(h_{nt}))\)`: diagonal matrix of **time-varying variances**; h_t are the **log-volatilities**. This is the stochastic-volatility (SV) block.&lt;br&gt; 
  
  3. Together, the **time-varying covariance** is `\(Σ_t = A_t^{-1} Λ_t A_t^{-⊤}\)`; `\(e_t\)` are `\(i.i.d\)`. standard shocks. &lt;br&gt;
  

*  State evolution (what moves over time)
  – **Free elements of** `\(A_t\)` follow a **random walk** (or very persistent AR(1)): slow drift captures changing contemporaneous structure. &lt;br&gt;
  
  – **Log-volatilities h_t** follow an **AR(1)** around a mean μ (persistence Φ): this delivers smooth volatility regimes (quiet vs turbulent periods). &lt;br&gt;
  

* Estimation workflow you’ll mention
  – Treat as a **state-space** model and use **Bayesian Gibbs**:&lt;br&gt;
      1) draw coefficient paths (FFBS/Kalman smoother),&lt;br&gt;
      2) draw A_t states,&lt;br&gt;
      3) draw SV states (h_t) with the **KSC** mixture sampler,&lt;br&gt;
      4) update variance blocks and hyperparameters.&lt;br&gt;
    Correct **step ordering** (Del Negro–Primiceri) improves mixing in TVP-SV VARs. &lt;br&gt;

• Why this specification matters&lt;br&gt;
  – If we keep Σ fixed, volatility spikes can be misread as parameter change; allowing **time-varying Σ_t** reduces that confounding and typically improves fit/forecasts in macro/finance data. &lt;br&gt;

• Identification note (one sentence)&lt;br&gt;
  – With a triangular A_t we get a time-varying **Cholesky** identification (ordering matters); alternative schemes (e.g., time-varying sign or external instruments) can be layered if needed.&lt;br&gt;

• Take-away line&lt;br&gt;
  – “Heteroskedastic TVP-VARs let the **instantaneous network** (A_t) and **uncertainty** `\((Λ_t/Σ_t)\)` evolve together—so we can tell whether instability comes from changing transmission, changing volatility, or both.”&lt;br&gt;



---
# Heteroskedastic TVP–VARs: Model Structure

**Where:**  
- `\((y_t)\)` is the `\((N\times 1)\)` vector of observed variables; \(Z_t\) stacks lags and exogenous predictors; `\((\beta_t)\)` is the `\((K\times 1)\)` vector of time‑varying coefficients.  
- `\((\varepsilon_t)\)` are reduced‑form shocks with time‑varying covariance `\((\Sigma_t)\)`; `\((u_t)\)` are coefficient innovations.
- `\((A_t)\)` is lower‑triangular with ones on the diagonal (recursive identification); `\((a_t)\)` collects its free off‑diagonal elements.  
- `\((Q,S,W)\)` are state‑innovation covariance matrices; `\((h_t)\)` are log variances driving `\((\Lambda_t)\)`.  

**Bayesian estimation (very high level):**  
Carter–Kohn FFBS draws for `\((beta_{1:T})\)` (and `\((a_{1:T})\)`); SV block for `\((h_{1:T})\)` via the Kim–Shephard–Chib mixture; then draws of \(Q,S,W\) (correct MCMC step ordering per the FRBNY corrigendum).

???
# Heteroskedastic TVP–VARs: Model Structure — presenter notes

• What “heteroskedastic” adds  
  – Unlike the homoskedastic TVP–VAR, here the **shock variances and covariances change over time** (stochastic volatility). We also allow the **instantaneous (contemporaneous) relations** among variables to evolve via a time-varying triangular impact matrix. This lets the model capture changes in **transmission** *and* **uncertainty** jointly. &lt;br&gt;

• Key building blocks (speak roles, don’t read equations) &lt;br&gt; 
  – `\(y\)`*: N×1 vector of observed variables (e.g., inflation, output, rate). &lt;br&gt; 
  – `\(Z_t\)`: N×K predictor matrix (VAR lags + optional exogenous terms).  &lt;br&gt;
  – `\(β_t\)`: K×1 vector of **time-varying coefficients**; evolves smoothly over time (random-walk/near-RW). This is the “time-varying parameter” part.&lt;br&gt; 
  
  – `\(ε_t\)` with **Σₜ**: reduced-form shocks with **time-varying** covariance; Σₜ is built from a factorization with a **time-varying lower-triangular Aₜ** (ones on the diagonal) and a **diagonal volatility matrix Λₜ** whose entries are exp(hₜ). This is the standard Primiceri (2005) decomposition&lt;br&gt;.
  – `\(u_t\)` with Q: shocks that move β_t from one period to the next; Q controls how fast coefficients can drift (small Q → nearly constant parameters; larger Q → more flexibility).&lt;br&gt;

• States that evolve over time (random-walk / AR(1) style)  &lt;br&gt;
  – **Free elements of Aₜ** (collect them in aₜ): allow **contemporaneous structure** to drift over time (policy/information structure can change).  &lt;br&gt;
  – **Log-volatilities hₜ** (diagonal of Λₜ): typically follow **AR(1)** dynamics so volatility moves in persistent regimes (quiet vs turbulent periods). These two together produce **Σₜ = Aₜ⁻¹ Λₜ Aₜ⁻ᵀ**&lt;br&gt;
  

• Why we need this specification  &lt;br&gt;
  – If we fixed Σ over time, **volatility shifts could be misread as parameter change**; letting Σₜ vary (SV) removes that confounding and typically improves fit and policy-relevant inference. Seminal applications show both **coefficients** and **volatility** change through time.&lt;br&gt;
  

• Bayesian estimation — what one Gibbs iteration does (very high level)  &lt;br&gt;
  1) **Draw β₁:ₜ | (Σₜ, Q, data)** using **FFBS** (Carter–Kohn): filter forward with Kalman filter, then sample the full β-path backward. This block update is efficient in high-dimensional state-space models. &lt;br&gt;  
  2) **Draw the Aₜ states** (free elements aₜ) given β and SV states; these are conditionally Gaussian state updates because Aₜ enters linearly after suitable re-writing. (Primiceri’s TVP-SV framework.)&lt;br&gt;
  
  3) **Draw the SV (hₜ) states** using the **Kim–Shephard–Chib (KSC) mixture** sampler (approximates the log-χ² density with a normal mixture to sample all volatilities efficiently).&lt;br&gt; 
  
  4) **Draw variance blocks**:  
     – **Q** for the β-evolution (IW/IG depending on parameterization); tight Q ⇒ slow coefficient drift, loose Q ⇒ faster drift.&lt;br&gt;
     
     – **S** for the Aₜ-state innovations and **W** for the hₜ-state innovations (typically IG/IW priors in practice). &lt;br&gt;
     
  5) **Respect the corrected step ordering**: Del Negro &amp; Primiceri show the original algorithm’s ordering should be changed for proper mixing/stability in TVP-SV VARs—**same steps, different order**. This is standard practice now. &lt;br&gt;

• Identification and interpretation (one sentence)  &lt;br&gt;
  – With **lower-triangular Aₜ** we have a **time-varying Cholesky** identification (ordering matters); alternative identification schemes (e.g., sign or external instruments) can be layered if the application requires it. &lt;br&gt;

• Practical pointers to say aloud  &lt;br&gt;
  – Start with reasonable shrinkage on the **Q/S/W** blocks to avoid spurious time variation; relax only if the data demand it.  &lt;br&gt;
  – Monitor **MCMC diagnostics** (trace/R-hat/ESS) and **VAR stability** (roots inside unit circle) draw-by-draw before reporting IRFs/forecasts.  &lt;br&gt;
  – For large systems, surveys recommend global–local shrinkage or hierarchical tightness to keep the sampler well-behaved. &lt;br&gt;

• One-liner close  &lt;br&gt;
  – “Heteroskedastic TVP-VARs let the **transmission matrix** and **volatility** evolve together, so we can tell whether instability comes from changing dynamics, changing uncertainty, or both.”&lt;br&gt; 


---
# Parameter Proliferation and Shrinkage
&lt;div style="font-size: 25px;"&gt;

--

- TVP–VARs have T times as many parameters as standard VARs.

--

- Hierarchical prior from state equation can provide sufficient shrinkage

--

- Shrinkage priors can be applied to the coefficients `\(\beta_t\)` to control overfitting and improve estimation accuracy.

--

- These priors include:

  - Minnesota Prior: Coefficients for own lags are set to 1 (random walk belief), while others are shrunk toward zero.
  - Hierarchical Minnesota Prior: Coefficients are shrunk based on their lag length, with stronger shrinkage for longer lags.
  - Adaptive Hierarchical Priors (AHPs): Coefficients are shrunk adaptively based on the data, allowing for different degrees of shrinkage across coefficients.

--

- Often advisable to use tight priors on Q

???
# Parameter Proliferation and Shrinkage — presenter notes (TVP–VAR)

• Why proliferation is severe  &lt;br&gt;
  – In a TVP–VAR every coefficient becomes a **time series**: instead of one β, we have a whole **path {β₁,…,β_T}**. That multiplies the parameter count and makes naïve estimation unstable—especially in medium/large systems. Surveys and applications stress that TVP models are **high-dimensional** and need regularization.&lt;br&gt;

• Role of the state-equation prior (baseline shrinkage)  &lt;br&gt;
  – The random-walk evolution for coefficients adds a **hierarchical prior** through the innovation covariance **Q**. Small elements of Q **shrink the drift** in βₜ toward constancy (i.e., toward a fixed-parameter VAR); larger Q permits faster structural change. Practical TVP algorithms emphasize setting/drawing Q carefully to avoid spurious time variation. &lt;br&gt;

• Shrinkage on the coefficients (β)  &lt;br&gt;
  – **Minnesota prior (and variants):** centers cross-lags near 0, keeps own first lags relatively loose, and imposes **lag-decay**—a proven default for VARs; hierarchical versions learn the tightness from the data. &lt;br&gt;
  – **Adaptive/Global–Local priors (e.g., Normal–Gamma, Horseshoe):** add a **global** tightness plus **local** scales per coefficient so irrelevant terms are shrunk hard while important ones “escape” shrinkage—effective in large (S)V VARs.&lt;br&gt;

• Why these priors matter in TVP settings &lt;br&gt; 
  – Time-varying models can be **over-parameterized**; modern work shows global–local shrinkage (NG, Horseshoe) improves inference/forecasting by inducing sparsity while allowing occasional large movements in βₜ.&lt;br&gt;

• Practical guidance to say aloud &lt;br&gt; 
  – Start with Minnesota-style structure; if k and p are large, **upgrade to adaptive (global–local) shrinkage**.  
  – Keep **Q relatively tight** unless the data clearly demand change—then let the sampler relax it; this controls spurious drift in βₜ.&lt;br&gt;

• One-liner &lt;br&gt; 
  – “TVP–VARs explode in parameters; **hierarchical and adaptive shrinkage** (on β and on Q) are the tools that keep the model stable and forecast-useful.”&lt;br&gt; 

---
# Shrinkage and Prior Choice
&lt;div style="font-size: 25px;"&gt;

--

- Non-informative priors for `\(Q\)` can assign excessive probability to large `\(Q\)` (high coefficient variation)

--

- Careful choice of hyperparameters for `\(Q\)` and shrinkage priors (e.g., Minnesota, SSVS) is important

--

- Even with shrinkage, time-varying coefficients mean impulse responses can differ substantially over time



???
# Shrinkage and Prior Choice — presenter notes (TVP–VAR)

• Why priors on **Q** matter  &lt;br&gt;
  – In TVP models, βₜ follows a (near) random walk. If **Q** is weakly or non-informative, the sampler can put too much mass on **large Q**, letting coefficients wander and **overfit** transitory noise. Standard guidance: use **tighter priors on the β-evolution variance** than on contemporaneous structure or volatility to avoid implausible drift. &lt;br&gt;

• What “tight on Q” does (plain English)  
  – Small Q ⇒ coefficients change **slowly**, trending only when data strongly demand it.  
  – Large Q ⇒ coefficients **wiggle a lot**, which can look like frequent “policy shifts” even when relationships are stable. Simulation and survey evidence emphasize this risk and the role of shrinkage. &lt;br&gt;

• Choosing hyperparameters and shrinkage priors  
  – **Minnesota-style shrinkage** for VAR coefficients: center cross-lags near 0, keep own first lag relatively loose, and impose lag-decay; hierarchical versions **learn tightness** from the data. Useful default even in large systems. &lt;br&gt; 
  
  – **SSVS (Stochastic Search Variable Selection)**: mixture/indicator prior that probabilistically switches coefficients “on/off,” encouraging sparse dynamics while allowing important terms to survive. Often paired with Minnesota or used on blocks (cross-lags, higher lags).&lt;br&gt;
  
  – In high-dimensional TVP-VARs, surveys show that combining hierarchical/global–local shrinkage with careful Q-priors stabilizes estimation and improves forecasts. &lt;br&gt;

• Even with shrinkage, IRFs will move over time  
  – Shrinkage **controls noise**, it doesn’t force invariance. When coefficients and/or volatilities truly evolve, **time-varying impulse responses** can differ a lot across dates; that is the point of TVP modeling. (Empirical overviews report large changes in IRFs across regimes.)&lt;br&gt;

• What to say you’ll do in practice  &lt;br&gt;
  – Start with Minnesota-type priors and **tight Q**; run prior-sensitivity checks (loosening Q, or allowing SV vs. homoskedasticity).  &lt;br&gt;
  – Prefer **hierarchical** tightness so the data tune overall shrinkage; consider SSVS for cross-lags/higher lags.  &lt;br&gt;
  – Judge prior settings by **out-of-sample forecasts**, stability of draws, and plausibility of **time-varying IRFs** (not just in-sample fit). &lt;br&gt;


This slide emphasizes that allowing coefficients to drift freely in a time-varying model can lead to over-fitting unless we constrain how much they’re allowed to move. If you choose completely diffuse (non-informative) priors for the coefficient‐innovation variance𝑄, the model may place too much weight on large values of Q, causing coefficients to wander wildly and impulse responses to swing erratically. &lt;br&gt;

- To prevent this, practitioners impose shrinkage priors—for example, Minnesota-style priors that pull parameter changes toward zero, or Stochastic Search Variable Selection (SSVS) priors that encourage many small coefficient innovations. Even with shrinkage, however, the very nature of a time-varying framework means that the dynamic responses to shocks can still vary substantially over time; the goal of shrinkage is simply to ensure that such variation is driven by genuine data evidence rather than noise.&lt;br&gt;

---
# Combining TVP with Minnesota or SSVS Priors
&lt;div style="font-size: 29px;"&gt;

--

- TVP–VAR can be combined with Minnesota or SSVS priors:

  - Minnesota prior shrinks coefficients for lags other than the own lag toward zero.
  - SSVS prior allows for variable selection by shrinking some coefficients to exactly zero.

`$$\beta_{t+1} = A_0 \beta_t + (I - A_0) \beta_0 + u_t$$`
- `\(A_0\)` and `\(\beta_0\)` set or estimated to reflect prior beliefs

- SSVS prior: allows for variable selection, promoting sparsity and reducing over-parameterisation


???
# Combining TVP with Minnesota or SSVS Priors — presenter notes

• Why combine?&lt;br&gt;
  – TVP–VARs are parameter-rich; **shrinkage** keeps them stable and forecast-useful.
  – Two popular options: **Minnesota** (structured shrinkage by lag/block) and **SSVS** (spike-and-slab variable selection). &lt;br&gt;

• Minnesota inside TVP
  – Use Minnesota beliefs to **center coefficients** and scale variances:
    • Own first lags relatively loose (persistence), cross/higher lags shrunk near 0.  
    • Hyperparameters (overall tightness, lag decay, cross-lag penalty) can be set or **learned hierarchically**. &lt;br&gt;
  – In TVP form, you can (i) place Minnesota on the **initial state** \(β₀\) and on **state-innovation scales** (so drift is small for cross/higher lags), or (ii) use a **mean-reverting evolution** where coefficients drift toward a Minnesota mean \(β₀\) (random-walk is the special case when mean reversion ≈ 0). &lt;br&gt;
    `$$\beta_{t+1} = A_0 \beta_t + (I - A_0) \beta_0 + u_t$$`
    – Here, \(A₀\) controls mean reversion (diagonal with elements in [0,1]); \(β₀\) is the Minnesota prior mean. Setting \(A₀\) near identity gives near-RW drift; smaller values pull coefficients back to \(β₀\). &lt;br&gt;
  – Practical effect: lets **own-lag persistence “breathe”** while discouraging noisy cross-lag movement over time. &lt;br&gt;

• SSVS inside TVP
  – **Stochastic Search Variable Selection** uses a **two-variance mixture** per coefficient: a tight “spike” (near zero) vs a loose “slab” (free). Binary indicators are sampled in the Gibbs loop to switch terms on/off. &lt;br&gt;
  – In VARs (and TVP-VARs), SSVS efficiently selects **lags/blocks/cross-effects**, yielding sparse dynamics without manual pre-testing. Applications demonstrate SSVS for VAR restrictions and forecasting (including TVP specifications). &lt;br&gt;
  – Implementation sketch: sample (β-paths | indicators), then **indicators | β**, then variance blocks; repeat. The selection can also be put on **elements of Q** so only a subset of coefficients is allowed to vary over time. &lt;br&gt;

• When to prefer which?&lt;br&gt;
  – **Minnesota**: transparent structure, fast, great baseline; use hierarchical tuning of tightness for robustness.&lt;br&gt;
  – **SSVS**: when the system is large and you suspect many **irrelevant cross/higher lags**; promotes sparsity and can improve forecasts. &lt;br&gt;
  – They are **complementary**: Minnesota for sensible scaling/means; SSVS on top for **automatic exclusion** of weak terms.&lt;br&gt;

• Practical tips to say aloud
  – Start Minnesota-TVP with **tight Q** on cross/higher lags; compare to SSVS-TVP by out-of-sample forecasts and time-varying IRFs.&lt;br&gt;
  – Report sensitivity to (i) overall tightness and lag decay, (ii) SSVS spike/slab variances; both sets of hyperparameters are interpretable and can be **data-driven**. &lt;br&gt;

• One-liner&lt;br&gt;
  – “Minnesota gives structured, interpretable shrinkage; SSVS adds automatic sparsity. In TVP-VARs, combining them yields **stable yet adaptive** time-varying dynamics.” &lt;br&gt;

- Combining TVP–VARs with shrinkage priors involves embedding your time-varying coefficient framework within a structure that tethers parameter drift to economically plausible bounds. Under a Minnesota-style prior, you impose tighter variance on cross-equation and higher-lag coefficients—shrinking them toward zero unless the data strongly support movement—while still allowing each coefficient to evolve over time. &lt;br&gt;

- Alternatively, an SSVS prior treats each coefficient innovation as a mixture of a “spike” (near zero variance) and a “slab” (larger variance), effectively selecting only those coefficients whose time-variation is warranted by the evidence and setting the rest (temporarily) to zero. &lt;br&gt;

- In both cases, you calibrate hyperparameters—either the Minnesota decay rates or the spike-and-slab variances and inclusion probabilities—to control the degree of allowed flexibility. This layered approach preserves the ability of TVP models to capture genuine structural change (“bad policy”) while guarding against over-fitting driven by noise (“bad luck”), promoting parsimonious, robust inference in high-dimensional VARs.&lt;br&gt;

---
# Hierarchical Priors and Layered State Eq
&lt;div style="font-size: 30px;"&gt;

- Can add another state equation (Chib &amp; Greenberg, 1995):

`$$\bf{\beta}_{t+1} = A_0 \bf{\theta}_{t+1} + \bf{u}_t \\ \bf{\theta}_{t+1} = \bf{\theta}_t + \bf{\eta}_t$$`

- `\(A_0\)` can encode shared structure across groups (e.g., panel VARs)

- Allows for parsimonious specification and information pooling

- The hierarchical prior structure allows for more flexible modeling of the time-varying coefficients, enabling the model to capture common patterns across different variables or groups.

???

- Hierarchical priors augment the basic TVP framework by introducing an extra layer of latent states that govern how the time-varying coefficients themselves evolve, enabling the model to “borrow strength” across related equations or units.&lt;br&gt;

- Concretely, instead of treating each coefficient path as an independent random walk, one posits that they are driven by a lower-dimensional set of factors or shared parameters—often denoted by a common state vector θₜ—multiplied by a loading matrix A₀.&lt;br&gt;

- This construction means that any group of coefficients can fluctuate coherently according to overarching dynamics captured by θₜ, while idiosyncratic movements are still accounted for via individual innovations uₜ.&lt;br&gt;

- In panel-VAR settings or models with many related variables, this layered approach drastically reduces the effective number of parameters, pools information across series, and focuses estimation on common structural shifts rather than noise. &lt;br&gt;

- The result is a parsimonious yet flexible specification that can adapt to evolving relationships both at the group level (through θₜ) and at the individual coefficient level (through uₜ), improving estimation stability and interpretability in high-dimensional TVP-VARs.&lt;br&gt;




---
# Imposing Inequality Restrictions
&lt;div style="font-size: 25px;"&gt;

- TVP–VARs may require stability: restrict roots of VAR polynomial to be outside the unit circle

--

- Inequality restrictions can be imposed on the coefficients to ensure stability and economic plausibility.

--

- These restrictions can be incorporated into the prior distributions or as constraints in the estimation process.

--

- Standard MCMC algorithms do not naturally handle such constraints

--

- Approaches:
  - Reject entire vector if any draw violates restriction (can be inefficient)
  
  - Single-move algorithms (draw one `\(\beta_t\)` at a time; slow mixing but better feasibility)
  
- See Koop &amp; Potter (2009) for algorithmic details.

???
- Inequality restrictions in TVP–VARs are imposed to ensure that, at each point in time, the VAR remains stationary by keeping the roots of its characteristic polynomial outside the unit circle—otherwise impulse responses can explode and economic interpretation breaks down. &lt;br&gt;


- Practitioners incorporate these restrictions either by truncating the prior so that disallowed coefficient values have zero probability or by rejecting any MCMC draw that yields instability. &lt;br&gt;


- The simplest “global‐move” approach—drawing the entire coefficient path via FFBS and discarding it if any period is unstable—is easy to implement but often suffers from extremely low acceptance rates.&lt;br&gt;

- A more efficient alternative is a “single‐move” or block‐update sampler, which perturbs one coefficient (or small block) at a time and checks stability locally; this improves feasibility at the cost of slower mixing across the full state space.&lt;br&gt;

- For detailed algorithms and exact‐sampling solutions, see Koop and Potter (2011), who develop both exact and approximate MCMC schemes that honor inequality constraints without relying on prior normalizing‐constant approximations. &lt;br&gt;

# Imposing Inequality Restrictions — presenter notes (TVP–VAR)

• Motivation (what/why)
  – In TVP–VARs we often want **stability at every t**: the VAR’s characteristic roots should lie **outside the unit circle** so the system is non-explosive; otherwise IRFs/forecasts can blow up. &lt;br&gt;

• Why it’s hard in MCMC
  – The **stationary region is a complicated, non-rectangular set**, so naïvely drawing coefficients and “rejecting if unstable” can be **inefficient or even incorrect** for state-space samplers. &lt;br&gt;
  
  – In medium/large VARs the chance a full draw satisfies stability **at every time t** can be tiny, which explains the poor mixing of brute-force rejection schemes. &lt;br&gt;

• What the literature recommends (high level)
  1) **Algorithms that enforce inequality constraints on states**  
     – Koop &amp; Potter develop **exact** MCMC methods for TVP-VARs with inequality-restricted states (e.g., stability at each t), noting standard multi-move smoothers can fail or mix poorly under such constraints. &lt;br&gt;
     – Their methods ensure draws always satisfy the constraints, avoiding the inefficiency of naïve rejection. &lt;br&gt;
  2) **Single-move/blocked state updates**  
     – Update one coefficient/state (or a small block) at a time **conditional on the rest**, checking the constraint locally; mixes more slowly but **dramatically improves feasibility** vs. all-at-once draws. &lt;br&gt;
  3) **Prior-based enforcement (“soft” constraints)**  
     – Put **stationarity-enforcing priors** so unstable regions have essentially zero prior mass; this avoids hard rejection while keeping draws in the stable set. &lt;br&gt;
  4) **Inequality restrictions beyond stability**  
     – The same machinery handles other inequalities (e.g., sign/zero/ordering constraints used for identification) within a Bayesian sampler. &lt;br&gt;

• Practical playbook you can say aloud
  – **Screen initialization**: start from a stable constant-parameter VAR, then switch on time variation.  &lt;br&gt;
  – **Tight evolution variance (Q)**: shrink β’s random-walk innovations so drift is modest unless data demand it; this reduces excursions to unstable regions. (You can still allow SV in Σₜ.) &lt;br&gt;
  
  – **Choose the enforcement style**:  
    • Small systems: hard stability **at every t** via constrained state updates.&lt;br&gt;
    
   • Larger systems: consider **prior-based** stationarity or enforce stability on key horizons (e.g., end-of-sample) to keep computation tractable. &lt;br&gt;
  – **Always monitor**: proportion of unstable proposals, acceptance, trace plots, and the share of posterior draws that satisfy stability.&lt;br&gt;

• One-liner to close&lt;br&gt;
  – “Inequality restrictions (like stability) are essential but non-trivial in TVP-VARs; use **constrained state updates or stationarity-enforcing priors** instead of naïve rejection, and keep Q tight so the sampler stays in the economically plausible region.” &lt;br&gt;

---
#  Stochastic Volatility in TVP–VARs
&lt;div style="font-size: 25px;"&gt;

- Previous discussion focused on homoskedastic TVP–VARs (constant `\(\Sigma\)`)

--

- TVP–VARs can incorporate stochastic volatility to capture time-varying error variances.

--

- Stochastic volatility allows the model to account for changing uncertainty in the data, which can be particularly important in macroeconomic applications.


--

- Allowing for multivariate stochastic volatility is critical in most empirical macroeconomics applications

--

- Stochastic volatility can be incorporated into the model by allowing the covariance matrix `\(\Sigma\)` to evolve over time according to a random walk or other stochastic process.

- The model can be expressed as:

`$$Z_t = \sum_{i=1}^{p} A_i(t) Z_{t-i} + u_t, \quad u_t \sim N(0, \Sigma(t))$$`

???
- Where:
  - `\(Z_t\)` is the `\((m+r)\times1\)` vector of observed variables and latent factors at time `\(t\)`.
  - `\(A_i(t)\)` is the `\((m+r)\times(m+r)\)` coefficient matrix for lag `\(i\)` at time `\(t\)`, allowing for time-varying coefficients.&lt;br&gt;
  - `\(\Sigma(t)\)` is the covariance matrix of the errors at time `\(t\)`, allowing for time-varying volatility.&lt;br&gt;
  

- The model assumes that the coefficients `\(A_i(t)\)` and the covariance matrix `\(\Sigma(t)\)` follow a random walk process, allowing them to change gradually over time.&lt;br&gt;

- The random walk prior on the coefficients allows for gradual changes, capturing the evolving nature of economic relationships.&lt;br&gt;

- The stochastic volatility component allows the model to account for changing uncertainty in the data, which can be particularly important in macroeconomic applications where volatility may vary due to structural changes or external shocks.&lt;br&gt;

---
# Connectedness in Bayesian TVP-VAR
&lt;div style="font-size: 17px;"&gt;
- **Concept**  
  - Connectedness captures how shocks in one variable transmit to others over time.  
  - In **Bayesian TVP-VAR**, these linkages are *time-varying*, reflecting evolving macroeconomic dynamics.  
- **Measurement**  
  - Based on **Forecast Error Variance Decompositions (FEVDs)**:  
    - Share of forecast error in variable *i* explained by shocks in variable *j*.  
  - **Generalized FEVD** avoids ordering problems.  
  - Connectedness indices summarize system-wide spillovers.  
- **Indices** (Diebold–Yilmaz style)  
  - **Total Connectedness Index (TCI):** Overall spillovers in the system.  
  - **Directional Connectedness:**  
    - *TO others*: variable’s shock influence on others.  
    - *FROM others*: variable’s vulnerability to shocks.  
  - **Net Connectedness:** Difference between *TO* and *FROM*.  
- **Applications**  
  - Financial contagion, monetary policy transmission, inflation–exchange rate–interest rate linkages
  - Useful for central banks and policymakers to track systemic risk &amp; policy effectiveness.  

---
# Example of TVP-BVAR
&lt;div style="font-size: 18px;"&gt;



``` r
library(tsaccessories)
library(Bayesiantvpvars)
library(dplyr)
library(lubridate)
library(zoo)
library(readxl)

data&lt;-read_excel("datawork/data.xlsx", sheet = "Sheet1")
df &lt;- tsconvert(data, start = c(1959, 1), period = "quarterly")

vars &lt;- c("inflation", "unemployment_rate", "interest_rate")
# new data frame with just those columns
df_sel &lt;- df[ , vars, drop = FALSE]
str(df_sel)
# Keep Date + variables, drop rows with NA in selected vars only
tmp &lt;- df %&gt;%
  transmute(Date = as.Date(Date), across(all_of(vars), as.numeric)) %&gt;%
  filter(if_all(all_of(vars), ~ !is.na(.)))

# if Date wasn’t a true Date, try parsing (remove this if already Date)
if (!inherits(tmp$Date, "Date")) {
  tmp$Date &lt;- suppressWarnings(lubridate::ymd(df$Date))[complete.cases(df[vars])]
}

# sort and handle any duplicate dates (here: average duplicates)
tmp &lt;- tmp %&gt;% arrange(Date) %&gt;%
  group_by(Date) %&gt;% summarise(across(all_of(vars), ~ mean(.x, na.rm = TRUE)), .groups = "drop")

X &lt;- as.matrix(tmp[, vars])
Z &lt;- zoo::zoo(X, order.by = tmp$Date)
p &lt;- 2
tau_auto &lt;- pick_tau(nrow(X), p) 

fit &lt;- obtvpvar(
  X, p = p, tau = tau_auto,
  nrep = 6000, nburn = 1000, thinfac = 5, verbose_every = 5000
)

irf_sel &lt;- obtvpvar_irfchoose(
  fit,
  impulses  = c("inflation", "unemployment_rate", "interest_rate"),          # names (case-insensitive, space/underscore-insensitive)
  responses = c("inflation", "unemployment_rate", "interest_rate"),
  nhor = 16
)
p_irfs_sel &lt;- plot_irfs_grid_selected(irf_sel, free_y = TRUE)
print(p_irfs_sel)

# 5) FEVD at a chosen time &amp; horizon via fevd_at_t() -----------
fevd_now &lt;- obtvpvar_fevd(fit, H = 12)     # median shares (with p05/p95)
# Paper-style stacked bar plot
p_fevd_now &lt;- obtvpvarplot_fevd(fevd_now, label_size = 2.8)
print(p_fevd_now)

# 6) Time-varying FEVD for selected responses ------------------
#    Uses fevd_overtime_selected() which internally uses .resolve_pos()
fevd_tv &lt;- obtvpvarfevdtm(
  fit,
  responses = c("inflation", "unemployment_rate"),  # select 2 responses to illustrate
  H = 12,
  t_start = 10,
  t_end   = NULL              # defaults to last available t
)

# Returns a named list of ggplots (one per selected response)
fevd_tv_plots &lt;- obtvpvarplotfevdtm(fevd_tv)
# Print them (patchwork or loop)
print(fevd_tv_plots)


# 7) Connectedness using TVP-VAR engine ------------------------
#    End-to-end: estimate time-domain connectedness &amp; grab plots

# then run connectedness
conn &lt;- connectedness(
  X = Z, p = p, H = 10, kappa = c(0.99, 0.99), corrected_tci = TRUE
)
```

---
# Example of TVP-BVAR
&lt;div style="font-size: 22px;"&gt;

.pull-left[
&lt;img src="img/tvpirf1.png" width="1000"/&gt;


- Inflation persistence:Positive and persistent, peaking around horizon 2–3 and then slowly declining. Confirms inflation inertia (Strong, self-driven)

- Policy rule: Interest rate reacts positively to inflation and negatively to unemployment → consistent with a Taylor-type rule.
]


.pull-left[

- Unemployment response: Positive → interest rate hikes increase unemployment over time, reflecting a demand-side trade-off.

- Policy trade-off: Monetary tightening lowers inflation but raises unemployment.

- Phillips Curve: Negative relationship between unemployment and inflation confirmed.

- Shock persistence: Unemployment shocks are the most persistent.

]



---
# Example of TVP-BVAR
&lt;div style="font-size: 22px;"&gt;

.pull-left[
&lt;img src="img/tmirf1.png" width="1000"/&gt;


- The IRFs of unemployment to an interest rate shock reveal clear time variation. During the pre-Volcker period (1969), interest rate hikes raised unemployment persistently, peaking after about three years.
]


.pull-left[

- In the early 1980s (1983Q4), the effect was strongest and most prolonged, consistent with the costs of the Volcker disinflation.

- By the late 1990s, the response remained significant but less persistent, reflecting improved policy credibility.

- In contrast, the post-crisis period (2012) shows a much weaker and shorter-lived effect, consistent with diminished monetary transmission at the zero lower bound.

- These results underscore the evolving trade-offs between monetary tightening and labor market outcomes across regimes.

]


---
# Example of TVP-BVAR
&lt;div style="font-size: 26px;"&gt;

.pull-left[
&lt;img src="img/tpvfevd.png" width="1000"/&gt;


- Inflation is highly self-driven with limited influence from monetary policy or labor market shocks at this horizon.
]


.pull-left[

- Interest rates are partly policy autonomous, but unemployment plays a large role in driving interest rate dynamics — consistent with central banks reacting to labor market conditions (countercyclical policy). Inflation explains only a modest share.

- Unemployment is primarily determined by its own innovations, but inflation explains a non-trivial portion (~20%), consistent with a Phillips-curve style channel.

]


---
# Example of TVP-BVAR
&lt;div style="font-size: 22px;"&gt;

.pull-left[
&lt;img src="img/TVPFEVD.png" width="1000"/&gt;

- Unemployment rigidity: ~75% of its variance explained by own shocks → labor market is mostly self-driven.

- Inflation–unemployment link: ~20% explained by inflation shocks → evidence of a persistent Phillips curve effect.



]


.pull-left[

- Weak monetary policy channel: Interest rate shocks explain very little (2–4%), showing that policy shocks matter less than inflationary dynamics for unemployment.
- The time-varying FEVD at horizon 12 shows that unemployment dynamics are overwhelmingly explained by own shocks (≈75%), with inflation shocks accounting for ≈20% and interest rate shocks contributing only marginally (≈2–4%). 
- Importantly, these shares remain stable across the sample, underscoring the structural persistence of unemployment in the economy. 
- This suggests that while inflationary dynamics exert some influence on labor market fluctuations, the unemployment process is largely self-driven, and the role of monetary policy shocks is limited.
]


---
# Example of TVP-BVAR
&lt;div style="font-size: 22px;"&gt;

.pull-left[

### Total Connectedness Index
&lt;img src="img/cn1.png" width="600"/&gt;

- The Total Connectedness Index (TCI) fluctuates between 35% and 55% over the sample period, indicating a moderate-to-high level of shock transmission across variables.




]


.pull-left[
- Connectedness was relatively low in the 1960s (≈30–35%) but increased significantly during the late 1970s and early 1980s, coinciding with global oil shocks and disinflationary policies.

- Since the 1990s, the index has remained persistently high (≈45–50%), reflecting greater macro-financial integration and stronger interdependence of shocks. 

- This pattern highlights that the economy has become progressively more interconnected, with policy and macro shocks exerting broader systemic effects.

]



---
# Example of TVP-BVAR
&lt;div style="font-size: 22px;"&gt;

.pull-left[

### Directional TO others

&lt;img src="img/cn2.png" width="600"/&gt;

- The directional connectedness analysis reveals that inflation was the dominant transmitter of shocks in the 1970s, reflecting supply-side shocks and inflationary spirals.




]


.pull-left[
- During the 1980s, monetary policy (interest rates) became the primary transmitter, consistent with the Volcker disinflation.

- From the 1990s onwards, unemployment shocks gained prominence as transmitters, indicating that labor market dynamics have become central in propagating macroeconomic shocks. 

- This shift underscores the evolving nature of systemic risk transmission in line with structural reforms, monetary policy credibility, and labor market rigidities.

]



---
# Example of TVP-BVAR
&lt;div style="font-size: 22px;"&gt;

.pull-left[

### The directional FROM others

&lt;img src="img/cn3.png" width="600"/&gt;

- The decomposition reveals that interest rates are persistently the most reactive variable, absorbing shocks from inflation and unemployment in line with systematic policy responses.




]


.pull-left[

- Inflation, initially self-driven, became highly influenced by external shocks during the 1970s–1980s crises before stabilizing under credible monetary regimes.

- Unemployment remains largely self-driven but absorbs some shocks during recessions, especially in the early 1980s.

- Together with the ‘TO others’ results, this highlights an evolving structure of macroeconomic spillovers: inflation as a dominant transmitter in the 1970s, interest rates as the main responder, and unemployment increasingly acting as a shock transmitter in recent decades.

]


---
# Example of TVP-BVAR
&lt;div style="font-size: 20px;"&gt;

.pull-left[

### Net Connectedness
&lt;img src="img/cn4.png" width="600"/&gt;

- The net connectedness results highlight regime shifts in systemic shock transmission.




]


.pull-left[

- Inflation was the dominant transmitter in the 1960s–1970s, but became a net receiver during the Volcker disinflation of the early 1980s, reflecting the dominance of monetary tightening.
- Interest rates appear persistently as net receivers, consistent with their role as reactive policy instruments, except during extraordinary disinflationary episodes when they briefly transmit shocks.
- Unemployment, by contrast, emerges as a persistent net transmitter from the 1980s onwards, suggesting that labor market shocks play a central role in driving macroeconomic connectedness in the later period.
- This dynamic underscores the evolving nature of systemic risk transmission in line with structural reforms, monetary policy credibility, and labor market rigidities.

]






---
# Example of TVP-BVAR
&lt;div style="font-size: 20px;"&gt;

.pull-left[

### Connectedness Network Graph
&lt;img src="img/cn5.png" width="600"/&gt;




]


.pull-left[

- The connectedness network reveals that unemployment is the dominant transmitter of shocks, with strong spillovers to both interest rates and inflation.

- Interest rates serve as a conduit, absorbing shocks from unemployment and transmitting them to inflation, consistent with a Taylor-rule framework where policy responds to labor market conditions and influences price stability. 

- Inflation, by contrast, appears as a net receiver of shocks, reflecting its dependence on labor market pressures and monetary policy actions rather than acting as a primary source of systemic spillovers.
]

---
# From VAR to SVAR 
&lt;div style="font-size: 30px;"&gt;
- A standard VAR can be written in reduced form as:
`$$y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + \cdots + A_p y_{t-p} + u_t, \quad u_t \sim N(0, \Sigma)$$`

- where `\(y_t\)` is a vector of endogenous variables, `\(c\)` is a vector of intercepts, `\(A_i\)` are coefficient matrices, and `\(u_t\)` are reduced-form errors with covariance matrix `\(\Sigma\)`.

--

- However, the reduced-form errors `\(u_t\)` are generally correlated across equations, making it difficult to interpret the shocks economically.

--

- To obtain economically meaningful shocks, we need to transform the reduced-form errors into structural shocks that are uncorrelated and have clear interpretations.

--

- This is done by specifying a structural VAR (SVAR) model, which imposes additional restrictions to identify the structural shocks.

---
# From VAR to SVAR 
&lt;div style="font-size: 24px;"&gt;

- A common way to represent the SVAR is:

--

 `$$A_0 y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + \cdots + A_p y_{t-p} + \varepsilon_t, \quad \varepsilon_t \sim N(0, I)$$`

- where `\(A_0\)` is a contemporaneous impact matrix that relates the structural shocks `\(\varepsilon_t\)` to the observed variables `\(y_t\)`.

- The structural shocks `\(\varepsilon_t\)` are assumed to be uncorrelated with each other and have unit variance.

--

- The relationship between the .my-coral[**reduced-form errors**] `\(u_t\)` and the .my-coral[**structural shocks**] `\(\varepsilon_t\)` are linked by
`$$u_t \equiv A_0^{-1}\varepsilon_t\,,\qquad \Sigma = A_0^{-1} (A_0^{-1})' \,.$$`

We call `\(B_0 \equiv A_0^{-1}\)` the **impact matrix**. Identifying the model = learning `\(B_0\)` (up to signs/labels).

---
# From VAR to SVAR 
&lt;div style="font-size: 30px;"&gt;



- The covariance matrix of the reduced-form errors `\(\Sigma\)` is related to the impact matrix `\(A_0\)` by:

`$$\Sigma = A_0^{-1} (A_0^{-1})'$$`

- To identify the structural shocks, we need to impose restrictions on `\(A_0\)`.

- Common identification strategies include:

  - Short-run restrictions (e.g., Cholesky decomposition)
  
  - Long-run restrictions (e.g., Blanchard-Quah)
  
  - Sign restrictions
  


---
# Identification in SVARs

- Identification is crucial in SVARs to obtain economically meaningful shocks.

- Without identification, the structural shocks cannot be uniquely determined from the reduced-form errors.

- The choice of identification strategy depends on the economic theory and context of the analysis.

- Different identification strategies impose different types of restrictions on the impact matrix `\(A_0\)`.

- The chosen restrictions should be justified based on economic theory and empirical evidence.

- The validity of the identification strategy can be assessed through robustness checks and sensitivity analyses.


???
# Identification in SVARs — presenter notes

• Why identification matters  
  – The reduced-form VAR only pins down the covariance of errors; **structural shocks are not unique without restrictions** (rotation problem). Identification supplies extra information so shocks are economically interpretable. Great overviews: Kilian &amp; Lütkepohl; Stock’s lecture notes. &lt;br&gt;

• Main identification families (what to tell the room)
  1) **Short-run (A₀) / Cholesky-type zero restrictions**  
     – Set some **impact** responses to zero using theory/timing. Fast, transparent; sensitive to variable ordering. See textbook treatments. &lt;br&gt;
  2) **Long-run restrictions (Blanchard–Quah)**  
     – Impose that certain shocks have **no permanent effect** on some variables (e.g., demand has no long-run effect on output). Useful for supply vs demand splits, but can be fragile in small samples. &lt;br&gt; 
  3) **Sign restrictions (Uhlig)**  
     – Constrain the **sign of impulse responses** over short horizons (e.g., contractionary monetary shock: ↑policy rate, ↓prices). Flexible and agnostic, but can leave sets of admissible shocks and is sensitive to design; see critiques and refinements. :&lt;br&gt;
  4) **External instruments / Proxy-SVAR (SVAR-IV)**  
     – Use an external series that is **correlated with the target shock but orthogonal to others** (e.g., high-frequency surprises, narrative shocks). Delivers point-identified IRFs under instrument relevance/exogeneity. &lt;br&gt; 
  5) **Identification through heteroskedasticity**  
     – Exploit **changes in volatility** (e.g., regimes) to separate shocks without zero/sign assumptions. Works when variances shift while the contemporaneous matrix is stable. &lt;br&gt; 
  6) **Hybrids**  
     – Combine sign restrictions with external instruments to sharpen identification and mitigate each method’s weaknesses. &lt;br&gt;
     

• Picking a strategy (how to guide the audience)&lt;br&gt;
  – **Start from theory &amp; context:** What shock? What institutional timing or neutrality beliefs are credible? (short-run vs long-run vs signs). &lt;br&gt;
  
  – **Data features:** If there are clear volatility regimes or good instruments, consider **heteroskedasticity** or **SVAR-IV**. &lt;br&gt;
  
  – **Transparency vs flexibility:** Cholesky is simple but order-dependent; sign restrictions are flexible but set-identified; SVAR-IV gives causal interpretation if instruments are strong and valid.&lt;br&gt;

• Good practice &amp; robustness (what to report)&lt;br&gt;
  – Show **sensitivity** to ordering, horizon windows for signs, alternative instruments, and admissible set summaries (median target, coverage). &lt;br&gt;
  
  – Check **first-stage strength** and exogeneity for instruments; document F-stats/covariances. &lt;br&gt;
  
  – Where feasible, **triangulate**: compare results across identification schemes (e.g., signs vs SVAR-IV). &lt;br&gt;

• One-liner to close&lt;br&gt;  
  – “SVAR identification is about adding credible restrictions—zeros, long-run, signs, instruments, or volatility—so shocks are uniquely interpretable; always pair the choice with **theory, diagnostics, and robustness checks**.” &lt;br&gt;

---
# Bayesian reduced‑form (conjugate) setup

Let `\(T\)` be sample size and define `\(X\)` with a column of ones and lagged `\(y_t\)`. A standard conjugate prior is **Matrix‑Normal–Inverse‑Wishart (MNIW)**:

`$$\Sigma \sim \mathcal{IW}(\nu_0,S_0), \qquad
\Phi \mid \Sigma \sim \mathcal{MN}(\Phi_0,\, \Sigma,\, V_0)\,.$$`

The **posterior** is also MNIW:
`$$V_T = (V_0^{-1} + X'X)^{-1},\qquad
\Phi_T = V_T\big(V_0^{-1}\Phi_0 + X'Y\big)\,,$$`

`$$\nu_T = \nu_0 + T,\qquad
S_T = S_0 + (Y - X\Phi_T)'(Y - X\Phi_T) + (\Phi_T-\Phi_0)'V_0^{-1}(\Phi_T-\Phi_0)\,.$$`

**Sampling** one draw from the reduced‑form posterior:
1. Draw `\(\Sigma^{(s)} \sim \mathcal{IW}(\nu_T,S_T)\)`.
2. Draw `\(\Phi^{(s)} \sim \mathcal{MN}(\Phi_T,\, \Sigma^{(s)},\, V_T)\)`.

Minnesota‑type priors can be approximated within this conjugate family via suitable `\(\Phi_0,V_0\)` choices.

???
# Bayesian reduced-form (conjugate) setup — presenter notes

• What the symbols represent (no equations)&lt;br&gt;
  – **Σ**: covariance matrix of reduced-form shocks (how equation errors co-move). &lt;br&gt;
  – **Φ**: all VAR coefficients stacked (intercepts + lag coefficients, one column per equation). &lt;br&gt; 
  – **X, Y**: stacked regressor matrix and response matrix from the VAR written as a multivariate regression.&lt;br&gt;

• What “Matrix-Normal / Inverse-Wishart (MNIW) prior” means&lt;br&gt;
  – We place a **matrix-normal prior on Φ conditional on Σ** and an **inverse-Wishart prior on Σ**. &lt;br&gt; 
  – This choice is **conjugate** for a Gaussian likelihood: the **posterior stays in the same family** with updated hyperparameters that blend prior information and sample information.&lt;br&gt;

• How to *read* the posterior updates on the slide (intuition only)&lt;br&gt;
  – The updated **coefficient covariance** combines prior precision with sample precision (from `\((X^\top X)\)`; more data ⇒ more weight on the sample.  &lt;br&gt;
  – The updated **coefficient mean** blends the prior mean with the least-squares fit `\((X^\top Y)\)`, weighted by those precisions. &lt;br&gt; 
  – The updated **scale/df for Σ** adds a “sum of squared residuals” term to the prior scale; more data ⇒ tighter uncertainty about Σ.&lt;br&gt;

• How to **draw one posterior sample** (the two-step recipe)&lt;br&gt;
  1) **Draw Σ** from its inverse-Wishart posterior (cross-equation variability).  &lt;br&gt;
  2) **Draw Φ | Σ** from a matrix-normal with the posterior mean and the updated row/column scales. &lt;br&gt; 
  – These two steps give a full draw `\(((Φ^{(s)}, Σ^{(s)})\)` used for forecasts and IRFs.&lt;br&gt;

• Where **Minnesota-type beliefs** fit in&lt;br&gt;
  – You can **encode Minnesota structure inside MNIW** by choosing the prior **mean** \(Φ_0\) (e.g., own-lag persistence, cross-lags near zero) and the prior **covariance** \(V_0\) (lag-decay, scaling by residual std. devs.).  &lt;br&gt;
  – This keeps conjugacy (fast computation) while providing sensible shrinkage.&lt;br&gt;

• Quick **dimension check** (helps when coding)&lt;br&gt;
  – If there are `\((k)\)` variables and `\((p)\)` lags:  &lt;br&gt;
    • `\(Y\)`: `\((T \times k)\)` (rows after lagging).  &lt;br&gt;
    • `\(X\)`: `\((T \times (1+kp)\)` (intercept + all lags).  &lt;br&gt;
    • `\(Φ\)`: `\((1+kp) \times k)\)`.  &lt;br&gt;
  – In the matrix-normal, **Σ** scales **columns** (equations) and the updated **(V)** scales **rows** (regressors).&lt;br&gt;

• Why this setup is popular&lt;br&gt;
  – **Speed &amp; stability:** closed-form updates → efficient Gibbs sampling (draw Σ, then Φ).  &lt;br&gt;
  – **Forecasting:** each draw gives full predictive paths; averaging across draws yields **density forecasts** and **credible bands**.  &lt;br&gt;
  – **Interpretability:** priors `\((Φ_0, V_0)\)` are an explicit, tunable way to embed domain beliefs (e.g., Minnesota).&lt;br&gt;

• Practical tips to say aloud
  – Standardize variables or use appropriate scaling so `\((V_0)\)` works as intended. &lt;br&gt; 
  – Avoid ultra-diffuse priors in small samples; use shrinkage to stabilize multi-equation estimates.&lt;br&gt;  
  – Check that Σ draws are positive-definite and monitor convergence before reporting IRFs/forecasts.&lt;br&gt;

---
# From reduced‑form posterior to **structural** posterior

Let the Cholesky of `\(\Sigma^{(s)}\)` be `\(\Sigma^{(s)} = P P'\)` with `\(P\)` lower‑triangular. Any **orthonormal** matrix `\(Q\)` (`\(Q'Q=I\)`) generates a **candidate impact matrix**

`$$B_0^{(s)} = P\,Q \quad\Rightarrow\quad \Sigma^{(s)} = B_0^{(s)} (B_0^{(s)})' \,.$$`

&gt; **Key Bayesian device (RRWZ 2010):** draw `\(Q\)` from the **Haar** measure (e.g., QR of i.i.d. `\(\mathcal N(0,1)\)` matrix, with sign fix), compute implied **structural IRFs**, and **accept** `\(Q\)` iff the chosen identification **restrictions** hold.

Given `\((\Phi^{(s)}, B_0^{(s)})\)`, **structural MA** and IRFs follow from the reduced‑form MA `\(D_h\)`:

`$$y_t = \sum_{h=0}^{\infty} D_h e_{t-h} = \sum_{h=0}^{\infty} C_h \varepsilon_{t-h},
\qquad C_h = D_h B_0 \,,$$`

with `\(D_0=I\)`, and for `\(h\ge 1\)`, `\(D_h = \sum_{j=1}^{\min(p,h)} A_j D_{h-j}\)`.

???
# From reduced-form posterior to **structural** posterior — presenter notes

• Goal in one line  &lt;br&gt;
  – We start from a posterior draw of the **reduced-form** VAR, i.e., coefficients Φ^(s) and covariance Σ^(s). We want a corresponding draw of **structural objects** (impact matrix B₀^(s) and structural IRFs). &lt;br&gt;

• Key device (RRWZ 2010) — how to get a candidate **impact matrix**  &lt;br&gt;
  1) Factor the reduced-form covariance with a **Cholesky**: Σ^(s) = P P′ (P lower triangular). &lt;br&gt; 
  2) Draw an **orthonormal matrix Q uniformly** from the orthogonal group (the **Haar** distribution). A practical recipe: take a random matrix with i.i.d. N(0,1) entries and use a **QR** decomposition; with a sign fix on the diagonal of R, the Q you get is Haar.  &lt;br&gt;
  3) Form a **candidate impact matrix** as B₀^(s) = P Q. By construction this reproduces Σ^(s) because B₀^(s) B₀^(s)′ = Σ^(s).  &lt;br&gt;
  – This is the standard RRWZ algorithm used in modern SVAR work.&lt;br&gt; 
  
• Enforcing identification **restrictions**  &lt;br&gt;
  – With a candidate B₀^(s) in hand, **check the user’s identifying restrictions** (e.g., sign restrictions over certain horizons; zero restrictions; long-run neutrality constraints). &lt;br&gt; 
  – **Accept** Q (hence B₀^(s)) **iff** all restrictions hold; otherwise **draw another Q** and test again. This accept–reject loop delivers draws from the **structural posterior** consistent with your reduced-form draw and your identification scheme. &lt;br&gt;

• From impact to **IRFs**  &lt;br&gt;
  – Given Φ^(s) and an accepted B₀^(s), compute the **moving-average (MA) coefficients** of the VAR by standard recursion from the reduced-form coefficients (start at the identity and iterate using lag polynomials).  &lt;br&gt;
  – Multiply those MA coefficients by B₀^(s) to obtain **structural impulse responses** for each horizon. Repeat for many draws to get posterior bands. &lt;br&gt;

• Why the Haar draw matters (intuition you can say aloud)  &lt;br&gt;
  – The **Haar distribution** is the unique rotation-invariant uniform distribution on orthogonal matrices, so every admissible rotation is sampled **without bias**. QR-from-Gaussian is a fast way to sample it in practice. &lt;br&gt;

• Practical tips for the audience  &lt;br&gt;
  – **Sign/zero windows:** specify horizons and directions clearly; acceptance rates depend on how tight the restrictions are.  &lt;br&gt;
  – **Normalizations:** fix column signs (and possibly column ordering) so shocks are comparable across draws.  &lt;br&gt;
  – **Empty sets:** if acceptance is near zero, relax horizons or revisit the identifying assumptions.&lt;br&gt;  
  – **Posterior summaries:** report median/quantile IRFs and, where relevant, the share of accepted draws that satisfy the restrictions. &lt;br&gt;

• One-liner wrap-up  &lt;br&gt;
  – “For each reduced-form draw, rotate the Cholesky by a Haar-uniform Q, keep only rotations that satisfy your identifying restrictions, and compute IRFs—those accepted rotations are your **structural posterior**.” &lt;br&gt;

---
#Identification by **restrictions on `\(B_0\)` (impact)** — “zero/recursive”

**Recursive (short‑run) scheme** fixes a lower‑triangular `\(A_0\)` with ones on the diagonal, hence a lower‑triangular `\(B_0=A_0^{-1}\)`.
Example for `\((\pi_t, y_t, i_t)\)` ordered as prices, output, interest rate:
`$$A_0 =
\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
a_{21} &amp; 1 &amp; 0 \\
a_{31} &amp; a_{32} &amp; 1
\end{bmatrix},
\qquad
B_0 = A_0^{-1}\,.$$`

**Economic reading:** `\(i_t\)` can react within‑period to `\(\pi_t,y_t\)`, but not vice‑versa. In Bayesian inference, keep only `\(Q\)` that reproduce this zero pattern for `\(A_0=B_0^{-1}\)`.

**General impact zero/calibration:** Impose `\((B_0)_{ij}=0\)` for some `\((i,j)\)`, or fix scales `\((B_0)_{jj}&gt;0\)`. These are **linear** constraints in `\(A_0\)` elements.

???
# Identification by **zero/recursive** restrictions — presenter notes

• What “recursive / short-run zero” means &lt;br&gt; 
  – Impose a **lower-triangular impact matrix** (ones on the diagonal). Economically: variables ordered earlier can affect later variables **within the period**, but not vice-versa; later variables respond only with a lag to earlier ones. This is the classic **Cholesky** (short-run exclusion) identification. &lt;br&gt;

• How it’s implemented in practice  &lt;br&gt;
  – From a reduced-form covariance draw, take its **Cholesky factor**; that factor is the **impact matrix** consistent with the recursive zero pattern. In Bayesian or frequentist work, this corresponds to setting the upper-triangular entries to **zero** by construction. &lt;br&gt;

• Relationship to the “rotation” view  &lt;br&gt;
  – Any valid impact matrix reproducing the same reduced-form covariance can be written as a **Cholesky factor times an orthonormal rotation**. The recursive scheme picks the **no-rotation** case (the Cholesky itself), i.e., the unique lower-triangular candidate with positive diagonal.&lt;br&gt; 

• Economic reading to vocalize  &lt;br&gt;
  – Ordering = **timing assumption**: if variables are `\([ \pi_t, y_t, i_t ]\)`, the policy rate reacts contemporaneously to prices and output, but prices/output **do not** respond within-period to the policy shock. Choose an order that matches institutional timing or theory for your application. &lt;br&gt;

• Calibration / linear constraints you can add  &lt;br&gt;
  – You may set specific **zeros** in the impact matrix (beyond triangularity) or **fix/normalize** diagonal elements (e.g., make them positive or set units). These are linear restrictions on the structural matrix’s entries and are easy to enforce in recursive setups. &lt;br&gt;

• Strengths and caveats to mention  &lt;br&gt;
  – **Pros:** fast, transparent, and easy to communicate; produces point-identified shocks and IRFs.&lt;br&gt;
  
  – **Caveats:** results depend on **variable ordering**; if the timing assumptions are questionable, consider alternatives (sign restrictions, long-run restrictions, external instruments) or test multiple plausible orderings. &lt;br&gt;

• Presenter tip  &lt;br&gt;
  – When showing IRFs, remind the audience which **ordering** you used and why it’s credible in your context; if you’ve checked robustness to re-orderings, say so explicitly. (Audiences expect this with recursive identification.) &lt;br&gt;

---
#Identification by **sign restrictions**

Specify signs for selected **impulse responses** over horizons `\(h \in \mathcal H\)`. Example (**contractionary monetary shock**, variable order `\((\pi, y, i)\)`):
 `$$\text{At } h=0:\quad \Delta i \;&gt;\; 0$$`
 `$$h \in \{1,\ldots,4\}:\quad \Delta y \;\le\; 0$$`
 `$$h \in \{4,\ldots,12\}:\quad \Delta \pi \;\le\; 0$$`

Algorithm (Uhlig 2005; Rubio‑Ramírez, Waggoner &amp; Zha 2010):
1. Draw `\((\Phi^{(s)},\Sigma^{(s)})\)` from the reduced‑form posterior.
2. Factor `\(\Sigma^{(s)}=PP'\)`; draw `\(Q\)` orthonormal (Haar).
3. Form `\(B_0^{(s)}=PQ\)` and IRFs `\(C_h^{(s)}=D_h^{(s)}B_0^{(s)}\)`.
4. **Accept** if all sign constraints hold (otherwise redraw `\(Q\)`).

This yields **posterior sets** of admissible IRFs/shocks; report medians and credible bands over accepted draws.


???
# Identification by **long-run restrictions** — presenter notes

• What “long-run restriction” means  &lt;br&gt;
  – We impose restrictions on the **total accumulated (infinite-horizon) effect** of a structural shock on the variables. This is encoded in the **long-run impact matrix** (sum of all MA coefficients multiplied by the impact matrix). &lt;br&gt;
  – Economically, this often reflects **neutrality** or **steady-state** beliefs (e.g., demand shocks do not affect output in the long run). &lt;br&gt;
• Canonical example to say aloud (Blanchard–Quah)  &lt;br&gt;
  – In a two-variable system (output level and unemployment or output level and prices), **aggregate demand shocks have **no long-run effect** on the level of output**, whereas supply shocks may have permanent effects. This single zero restriction delivers identification. &lt;br&gt;

• How we implement it for each posterior draw (high-level recipe)  &lt;br&gt;
  1) From the reduced-form draw `\(((\Phi^{(s)}, \Sigma^{(s)}))\)`, compute the **MA representation** and its **infinite-horizon sum** (the long-run multiplier). Software and notes show that this is obtained from the VAR coefficients without simulation of infinite horizons. &lt;br&gt;
  
  2) Combine with a candidate **impact matrix** and **enforce linear restrictions** on the long-run multiplier (e.g., set selected entries to zero). Packages provide direct ways to impose these constraints on the long-run matrix. &lt;br&gt;
  
  3) If using a rotation approach: generate admissible rotations (e.g., by QR/Haar), **accept only those** whose long-run constraints hold, and compute IRFs from accepted draws. (Same accept–reject idea used for other SVAR restrictions.) &lt;br&gt;

• Stationarity / integration considerations you can mention&lt;br&gt;  
  – For **stationary VARs**, the long-run sum exists directly. &lt;br&gt; 
  – With **I(1) data** (as in Blanchard–Quah), long-run restrictions are typically imposed on the **levels** using a **VECM** or differenced VAR with appropriate mapping; textbook treatments explain the required conditions. &lt;br&gt;
  

• Why use long-run restrictions  &lt;br&gt;
  – They exploit **economic neutrality/steady-state** ideas (e.g., demand neutrality for output in the long run) to separate shocks when short-run timing is less credible. Widely used in macro applications and discussed in modern surveys. &lt;br&gt;
  

• Practical tips / cautions to say aloud &lt;br&gt; 
  – Results can be **sensitive to lag length**, unit-root/cointegration specification, and small-sample uncertainty—report robustness (alternative lags, sample windows). &lt;br&gt;
  – Always check that the **long-run matrix is well-defined** (VAR stability or correct VECM mapping) before imposing restrictions. &lt;br&gt;

• One-liner  &lt;br&gt;
  – “Identify shocks by pinning down their **permanent effects**: compute the long-run multiplier from each reduced-form draw, **impose the zero (or sign) restrictions on that multiplier**, and keep only solutions that satisfy them.”&lt;br&gt;

---
#Identification by **long‑run restrictions**

Define the **long‑run impact matrix**

`$$C(1) \equiv \sum_{h=0}^{\infty} C_h = \left(\sum_{h=0}^{\infty} D_h\right) B_0 \,.$$`

**Blanchard–Quah (1989) example (two‑variable case):** demand shocks have **zero long‑run** effect on the level of output,
`$$\big[C(1)\big]_{y,\;\varepsilon^{\text{demand}}} = 0 \,,$$`

while supply shocks may have permanent effects. In practice, enforce linear restrictions on `\(C(1)\)` using the reduced‑form `\(D_h\)` from each posterior draw.

???
# Identification by **long-run restrictions** — presenter notes

• Intuition (say first)&lt;br&gt;
  – We identify shocks by pinning down their **total (infinite-horizon) effect** on variables.  
  – The *long-run impact matrix* is the **sum of all impulse responses**; in a stable VAR this equals a closed-form function of the VAR lags (so we don’t need to literally sum to infinity).&lt;br&gt;

• Canonical example (Blanchard–Quah)&lt;br&gt;
  – In a 2-variable system with output and (say) unemployment/prices, impose: **demand shocks have zero long-run effect on the level of output**; supply shocks may have permanent effects. That single zero restriction delivers identification. &lt;br&gt;

• How we implement this for each posterior draw (recipe)&lt;br&gt;
  1) From the reduced-form draw `\(((\Phi^{(s)},\Sigma^{(s)}))\)`, compute the **long-run multiplier** of the VAR (the sum of MA coefficients, or equivalently the closed-form long-run matrix implied by `\((\Phi^{(s)})\)` when roots are inside the unit circle).&lt;br&gt; 
  
  2) Combine that long-run multiplier with a candidate **impact matrix** `\((B_0^{(s)})\)`.&lt;br&gt;  
  3) **Enforce linear long-run restrictions** (e.g., set selected entries of the long-run matrix to zero). If you sample rotations (QR/Haar), **accept only those** that satisfy the long-run constraints; otherwise, draw another rotation and re-test. &lt;br&gt;
  
  4) With an accepted `\((B_0^{(s)})\)`, recover **structural IRFs** in the usual way (recursion for MA coefficients × `\((B_0^{(s)}))\)`. &lt;br&gt;
  

• Practical notes you can say aloud&lt;br&gt;
  – **Stationarity vs I(1):** The closed-form long-run sum requires a stable VAR. With unit roots/cointegration (as in BQ), impose long-run restrictions in a **VECM** framework or use the appropriate mapping from the VAR in differences to levels.&lt;br&gt; 
  
  – **Sensitivity:** Results can depend on lag length and sample; show robustness (alternative lags/windows). &lt;br&gt;
  
  – **Software hint:** Many packages compute the long-run multiplier and apply BQ-style constraints directly (e.g., deriving `\((B_0)\)` from the long-run matrix and `\((\Sigma))\)`. &lt;br&gt;

• One-liner to close&lt;br&gt;
  – “Long-run identification = constrain **permanent effects**. Compute the long-run multiplier from each reduced-form draw, **impose zero (or sign) restrictions on that multiplier**, accept admissible solutions, and read off the structural IRFs.”&lt;br&gt; 

---
#  Worked patterns you can reuse

**A) Recursive monetary policy (impact zeros on `\(A_0\)`)**  
Order `\((\pi, y, i)\)`, constrain

`$$A_0 = \begin{bmatrix} 1 &amp; 0 &amp; 0\\ * &amp; 1 &amp; 0\\
* &amp; * &amp; 1 \end{bmatrix} \quad\Longleftrightarrow\quad (B_0)_{12}=(B_0)_{13}=(B_0)_{23}=0.$$`

**B) Fiscal SVAR (impact restrictions on `\(B_0\)`)**  
Order `\((\text{tax},\text{spend},y)\)`, constrain contemporaneous tax/spend elasticities via
`$$(B_0)_{1,3} = \kappa_\tau,\qquad (B_0)_{2,3} = 0,\qquad (B_0)_{3,1} = 0,$$`
with `\(\kappa_\tau\)` calibrated from elasticities; others free (orthogonality maintained).

---
#  Worked patterns you can reuse

**C) Monetary sign identification**  
For shock `\(j=\varepsilon^{\text{MP}}\)`, require
`$$\{C_h\}_{i,j} \in \begin{cases}
(+, \; \text{at } i=i_t,\, h=0),\\
(-, \; \text{at } i=y_t,\, h=1\!:\!4),\\
(-, \; \text{at } i=\pi_t,\, h=4\!:\!12).
\end{cases}$$`

**D) Long‑run BQ (two‑variable)**  
With `\(y_t = \Delta \log Y_t\)`, impose
`$$\sum_{h=0}^\infty (C_h)_{y,\; \varepsilon^{\text{demand}}} = 0 \,.$$`


---
# Posterior computation, full B‑SVAR sampler

1. **Reduced‑form draw** `\((\Phi^{(s)},\Sigma^{(s)}) \sim \text{MNIW posterior}\)`.

2. **Shock rotation**: `\(\Sigma^{(s)}=PP'\)`, draw `\(Q \sim \text{Haar}\)`, set `\(B_0^{(s)}=PQ\)`.

3. **Check restrictions** (impact zeros/values, signs over `\(h\in\mathcal H\)`, long‑run zeros on `\(C(1)\)`).

4. **Accept** `\((\Phi^{(s)},B_0^{(s)})\)` if constraints hold; else go back to 2.

5. Compute IRFs `\(C_h^{(s)}\)`, FEVDs, and historical decompositions.

6. Summarize across accepted draws: medians and `\(68\%/90\%\)` credible sets.





---
# Example for Bayesian SVAR
&lt;div style="font-size: 18px;"&gt;

### Short-Run Restrictions

``` r
library(bsvars)
library(readxl)
library(tsaccessories)
library(dplyr)
dat&lt;-read_excel("datawork/data.xlsx", sheet = "Sheet1")
set.seed(123) #Fixes R’s random number generator seed so MCMC sampling and other random draws are reproducible across runs.
dat &lt;- tsconvert(dat, start = c(1959, 1), period = "quarterly")
#Keep only numeric series
is_num &lt;- vapply(dat, is.numeric, logical(1))
#Checks each column with is.numeric.
if (!all(is_num)) {
  message("Dropping non-numeric columns: ",
          paste(names(dat)[!is_num], collapse = ", "))
  dat &lt;- dat[ , is_num, drop = FALSE]
}

dat &lt;- dat[, c("inflation", "unemployment_rate", "interest_rate"), drop = FALSE]

#Handle missing data and build T×N matrix
# Remove any rows with missing values in any remaining column. 

dat &lt;- stats::na.omit(dat)
Y &lt;- as.matrix(dat)  # Keep as T x N

# Safety: auto-correct if the file was N×T

if (ncol(Y) &gt; nrow(Y)) {
  message("Detected more columns than rows; assuming N x T and transposing to T x N.")
  Y &lt;- t(Y)
}
```
---
# Example for Bayesian SVAR
&lt;div style="font-size: 18px;"&gt;

Short-Run Restrictions

.pull-left[

``` r
#Dimension printouts and sanity checks
#Store the number of observations and variables for later checks/messages.
T_ &lt;- nrow(Y); N_ &lt;- ncol(Y)
cat(sprintf("Detected T = %d observations, N = %d variables\n", T_, N_))
```

```
## Detected T = 215 observations, N = 3 variables
```

``` r
#Specify the BSVAR
# Build a model specification object with endogenous data Y
# p = 2 sets the autoregressive lag length to 2.
# With no extra identification block supplied, bsvars uses its default structural identification: recursive (Cholesky) on the variable order in Y.

# MCMC estimation (single pass) #Set the number of MCMC iterations to run and mark it as an integer with L.
```
]

.pull-right[

``` r
bsvar &lt;- specify_bsvar$new(
  Y,    # T x N
  p = 2 # lag order
)
```

```
## The identification is set to the default option of lower-triangular structural matrix.
```

``` r
S_total &lt;- 1500L    # toy; increase massively in real work
```
]

---
# Example for Bayesian SVAR
&lt;div style="font-size: 18px;"&gt;

### Short-Run Restrictions

``` r
#Set the number of MCMC iterations to run and mark it as an integer with L. This is only a small demo value—use thousands for applied work.
post_bsvar&lt;- estimate(bsvar, S = S_total)
#Post-estimation objects
irf_bsvar  &lt;- compute_impulse_responses(post_bsvar, horizon = 12L)
fevd_bsvar &lt;- compute_variance_decompositions(post_bsvar, horizon = 12L)
fitted_bsvar&lt;- compute_fitted_values(post_bsvar)
hd_bsvar&lt;- compute_historical_decompositions(post_bsvar, show_progress = TRUE)
sigma_bsvar&lt;- compute_conditional_sd(post_bsvar)
print(summary(post_bsvar))
plot(irf_bsvar)
plot(fevd_bsvar)
plot(hd_bsvar)
plot(sigma_bsvar)
```

---
# Example for Bayesian SVAR
&lt;div style="font-size: 18px;"&gt;

.pull-left[

&lt;img src="img/bsvarirf.png" width="800"/&gt;

- An inflationary shock (Shock 1) raises both inflation and unemployment, prompting tighter monetary policy — suggestive of stagflationary pressures.
]

.pull-right[
- A disinflationary shock (Shock 2) reduces inflation persistently but at the cost of higher unemployment, with interest rates falling as policy accommodates.

- A monetary easing shock (Shock 3) lowers interest rates persistently but has muted effects on inflation and unemployment, pointing to limited real sector responsiveness to policy interventions.

- These results underline the trade-offs faced by monetary authorities and the asymmetric effectiveness of different structural shocks.

]



---
# Example for Bayesian SVAR
&lt;div style="font-size: 18px;"&gt;

.pull-left[

&lt;img src="img/bsvarfevd.png" width="800"/&gt;

- Inflation variance is dominated by its own inflationary shock, consistent with persistent cost-push or demand-pull disturbances.
]

.pull-right[
- Unemployment is almost entirely explained by labor market shocks, reflecting rigidities and persistence in employment dynamics.

- Interest rate variance, by contrast, is distributed across all three shocks, with monetary policy shocks dominating at short horizons, while inflation and unemployment shocks gain importance at longer horizons.

- This highlights the endogenous and reactive nature of monetary policy within the structural system.

]


---
# Example for Bayesian SVAR
&lt;div style="font-size: 18px;"&gt;

.pull-left[

&lt;img src="img/bshdirf.png" width="800"/&gt;

- The historical decomposition results from the BSVAR indicate that inflation has been largely driven by supply-side shocks, while unemployment dynamics are almost entirely explained by persistent labor market shocks.
]

.pull-right[
- Monetary policy shocks, while not central to explaining inflation or unemployment, dominate key episodes of interest rate fluctuations, underscoring the endogenous and reactive nature of policy interventions.

- These findings reinforce the structural segmentation revealed by the FEVD, showing that inflation and unemployment are primarily self-driven, whereas interest rates reflect a mixture of autonomous policy actions and systematic responses to real-side shocks.

]


---
# Example for Bayesian SVAR
&lt;div style="font-size: 18px;"&gt;
### Sign Restrictions 


``` r
library(bsvarSIGNs)
library(readxl)
dat &lt;- read_excel("datawork/data.xlsx")
set.seed(123) #Fixes R’s random number generator seed so MCMC sampling and other random draws are reproducible across runs.

is_num &lt;- vapply(dat, is.numeric, logical(1))
if (!all(is_num)) dat &lt;- dat[, is_num, drop = FALSE]
dat &lt;- stats::na.omit(dat)

# Optional: enforce canonical order if present
want &lt;- c("inflation","unemployment_rate","interest_rate")
if (all(want %in% names(dat))) dat &lt;- dat[, want]

Y  &lt;- as.matrix(dat)
T_ &lt;- nrow(Y); N_ &lt;- ncol(Y)
stopifnot(T_ &gt; N_, T_ &gt;= 20)


# --- VAR &amp; SR settings ---
p      &lt;- 2L               # &lt;&lt; lag order = 2
H_irf  &lt;- 12L              # IRF horizons to plot
# Keep SR horizons modest for stability/acceptance
H_sgn  &lt;- 2L               # impose signs at h = 0..1 (impact + 1 period)


# --- Sign restrictions array: [variables x shocks x horizons] ---
# Variables in rows: 1=inflation, 2=unemployment, 3=interest_rate
# Shocks in columns: 1=Demand, 2=Supply (positive), 3=Monetary (tightening)
sign_irf &lt;- array(NA_integer_, dim = c(N_, N_, H_sgn))


# Demand: π +, u −, i +  (Taylor + Okun) at h = 0..1
for (s in 1:H_sgn) {
  sign_irf[1, 1, s] &lt;- +1
  sign_irf[2, 1, s] &lt;- -1
  sign_irf[3, 1, s] &lt;- +1
}

# Positive Supply: π −, u −, i − at h = 0..1
for (s in 1:H_sgn) {
  sign_irf[1, 2, s] &lt;- -1
  sign_irf[2, 2, s] &lt;- -1
  sign_irf[3, 2, s] &lt;- -1
}

# Monetary tightening: i + on impact; π −, u + over 0..1
sign_irf[3, 3, 1] &lt;- +1      # i at h=0
for (s in 1:H_sgn) {
  sign_irf[1, 3, s] &lt;- -1    # π falls
  sign_irf[2, 3, s] &lt;- +1    # u rises
}

# --- Specify SR-BSVAR ---
spec &lt;- specify_bsvarSIGN$new(
  data      = Y,
  p         = p,
  sign_irf  = sign_irf,
  max_tries = 5000L
)


# ================== PASS A: IRF/FEVD from larger posterior ==================
post_A &lt;- estimate(spec, S = 4000L, thin = 2L)  # ~2000 kept; adjust as you like

irf  &lt;- compute_impulse_responses(post_A, horizon = H_irf)
fevd &lt;- compute_variance_decompositions(post_A, horizon = H_irf)

# Save and free memory before HD
saveRDS(list(irf=irf, fevd=fevd), file = "sr_irf_fevd.rds")
rm(irf, fevd, post_A); gc()

# ================== PASS B: HD from a smaller posterior =====================
# Force single-threaded math to avoid native crashes (OpenMP/BLAS)
if (requireNamespace("RhpcBLASctl", quietly = TRUE)) {
  RhpcBLASctl::blas_set_num_threads(1L)
  RhpcBLASctl::omp_set_num_threads(1L)
}
Sys.setenv(
  OMP_NUM_THREADS           = "1",
  OPENBLAS_NUM_THREADS      = "1",
  MKL_NUM_THREADS           = "1",
  VECLIB_MAXIMUM_THREADS    = "1",
  RCPP_PARALLEL_NUM_THREADS = "1"
)

post_B &lt;- estimate(spec, S = 1200L, thin = 2L)  # smaller kept sample (stable for HD)

# Try HD; if anything goes wrong, fall back to an even smaller run
hd &lt;- try(compute_historical_decompositions(post_B), silent = TRUE)
if (inherits(hd, "try-error")) {
  rm(post_B); gc()
  post_B &lt;- estimate(spec, S = 800L, thin = 2L)
  hd &lt;- compute_historical_decompositions(post_B)
}

saveRDS(hd, file = "sr_hd.rds")

# --- Plots (optional) ---
# Reload IRF/FEVD and plot, then plot HD
obj &lt;- readRDS("sr_irf_fevd.rds")
plot(obj$irf,  probability = 0.68)
plot(obj$fevd, probability = 0.68)
plot(hd)
```





---
# Example for Bayesian SVAR
&lt;div style="font-size: 24px;"&gt;

.pull-left[

&lt;img src="img/sign1.png" width="800"/&gt;

- Demand shocks increase inflation and interest rates while reducing unemployment, consistent with the Taylor rule and Okun’s law.

]

.pull-right[

- Positive supply shocks reduce inflation and unemployment while allowing for lower interest rates, consistent with productivity-driven expansions.

- Monetary tightening shocks produce contractionary effects, lowering inflation at the cost of higher unemployment. 

- These results highlight the importance of distinguishing between demand, supply, and policy shocks when analyzing macroeconomic fluctuations.

]
???
- Demand Shock
  - Inflation (π) ↑
  - Unemployment (u) ↓
  - Interest rate (i) ↑
- (reflecting Taylor rule + Okun’s law).
- Positive Supply Shock
  - Inflation (π) ↓
  - Unemployment (u) ↓
  - Interest rate (i) ↓
- (expansionary supply, consistent with productivity gains).
- Monetary Tightening Shock
  - Interest rate (i) ↑ on impact
  - Inflation (π) ↓
  - Unemployment (u) ↑
(standard contractionary monetary policy).



---
# Example for Bayesian SVAR
&lt;div style="font-size: 24px;"&gt;

.pull-left[

&lt;img src="img/sign2.png" width="1900"/&gt;


]

.pull-right[
- The FEVD results indicate that demand shocks are the dominant drivers of macroeconomic fluctuations, explaining the majority of the variance in inflation, unemployment, and interest rates. 

- Supply shocks have short-lived effects, particularly on inflation, while monetary tightening shocks play only a modest role, consistent with the view that interest rate dynamics largely reflect endogenous policy responses to demand conditions rather than independent monetary disturbances.

]


---
# Example for Bayesian SVAR
&lt;div style="font-size: 23px;"&gt;

.pull-left[

&lt;img src="img/sign3.png" width="1900"/&gt;


]

.pull-right[
- The historical decomposition confirms that demand shocks were the predominant drivers of macroeconomic fluctuations across the sample, accounting for the bulk of the dynamics in inflation, unemployment, and interest rates. 

- Supply shocks contributed episodically, consistent with energy or cost-push disturbances, while monetary policy shocks played a relatively modest role, emerging mainly during tightening episodes. 

- These results suggest that macroeconomic volatility was largely demand-driven, with policy reacting endogenously rather than serving as an independent destabilizing force.

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "solarized-light",
  "highlightLanguage": ["r", "css", "yaml"],
  "highlightLines": true,
  "countIncrementalSlides": false,
  "ratio": "16:9",
  "slideNumberFormat": "<div class=\"slide-number\" style=\"position: absolute; bottom: 1em; right: 2em; font-size: 0.9em; color: #999;\">\n  %current% / %total%\n</div>"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
